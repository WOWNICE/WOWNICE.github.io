<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>In Love with CodeCode</title>
  
  <subtitle>Haizhou&#39;s Blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.shihaizhou.com/"/>
  <updated>2020-05-15T04:16:51.913Z</updated>
  <id>http://www.shihaizhou.com/</id>
  
  <author>
    <name>Haizhou Shi</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Mutual Information Maximization - Experiments</title>
    <link href="http://www.shihaizhou.com/2020/05/13/MI-Maximization-Experiments/"/>
    <id>http://www.shihaizhou.com/2020/05/13/MI-Maximization-Experiments/</id>
    <published>2020-05-13T11:28:43.000Z</published>
    <updated>2020-05-15T04:16:51.913Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://gtas.unican.es/files/docencia/TICC/apuntes/tema1bwp_0.pdf" target="_blank" rel="noopener">MI for correlated gaussian model</a></li><li><a href="https://github.com/seanie12/MINE" target="_blank" rel="noopener">github repo implementing MINE</a></li><li><a href="https://shihaizhou.com/2020/05/10/MI-Maximization/" target="_blank" rel="noopener">my former blog about mutual information theory</a></li></ul><h1 id="MI-Estimation"><a href="#MI-Estimation" class="headerlink" title="MI Estimation"></a>MI Estimation</h1><h2 id="Correlated-gaussian-variables"><a href="#Correlated-gaussian-variables" class="headerlink" title="Correlated gaussian variables"></a>Correlated gaussian variables</h2><p>Let $(X, Y)^T$ be a zero-mean Gaussian random vector with covariance matrix given by</p><script type="math/tex; mode=display">\mathbf{C}=\left(\begin{array}{cc}\sigma^{2} & \rho \sigma^{2} \\ \rho \sigma^{2} & \sigma^{2}\end{array}\right)</script><p>Then the theoratical MI of the correated Gaussian distribution is given by</p><script type="math/tex; mode=display">\begin{align}I(X ; Y) &=h(Y)-h(Y | X) \\ &=h(Y)+h(X)-h(X, Y) \\ &=-\frac{1}{2} \ln \left(1-\rho^{2}\right) \end{align}</script><p>需要注意的是，材料当中给出的公式是$\log$，但这log表示的就是以自然对数为底的，这一点和numpy中的log函数规范一致，即log2才表示已2为底的取对数。接着我们通过numpy简单生成correlated guassian samples. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mean = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">cov = [[<span class="number">1</span>, <span class="number">0.6</span>], [<span class="number">0.6</span>,<span class="number">1</span>]]</span><br><span class="line">ro = cov[<span class="number">0</span>][<span class="number">1</span>] / cov[<span class="number">1</span>][<span class="number">1</span>]</span><br><span class="line">mi = <span class="number">-0.5</span> * np.log(<span class="number">1</span>-ro**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">X, Y = np.random.multivariate_normal(mean, cov, <span class="number">500</span>).T</span><br><span class="line">plt.plot(X, Y, <span class="string">'.'</span>)</span><br><span class="line">plt.axis(<span class="string">'equal'</span>)</span><br><span class="line">plt.title(<span class="string">'true-mi = &#123;:.4&#125;'</span>.format(mi))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>从而我们得到了一个带有Ground-truth MI的样本. </p><p><img src="https://i.loli.net/2020/05/12/MXSuCpb1AVdPOh3.png" alt="image.png"></p><h2 id="MI-estimation-with-linear-discriminator"><a href="#MI-estimation-with-linear-discriminator" class="headerlink" title="MI estimation with linear discriminator"></a>MI estimation with linear discriminator</h2><p>We built a Linear-Discriminator-based MI estimator from scratch，其中采用的优化方法是梯度下降法。Linear discriminator会将两个sample $(a_n, b_n)$ 进行concatenation，通过一个简单的线性层即可就产生出scalar output. 因为是最简单的模型所以我们可以根据公式直接计算出每一步梯度的函数表示（具体看 KLD_estimate 和 gradient_descent 函数）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MIEstimator</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, X, Y, lr=<span class="number">0.01</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        X: N * D1</span></span><br><span class="line"><span class="string">        Y: N * D2</span></span><br><span class="line"><span class="string">        w: (D1+D2+1) * 1</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        N = len(X)</span><br><span class="line"><span class="comment">#         x_join_y = np.hstack([X, Y, np.ones([N,1])])</span></span><br><span class="line">        x_join_y = np.hstack([X, Y])</span><br><span class="line"></span><br><span class="line"><span class="comment">#         x_prod_y = np.hstack([X, Y_, np.ones([N,1])])</span></span><br><span class="line">        Y_ = np.vstack([np.array(Y) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>)])</span><br><span class="line">        np.random.shuffle(Y_)</span><br><span class="line">        x_prod_y = np.hstack([np.vstack([np.array(X) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>)]), Y_])</span><br><span class="line">        </span><br><span class="line">        w = np.ones(shape=[x_join_y.shape[<span class="number">1</span>], <span class="number">1</span>])</span><br><span class="line">        </span><br><span class="line">        self.N = N</span><br><span class="line">        self.x_join_y = x_join_y</span><br><span class="line">        self.x_prod_y = x_prod_y</span><br><span class="line">        self.w = w</span><br><span class="line">        self.lr = lr</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">KLD_estimate</span><span class="params">(self)</span>:</span></span><br><span class="line">        res =  np.matmul(self.x_join_y, self.w).mean()</span><br><span class="line">        res -= np.log2(np.exp(np.matmul(self.x_prod_y, self.w)).mean())</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">grad_descent</span><span class="params">(self)</span>:</span></span><br><span class="line">        delta_w = self.x_join_y.mean(axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>).T</span><br><span class="line"></span><br><span class="line">        exp_fx = np.exp(np.matmul(self.x_prod_y, self.w))</span><br><span class="line">        last_item = <span class="number">1</span>/np.log(<span class="number">2</span>) * <span class="number">1</span>/exp_fx.sum() * np.matmul(self.x_prod_y.T, exp_fx)</span><br><span class="line"></span><br><span class="line">        delta_w -= last_item</span><br><span class="line">        self.delta_w = delta_w</span><br><span class="line"></span><br><span class="line">        self.w += delta_w * self.lr</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">            self.grad_descent()</span><br><span class="line">            print(<span class="string">'------------------'</span>)</span><br><span class="line">            print(self.KLD_estimate)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.KLD_estimate</span><br></pre></td></tr></table></figure><p>通过实验我们发现，简单的linear discriminator不能够完成MI Estimation的任务（几乎拟合的点都在0左右）。同时还有一个不符合常规的情况，那就是如果我们给这个线性模型加上bias，则bias项的gradient将会变成常数（可以数学推导），使得estimation无限增大。这说明了$sup​$在所有函数中找到最优函数是一件非常困难的事情，使用更加复杂的模型来做estimation才是正确的道路。</p><h2 id="MI-estimation-with-neural-networks"><a href="#MI-estimation-with-neural-networks" class="headerlink" title="MI estimation with neural networks"></a>MI estimation with neural networks</h2><p>为了使用更加复杂的非线性函数，我们使用<a href="https://github.com/seanie12/MINE" target="_blank" rel="noopener">github for MINE</a>中pytorch实现Neural Estimation for MI的代码. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MINE</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_size=<span class="number">10</span>)</span>:</span></span><br><span class="line">        super(MINE, self).__init__()</span><br><span class="line">        self.layers= nn.Sequential(nn.Linear(<span class="number">2</span>, hidden_size),</span><br><span class="line">                                   nn.ReLU(),</span><br><span class="line">                                   nn.Linear(hidden_size,hidden_size),</span><br><span class="line">                                   nn.ReLU(),</span><br><span class="line">                                   nn.Linear(hidden_size, <span class="number">1</span>)</span><br><span class="line">                                  )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">        tiled_x = torch.cat([x, x,], dim=<span class="number">0</span>)</span><br><span class="line">        idx = torch.randperm(batch_size)</span><br><span class="line">        </span><br><span class="line">        shuffled_y = y[idx]</span><br><span class="line">        concat_y = torch.cat([y, shuffled_y], dim=<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        inputs = torch.cat([tiled_x, concat_y], dim=<span class="number">1</span>)</span><br><span class="line">        logits = self.layers(inputs)</span><br><span class="line"></span><br><span class="line">        pred_xy = logits[:batch_size]</span><br><span class="line">        pred_x_y = logits[batch_size:]</span><br><span class="line">        loss = -(torch.mean(pred_xy) </span><br><span class="line">               - torch.log(torch.mean(torch.exp(pred_x_y))))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><p>在不同相关性上的Correlated Gaussian Distribution上进行estimation。非常凑巧的是，原文中也做了双变量高斯分布的实验，并且和其他的estimation算法进行比较，得到的结果也非常优秀：几乎和True MI完全重合.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Hyperparameters</span></span><br><span class="line">hidden_size=<span class="number">10</span></span><br><span class="line">n_epoch = <span class="number">500</span></span><br><span class="line">data_size = <span class="number">2000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create model</span></span><br><span class="line">model = MINE(hidden_size)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">all_mi = []</span><br><span class="line">true_mi = []</span><br><span class="line"></span><br><span class="line">mean = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>):</span><br><span class="line">    <span class="comment"># change the covariance matrix</span></span><br><span class="line">    cov = [[<span class="number">1</span>, <span class="number">2</span>*i/<span class="number">10</span>], [<span class="number">2</span>*i/<span class="number">10</span>, <span class="number">1</span>]]</span><br><span class="line">    ro = cov[<span class="number">0</span>][<span class="number">1</span>] / cov[<span class="number">1</span>][<span class="number">1</span>]</span><br><span class="line">    mi = <span class="number">-0.5</span> * np.log(<span class="number">1</span>-ro**<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># new training trajactory</span></span><br><span class="line">    true_mi.append(mi)</span><br><span class="line">    all_mi.append([])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tqdm(range(n_epoch)):</span><br><span class="line">        X, Y = np.random.multivariate_normal(mean, cov, data_size).T</span><br><span class="line">        x_sample = np.expand_dims(X, axis=<span class="number">1</span>)</span><br><span class="line">        y_sample = np.expand_dims(Y, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        x_sample = torch.from_numpy(x_sample).float()</span><br><span class="line">        y_sample = torch.from_numpy(y_sample).float()</span><br><span class="line">        loss = model(x_sample, y_sample)</span><br><span class="line"></span><br><span class="line">        model.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        all_mi[<span class="number">-1</span>].append(-loss.item())</span><br><span class="line"></span><br><span class="line">print(true_mi)</span><br><span class="line">print(all_mi)</span><br></pre></td></tr></table></figure><p>最终利用以下code将结果进行可视化. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">plt.rcParams[<span class="string">"figure.figsize"</span>] = (<span class="number">25</span>,<span class="number">10</span>) <span class="comment"># Set the figure size</span></span><br><span class="line">plt.suptitle(<span class="string">'Simple NeuralNet MI Estimation'</span>) <span class="comment"># super title for all subplots</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    <span class="comment"># return the axis object.</span></span><br><span class="line">    ax = plt.subplot(<span class="number">2</span>, <span class="number">3</span>, i+<span class="number">1</span>)</span><br><span class="line">    ax.set_ylim([<span class="number">-0.05</span>, <span class="number">0.8</span>])</span><br><span class="line">    ax.plot(range(len(all_mi[i])), all_mi[i], color=<span class="string">'r'</span>, label=<span class="string">'Estimated MI'</span>)</span><br><span class="line">    ax.plot([<span class="number">0</span>, len(all_mi[i])], [true_mi[i],true_mi[i]], color=<span class="string">'b'</span>, label=<span class="string">'Ground-truth MI'</span>)</span><br><span class="line"><span class="comment">#     ax.set_xlabel('training steps')</span></span><br><span class="line">    ax.legend(loc=<span class="string">'best'</span>)</span><br><span class="line">    ax.set_title(<span class="string">'ro=&#123;:.2f&#125;'</span>.format(<span class="number">0.2</span>*i))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/05/13/EcRsnGKAYCtr9VM.png" alt="image.png"></p><h1 id="MI-Maximization"><a href="#MI-Maximization" class="headerlink" title="MI Maximization"></a>MI Maximization</h1><h2 id="Inductive-unsupervised-representation-learning-via-MI-maximization"><a href="#Inductive-unsupervised-representation-learning-via-MI-maximization" class="headerlink" title="Inductive unsupervised representation learning via MI maximization"></a>Inductive unsupervised representation learning via MI maximization</h2><p>Assume we have the simulated unsupervised dataset $\tilde X = g(X)$, without knowing the form or any other knowledge of generation function $g$. </p><p>The target of the representation learning is to learn a model $M_{\psi}: \tilde X \rightarrow \hat X$ which maximize the mutual information between $\tilde X$ and $\hat X$. By doing so, we hope the model $M_\psi$ will help come into use one day in the future (using the knowledge of unlabeled data for coming supervised learning). </p><h2 id="Theoretical-solution"><a href="#Theoretical-solution" class="headerlink" title="Theoretical solution"></a>Theoretical solution</h2><p>Recall we use Donsker-Varadhan representation of KL divergence to estimate MI: </p><script type="math/tex; mode=display">\mathcal{I}(X ; Y):=\mathcal{D}_{K L}(\mathbb{I} \| \mathbb{M}) \geq \widehat{\mathcal{I}}_{\omega}^{(D V)}(X ; Y):=\mathbb{E}_{\mathbb{J}}\left[T_{\omega}(x, y)\right]-\log \mathbb{E}_{\mathbb{M}}\left[e^{T_{\omega}(x, y)}\right]</script><p>The goal of learning the best moddel $M$ becomes simultaneously estimating and maximizing MI.</p><script type="math/tex; mode=display">(\hat{\omega}, \hat{\psi})_{G}=\underset{\omega, \psi}{\arg \max } \widehat{\mathcal{I}}_{\omega}\left(\tilde X ; M_{\psi}(\tilde X)\right)</script><p>where $\hat I$ is the total loss computed completely without supervision, i.e., given only $\tilde X$. </p><h2 id="Model-implementation"><a href="#Model-implementation" class="headerlink" title="Model implementation"></a>Model implementation</h2><p>Now we only need to treat them as a joint optimization problem. The internal logic of this unsupervised training setting is as follows:</p><ul><li>we have two smaller models - a inductive representation generator and an MI estimator - residing in the main model; </li><li>when given a mini-batch of samples of $\tilde X$, the generator maps $\tilde X$ to $\hat X$;</li><li>then the MI estimator treat the $(\tilde X, \hat X)$ as two distribution, estimate the mutual information of them, or more precisely, the lower bound of the their mutual information;<ul><li>when optimizing the MI estimation, the gradient of the objective also flows back to the generator;</li><li>that’s why we say they are trained jointly. </li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn </span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MIMaximizer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size=<span class="number">5</span>, hidden_size=<span class="number">10</span>)</span>:</span></span><br><span class="line">        super(MIMaximizer, self).__init__()</span><br><span class="line">        self.est_layers = nn.Sequential(nn.Linear(input_size+<span class="number">1</span>, hidden_size),</span><br><span class="line">                                        nn.ReLU(),</span><br><span class="line">                                        nn.Linear(hidden_size,hidden_size),</span><br><span class="line">                                        nn.ReLU(),                                   </span><br><span class="line">                                        nn.Linear(hidden_size, <span class="number">1</span>)</span><br><span class="line">                                       )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># model that generates representation of given input x</span></span><br><span class="line">        self.model_layers = nn.Sequential(nn.Linear(input_size, hidden_size),</span><br><span class="line">                                         nn.ReLU(),</span><br><span class="line">                                         nn.Linear(hidden_size,hidden_size),</span><br><span class="line">                                         nn.ReLU(),                                   </span><br><span class="line">                                         nn.Linear(hidden_size, <span class="number">1</span>)</span><br><span class="line">                                        )</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        y = self.model_layers(x)</span><br><span class="line">        </span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">        tiled_x = torch.cat([x, x,], dim=<span class="number">0</span>)</span><br><span class="line">        idx = torch.randperm(batch_size)</span><br><span class="line">        </span><br><span class="line">        shuffled_y = y[idx]</span><br><span class="line">        concat_y = torch.cat([y, shuffled_y], dim=<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        inputs = torch.cat([tiled_x, concat_y], dim=<span class="number">1</span>)</span><br><span class="line">        logits = self.est_layers(inputs)</span><br><span class="line"></span><br><span class="line">        pred_xy = logits[:batch_size]</span><br><span class="line">        pred_x_y = logits[batch_size:]</span><br><span class="line">        loss = -(torch.mean(pred_xy) </span><br><span class="line">               - torch.log(torch.mean(torch.exp(pred_x_y))))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> y, loss</span><br></pre></td></tr></table></figure><h2 id="Simulating-high-dimensional-data"><a href="#Simulating-high-dimensional-data" class="headerlink" title="Simulating high-dimensional data"></a>Simulating high-dimensional data</h2><p>To simulate high-dimensional data, the generation function $g: x \rightarrow [f_1(x), f_2(x), \cdots, f_n(x)]$ is to generate high dimensional representation of $x$. For the sake of simplicity, the original $x$ is sampled from uniform distribution. And we can also add some noise into the model to make it more close to the real-world scenario. </p><p>One reason to play with such a toy setting is that now we can treat the problem of finding function $g^{-1}: [f_1(x), f_2(x), \cdots, f_n(x)] \rightarrow x$ as a supervised regression problem, and we have the ground-truth label of them!</p><p>The experimented data generation methods are as follows: </p><ul><li>$x \rightarrow [x^1-x^2, x^2-x^3, \cdots, x^k-x^{k+1}, \cdots, x^{n-1}-x^n, x^n]$, when reversed as a regression task, is linear-solvable. </li><li>$x \rightarrow [x^1-x^2, x^2-x^3, \cdots, x^k-x^{k+1}, \cdots, x^{n-1}-x^n]$, when reversed as a regression task, is <strong>not</strong> linear-solvable, but can be easily approximated with linear model when $n$ is large. </li><li>$x \rightarrow [\sin(x), \cos(x), \log(2+x), x^2, \sinh(x), \cosh(x), \tanh(x)]$, where each non-linear function $f$ can be further mapped to even more complex space by $J: f \rightarrow [f(x)^{\frac{1}{i+1}}|x: x \in \operatorname{dom}_f]$</li></ul><h2 id="Sanity-check"><a href="#Sanity-check" class="headerlink" title="Sanity check"></a>Sanity check</h2><ul><li><strong>Target</strong>: check whether the algorithm works. </li><li><strong>Method</strong>: using LR model to fit on the generated representation and the ground truth $(\tilde X, X)$, to see whether it’s MSE is minimized along the MI maximization.</li><li><strong>Data</strong>: We use $x$ -&gt; $[x^1-x^2, x^2-x^3, \cdots, x^k-x^{k+1}, \cdots, x^{n-1}-x^n, x^n]$ to generate high-dimensional representation of $x$. </li></ul><p>Data Generation: </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = np.random.uniform(low=<span class="number">-1.</span>,high=<span class="number">1.</span>,size=<span class="number">3000</span>).reshape([<span class="number">-1</span>,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># generate x_representation based on [x^2, x^3, ... x^n]</span></span><br><span class="line">n = <span class="number">5</span></span><br><span class="line">x1 = np.hstack([np.power(x, i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, <span class="number">2</span> + n)])</span><br><span class="line">x2 = np.hstack([np.power(x, i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">1</span> + n)])</span><br><span class="line">x_tilde = np.hstack([x2 - x1, np.power(x, n+<span class="number">1</span>)])</span><br><span class="line">input_size = n+<span class="number">1</span></span><br></pre></td></tr></table></figure><p>The result of linear-solvable data $x \rightarrow [x^1-x^2, x^2-x^3, \cdots, x^k-x^{k+1}, \cdots, x^{n-1}-x^n, x^n]$ : </p><p><img src="https://i.loli.net/2020/05/14/MdeW8EOobrcuPIB.png" alt="image.png"></p><h2 id="Influence-of-dimensionality-of-representation"><a href="#Influence-of-dimensionality-of-representation" class="headerlink" title="Influence of dimensionality of representation"></a>Influence of dimensionality of representation</h2><ul><li><strong>Target</strong>: find how the dimension of the representation matters.</li><li><strong>Method</strong>: use different output dimension. </li><li><strong>Data</strong>: We use $x$ -&gt; $[x^1-x^2, x^2-x^3, \cdots, x^k-x^{k+1}, \cdots, x^{n-1}-x^n]$ to generate high-dimensional representation of $x$, which is not linear-solvable. </li></ul><p>Data Generation: </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = np.random.uniform(low=<span class="number">-1.</span>,high=<span class="number">1.</span>,size=<span class="number">3000</span>).reshape([<span class="number">-1</span>,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># generate x_representation based on [x^2, x^3, ... x^n]</span></span><br><span class="line">n = <span class="number">10</span></span><br><span class="line">x1 = np.hstack([np.power(x, i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, <span class="number">2</span> + n)])</span><br><span class="line">x2 = np.hstack([np.power(x, i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">1</span> + n)])</span><br><span class="line">x_tilde = np.hstack([x2 - x1])</span><br><span class="line">input_size = n</span><br></pre></td></tr></table></figure><p>Define <code>train()</code> function, train the model across different dimensionality setting: </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(x_tilde, n_epoch=<span class="number">1000</span>, output_size=<span class="number">1</span>, hidden_size=<span class="number">10</span>, lr=<span class="number">0.01</span>, log_every_n_steps=<span class="number">10</span>)</span>:</span></span><br><span class="line">    input_size = x_tilde.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># create the model </span></span><br><span class="line">    model = MIMaximizer(</span><br><span class="line">        input_size=input_size, </span><br><span class="line">        hidden_size=hidden_size, </span><br><span class="line">        output_size=output_size</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># print([_.size() for _ in model.parameters()])</span></span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">    all_mi = []</span><br><span class="line">    best_linear_model_loss = []</span><br><span class="line"></span><br><span class="line">    x_sample =  torch.from_numpy(x_tilde).float()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> tqdm(range(n_epoch)):</span><br><span class="line">        y, loss = model(x_sample)</span><br><span class="line"></span><br><span class="line">        model.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> epoch % log_every_n_steps == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="comment"># record the MI</span></span><br><span class="line">                all_mi.append(-loss.item())</span><br><span class="line"></span><br><span class="line">                <span class="comment"># find the best simple method modeling generated representation of x_tilde.</span></span><br><span class="line">                y_ = y.detach().numpy()</span><br><span class="line">                reg = LinearRegression().fit(y_, x)</span><br><span class="line">                x_hat = reg.predict(y_)</span><br><span class="line">                mse = mean_squared_error(x, x_hat)</span><br><span class="line">                best_linear_model_loss.append(mse)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                print(<span class="string">'nan!'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> all_mi, best_linear_model_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hidden_size = <span class="number">10</span></span><br><span class="line">n_epoch = <span class="number">10000</span></span><br><span class="line">output_size = <span class="number">5</span></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line">log_every_n_steps = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">all_mi_across_models = []</span><br><span class="line">all_mse_across_models = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> output_size <span class="keyword">in</span> range (<span class="number">1</span>,<span class="number">6</span>):</span><br><span class="line">    mis, mses = train(</span><br><span class="line">        x_tilde=x_tilde,</span><br><span class="line">        n_epoch=n_epoch,</span><br><span class="line">        output_size=output_size,</span><br><span class="line">        hidden_size=hidden_size,</span><br><span class="line">        lr=lr,</span><br><span class="line">        log_every_n_steps=<span class="number">10</span>   </span><br><span class="line">    )</span><br><span class="line">    all_mi_across_models.append(mis)</span><br><span class="line">    all_mse_across_models.append(mses)</span><br></pre></td></tr></table></figure><p>Use the following code to visualize: </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">plt.rcParams[<span class="string">"figure.figsize"</span>] = (<span class="number">20</span>,<span class="number">10</span>)</span><br><span class="line">plt.suptitle(<span class="string">'Study of Dimension of Representation'</span>)</span><br><span class="line"></span><br><span class="line">ax = plt.subplot(<span class="number">211</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    ax.plot(range(len(all_mi_across_models[i])), all_mi_across_models[i], label=<span class="string">'Dim=&#123;&#125;'</span>.format(i+<span class="number">1</span>))</span><br><span class="line">    ax.legend(loc=<span class="string">'best'</span>)</span><br><span class="line">    ax.set_title(<span class="string">'MI Maximization'</span>)</span><br><span class="line">    </span><br><span class="line">ax = plt.subplot(<span class="number">212</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    ax.plot(range(len(all_mse_across_models[i])), all_mse_across_models[i], label=<span class="string">'Dim=&#123;&#125;'</span>.format(i+<span class="number">1</span>))</span><br><span class="line">    ax.legend(loc=<span class="string">'best'</span>)</span><br><span class="line">    ax.set_title(<span class="string">'Best LR-MSE'</span>)</span><br></pre></td></tr></table></figure><p>(1) Data without noise:</p><p><img src="https://i.loli.net/2020/05/14/B4QpckTaKitYsjX.png" alt="image.png"></p><p>(2) Data with noise (scale=0.05):</p><p><img src="https://i.loli.net/2020/05/14/ujPaUWfS6AVMN8y.png" alt="image.png"></p><h2 id="Unsupervised-Representation-Learning"><a href="#Unsupervised-Representation-Learning" class="headerlink" title="Unsupervised Representation Learning"></a>Unsupervised Representation Learning</h2><ul><li>Target: Study the unsupervised learning using MI maximization.</li><li>Method: unsupervised training dataset, supervised training dataset, testset. </li><li>Data: We create the dataset (unsupervised/supervised/test dataset)</li></ul><p>some tricks regarding generating a list of functions: <a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017434209254976" target="_blank" rel="noopener">廖雪峰python教程</a></p><p>Data Generation given a list of functions:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_data</span><span class="params">(size, g_fns, split=<span class="params">(<span class="number">0.9</span>, <span class="number">0.05</span>, <span class="number">0.05</span>)</span>, noise_scale=<span class="number">0.05</span>)</span>:</span></span><br><span class="line">    x = np.random.uniform(low=<span class="number">-1.</span>,high=<span class="number">1.</span>,size=size).reshape([<span class="number">-1</span>,<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># g_fns should be numpy functions which support broadcasting. </span></span><br><span class="line">    x_tilde = np.hstack([fn(x) <span class="keyword">for</span> fn <span class="keyword">in</span> g_fns])</span><br><span class="line">    noise = noise_scale * np.random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=x_tilde.shape)</span><br><span class="line">    x_tilde += noise</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># split the data</span></span><br><span class="line">    unsup_x = x_tilde[:int(split[<span class="number">0</span>]*size),:]</span><br><span class="line">    </span><br><span class="line">    sup_x = x_tilde[int(split[<span class="number">0</span>]*size):int((<span class="number">1</span>-split[<span class="number">2</span>])*size),:]</span><br><span class="line">    sup_y = x[int(split[<span class="number">0</span>]*size):int((<span class="number">1</span>-split[<span class="number">2</span>])*size),:]</span><br><span class="line">    </span><br><span class="line">    test_x = x_tilde[int((<span class="number">1</span>-split[<span class="number">2</span>])*size):,:]</span><br><span class="line">    test_y = x[int((<span class="number">1</span>-split[<span class="number">2</span>])*size):,:]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> unsup_x, sup_x, sup_y, test_x, test_y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># some tricks for generating the function list</span></span><br><span class="line">g_fns = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">20</span>):</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(i)</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">j</span><span class="params">(x)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> np.power(x, i<span class="number">-1</span>) - np.power(x, i)</span><br><span class="line">        <span class="keyword">return</span> j</span><br><span class="line">        </span><br><span class="line">    g_fns.append(f(i))</span><br><span class="line"></span><br><span class="line">unsup_x, train_x, train_y, test_x, test_y = gen_data(<span class="number">1000</span>, g_fns=g_fns, noise_scale=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure><p>We use the method introduced above to generate data of 20 dimension to evaluate the performance of the MI maximization method. Since the higher the dimension, the more linear-solvable the data is, we can expect that simple Linear Regression would solve it very well.  Here is the result on different setting of noise. </p><h3 id="Results-on-linear-solvable-data-study-of-noise"><a href="#Results-on-linear-solvable-data-study-of-noise" class="headerlink" title="Results on linear-solvable data - study of noise"></a>Results on linear-solvable data - study of noise</h3><p><img src="https://i.loli.net/2020/05/15/7GNuBsaAPlRmYJO.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/05/15/iuP7U2woYCBMRSt.png" alt="image.png"></p><h3 id="Results-on-non-linear-solvable-data"><a href="#Results-on-non-linear-solvable-data" class="headerlink" title="Results on non-linear-solvable data"></a>Results on non-linear-solvable data</h3><p>Replace $\tilde X$ with non-linearity following:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root_g_fns = [np.sin, np.cos, <span class="keyword">lambda</span> x: np.log(<span class="number">2</span>+x), np.square, np.sinh, np.cosh, np.tanh]</span><br><span class="line"><span class="keyword">for</span> root_f <span class="keyword">in</span> root_g_fns:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">5</span>):</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(i)</span>:</span></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">j</span><span class="params">(x)</span>:</span></span><br><span class="line">                <span class="keyword">return</span> np.sign(root_f(x)) * ((np.abs(root_f(x))) ** (<span class="number">1</span>/(i+<span class="number">1</span>)))</span><br><span class="line">            <span class="keyword">return</span> j</span><br><span class="line">        g_fns.append(f(i))</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/05/15/ceK4HPaXJOErzkM.png" alt="image.png"></p><p>以上的结果是在数据维度为30维左右的，我们继续增大simulated data的维度到130维，我们可以发现，普通的LR模型会显著过拟合，而采用MI无监督学习过后的模型有更好的generalization能力。</p><p><img src="https://i.loli.net/2020/05/15/ul3bJx57C2jGItK.png" alt="image.png"></p><p>因为维度的高低是和数据量共同影响model效果的，所以我们想再探究一下稍微大一些的数据量的表现，如下图所示，我们发现即便数据量是维度的4倍左右（500/133），SL也并没有经过无监督学习过后的模型表现更好。</p><p><img src="https://i.loli.net/2020/05/15/qEpisDlBTxJvzWC.png" alt="image.png"></p><h3 id="Results-on-shifted-and-scaled-unsupervised-data-study-of-dimensionality"><a href="#Results-on-shifted-and-scaled-unsupervised-data-study-of-dimensionality" class="headerlink" title="Results on shifted and scaled unsupervised data - study of dimensionality"></a>Results on shifted and scaled unsupervised data - study of dimensionality</h3><p>Now all the data points are drawn from the exact same distribution, it’s time to check how the algorithm behave when there is <strong>shift/scale</strong> of the unsupervised data distribution. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_data</span><span class="params">(size, g_fns, split=<span class="params">(<span class="number">0.9</span>, <span class="number">0.05</span>, <span class="number">0.05</span>)</span>, noise_scale=<span class="number">0.05</span>, unsup_scale=<span class="params">(<span class="number">1.</span>, <span class="number">1.5</span>)</span>, unsup_shift=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="comment"># ... omitted. </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># split the data</span></span><br><span class="line">    unsup_x = x_tilde[:int(split[<span class="number">0</span>]*size),:]</span><br><span class="line">    <span class="comment"># add some distribution shift/scale to the unsupervised dataset</span></span><br><span class="line">    unsup_scale = np.random.uniform(low=unsup_scale[<span class="number">0</span>], high=unsup_scale[<span class="number">1</span>], size=unsup_x.shape[<span class="number">0</span>]).reshape([<span class="number">-1</span>,<span class="number">1</span>])</span><br><span class="line">    unsup_shift = np.random.normal(loc=unsup_shift, scale=<span class="number">0.05</span>)</span><br><span class="line">    unsup_x = unsup_scale * unsup_x + unsup_shift</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ... omitted.</span></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/05/15/g9EthkTW1O46XCe.png" alt="image.png"></p><h2 id="Analysis-of-the-results"><a href="#Analysis-of-the-results" class="headerlink" title="Analysis of the results"></a>Analysis of the results</h2><p>The result of which shows that the unsupervised learning via MI maximization is more stable across different data-noisy scearios including: </p><ul><li>small size of training data</li><li>scale/shift in unsupervised data</li><li>better performance with fewer parameters</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://gtas.unican.es/files/docencia/TICC/apuntes/tema1bwp_0.pdf&quot; target=&quot;_blank&quot; rel=&quot;
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
  </entry>
  
  <entry>
    <title>Bias-Variance Tradeoff of the Learning Algorithms</title>
    <link href="http://www.shihaizhou.com/2020/05/13/Bias-Variance-Tradeoff-of-the-Learning-Algorithms/"/>
    <id>http://www.shihaizhou.com/2020/05/13/Bias-Variance-Tradeoff-of-the-Learning-Algorithms/</id>
    <published>2020-05-13T09:57:25.000Z</published>
    <updated>2020-05-13T10:10:33.234Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://www.youtube.com/watch?v=KtNwjbWbnh8" target="_blank" rel="noopener">youtube demonstration</a></li><li><a href="http://www.inf.ed.ac.uk/teaching/courses/mlsc/Notes/Lecture4/BiasVariance.pdf" target="_blank" rel="noopener">course note mlsc</a></li></ul><h1 id="Bias-Variance-Tradeoff"><a href="#Bias-Variance-Tradeoff" class="headerlink" title="Bias-Variance Tradeoff"></a>Bias-Variance Tradeoff</h1><p>First recall the <strong>variance</strong> definition.</p><script type="math/tex; mode=display">\operatorname{var}(X) = \mathbb E[X^2] - \mathbb E[X]^2</script><p>Substitute random variable $X$ with $\hat \theta - \theta$, where $\hat \theta$ represents the estimate of the parameters given a certain architecture of the learning algorithm, and $\theta$ stands for the oracle optimal parameter setting. </p><script type="math/tex; mode=display">\operatorname{var}(\hat\theta-\theta) = \mathbb E[(\hat\theta-\theta)^2] -  [\mathbb E(\hat\theta-\theta)]^2</script><p>Since $\theta$ is constant, </p><script type="math/tex; mode=display">\operatorname{var}(\hat\theta-\theta) = \operatorname{var}(\hat\theta)</script><p>The second term is the definition of the mean squared error (MSE), </p><script type="math/tex; mode=display">\operatorname{mse}(\hat\theta) = \mathbb E[(\hat\theta-\theta)^2]</script><p>The third term is the definition of the squared bias,</p><script type="math/tex; mode=display">\operatorname{bias}(\hat\theta) = \mathbb E(\hat\theta-\theta)</script><p>Thus by moving the terms, finally we have, </p><script type="math/tex; mode=display">\operatorname{mse}(\hat\theta)= \operatorname{var}(\hat\theta) + \operatorname{bias}(\hat\theta)^2 \\</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=KtNwjbWbnh8&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;youtube demo
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
  </entry>
  
  <entry>
    <title>Divergence Estimation Framework</title>
    <link href="http://www.shihaizhou.com/2020/05/12/Divergence-Estimation-Framework/"/>
    <id>http://www.shihaizhou.com/2020/05/12/Divergence-Estimation-Framework/</id>
    <published>2020-05-12T06:15:19.000Z</published>
    <updated>2020-05-12T06:22:00.320Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/pdf/1606.00709.pdf" target="_blank" rel="noopener">paper “fGAN: Training Generative Neural Samplers using Variational Divergence Minimization”</a></li><li><a href="https://arxiv.org/abs/0809.0853" target="_blank" rel="noopener">paper “Estimating divergence functionals and the likelihood ratio by convex risk minimization”</a></li></ul><h2 id="The-f-divergence-Family"><a href="#The-f-divergence-Family" class="headerlink" title="The f-divergence Family"></a>The f-divergence Family</h2><p>A large class of different divergences are the so called <strong>f-divergences</strong>, also known as the Ali-Silvey distances. Given two distributions $P$ and $Q$ that possess, respectively, an absolutely continuous density function $p$ and $q$ with respect to a base measure $dx$ defined on the domain $\mathcal{X}$ , we define the f-divergence:</p><script type="math/tex; mode=display">D_{f}(P \| Q)=\int_{\mathcal{X}} q(x) f\left(\frac{p(x)}{q(x)}\right) \mathrm{d} x</script><h2 id="Jensen-Shannon-Divergence"><a href="#Jensen-Shannon-Divergence" class="headerlink" title="Jensen-Shannon Divergence"></a>Jensen-Shannon Divergence</h2><p><strong>Definition</strong> (Jensen-Shannon Divergence) <strong>The Jensen–Shannon divergence (JSD)</strong> is a symmetrized and smoothed version of the Kullback–Leibler divergence $D(P|Q)$. It is defined by: </p><script type="math/tex; mode=display">\operatorname{JSD}(P \| Q)=\frac{1}{2} D(P \| M)+\frac{1}{2} D(Q \| M)</script><p>where $M=\frac{1}{2}(P+Q)$. </p><h3 id="Bound"><a href="#Bound" class="headerlink" title="Bound"></a>Bound</h3><p>The Jensen–Shannon divergence is bounded by 1 for two probability distributions, given that one uses the base 2 logarithm.</p><script type="math/tex; mode=display">0 \leq \mathrm{JSD}(P \| Q) \leq 1</script><h2 id="Reconstruction-based-Autoencoder-Related"><a href="#Reconstruction-based-Autoencoder-Related" class="headerlink" title="Reconstruction-based Autoencoder Related"></a>Reconstruction-based Autoencoder Related</h2><p>In generative models that rely on reconstruction (e.g., denoising, variational, and adversarial autoencoders), the reconstruction error can be related to the <strong>Mutual Information</strong> as follows:</p><script type="math/tex; mode=display">\mathcal{I}_{e}(X, Y)=\mathcal{H}_{e}(X)-\mathcal{H}_{e}(X | Y) \geq \mathcal{H}_{e}(X)-\mathcal{R}_{e, d}(X | Y)</script><p>where:</p><ul><li>$X$ and $Y$ denote the input and output of an encoder.</li><li>$\mathcal{H}_{e}(X)$ and $\mathcal{H}_{e}(X|Y)$ denote the marginal and conditional entropy of $X$ in the distribution formed<br>by applying the encoder to inputs sampled from the source distribution.</li><li>$\mathcal{R}_{e,d}(X|Y)$ denotes the expected reconstruction error of $X$ given the codes $Y$.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1606.00709.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;paper “fGAN: Traini
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
  </entry>
  
  <entry>
    <title>Mutual Information Maximization - Theory</title>
    <link href="http://www.shihaizhou.com/2020/05/10/MI-Maximization/"/>
    <id>http://www.shihaizhou.com/2020/05/10/MI-Maximization/</id>
    <published>2020-05-10T06:29:18.000Z</published>
    <updated>2020-05-15T04:08:12.298Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence" target="_blank" rel="noopener">Wikipedia JS divergence</a> </li><li><a href="https://arxiv.org/abs/1908.01000" target="_blank" rel="noopener">paper “InfoGraph - Unsupervised and Semi-Supervised Graph-Level Representation Learning via Mutual Information Maximization”</a></li><li><a href="https://arxiv.org/pdf/1801.04062.pdf" target="_blank" rel="noopener">paper “Mutual Information Neural Estimation”</a></li><li><a href="https://arxiv.org/abs/1808.06670" target="_blank" rel="noopener">paper “Learning Deep Representations by Mutual Information Estimation and Maximization”</a></li><li><a href="https://fleuret.org/files/complement-slides-MI-estimator.pdf" target="_blank" rel="noopener">slides on MI estimation</a></li><li><a href="https://arxiv.org/abs/1907.13625" target="_blank" rel="noopener">paper “On Mutual Information Maximization for Representation Learning”</a></li></ul><h1 id="MI-Definition"><a href="#MI-Definition" class="headerlink" title="MI Definition"></a>MI Definition</h1><p>Recall the mutual information can be computed by information entropy or <strong>by KL divergence</strong>, the latter of which is often used for estimation.</p><script type="math/tex; mode=display">\begin{align}I(X ; Z) &:=H(X)-H(X | Z) \\&=D_{K L}\left(\mathbb{P}_{X Z} \| \mathbb{P}_{X} \otimes \mathbb{P}_{Z}\right)\end{align}</script><h1 id="MI-Estimation"><a href="#MI-Estimation" class="headerlink" title="MI Estimation"></a>MI Estimation</h1><p>When it’s for the discrete variables, mutual information is easy to compute. But when in the continuous case, the standard approach to estimating it is with parametric density models (e.g., Gaussian Distribution), which is unsatisfactory for real-world high-dimension signals. </p><p>Given the fundamental limitations of MI estimation, recent work has focused on deriving lower bounds on MI. Intuitively, these bounds are based on the following idea: If a classifier can accurately distinguish between samples drawn from the joint $p(x, y)$ and those drawn from the product of marginals $p(x)p(y)$, then $X$ and $Y$ have a high MI.</p><h2 id="The-Donsker-Varadhan-representation"><a href="#The-Donsker-Varadhan-representation" class="headerlink" title="The Donsker-Varadhan representation."></a>The Donsker-Varadhan representation.</h2><p>The following theorem gives a representation of the KL divergence. </p><script type="math/tex; mode=display">D_{K L}(\mathbb{P} \| \mathbb{Q})=\sup _{T: \Omega \rightarrow \mathbb{R}} \mathbb{E}_{\mathbb{P}}[T]-\log \left(\mathbb{E}_{\mathbb{Q}}\left[e^{T}\right]\right)</script><p>where the <strong>supremum is taken over all functions</strong> $T$ such that the two expectations are finite. </p><p>挑选出一个使$\mathbb{E}_{\mathbb{P}}[T]-\log \left(\mathbb{E}_{\mathbb{Q}}\left[e^{T}\right]\right)$最大的函数，我们只需要构造一个在每一个自变量取值处都取到函数最大值的最优函数$T$. 从而可以证明这条式子是成立的（在离散情况下）. Donsker-Varadhan representation提供了一个紧的下界，为了estimate KL divergence，我们只需要将这个实值函数$T$参数化，去maximize对应的函数值即可：</p><p>Given pairs of samples $x_n \sim \mathbb P, y_n \sim \mathbb Q, n=1, \cdots, N$. we can estimate the $\sup$ over all mappings with the optimization of a model:</p><script type="math/tex; mode=display">\hat D_{K L}\left(\mu_{X} \| \mu_{Y}\right)=\sup _{w} \frac{1}{N} \sum_{n=1}^{N} f\left(x_{n} ; w\right)-\log \left(\frac{1}{N} \sum_{n=1}^{N} \exp \left(f\left(y_{n} ; w\right)\right)\right)</script><p>For estimating mutual informaiton, the last thing required is to construct joint distribution and marginal product distribution: given pairs of samples i.i.d $\sim \mu_{A,B}$</p><script type="math/tex; mode=display">(a_n, b_n), n =1, \cdots, N</script><p>with $\sigma$ a random permutation (随机排列), we have the marginal product distribution</p><script type="math/tex; mode=display">\left(a_{n}, b_{\sigma(n)}\right), n=1, \ldots, N</script><p>as i.i.d samples $\sim \mu_{A} \otimes \mu_{B}$, from which we have the estimation method as an optimization process: </p><script type="math/tex; mode=display">\hat{I}(A ; B)=\sup _{w} \frac{1}{N} \sum_{n=1}^{N} f\left(a_{n}, b_{n} ; w\right)-\log \left(\frac{1}{N} \sum_{n=1}^{N} \exp \left(f\left(a_{n}, b_{\sigma(n)} ; w\right)\right)\right)</script><h2 id="InfoNCE"><a href="#InfoNCE" class="headerlink" title="InfoNCE"></a>InfoNCE</h2><p>InfoNCE is another MI estimator used in representation learning, which is defined as, </p><script type="math/tex; mode=display">I(X ; Y) \geq \mathbb{E}\left[\frac{1}{K} \sum_{i=1}^{K} \log \frac{e^{f\left(x_{i}, y_{i}\right)}}{\frac{1}{K} \sum_{j=1}^{K} e^{f\left(x_{i}, y_{j}\right)}}\right] \triangleq I_{\mathrm{NCE}}(X ; Y)</script><p>where the expectation is over $K$ independent samples $\left\{\left(x_{i}, y_{i}\right)\right\}_{i=1}^{K}$ from the joint distribution $p(x, y)$.</p><h1 id="MI-Maximization"><a href="#MI-Maximization" class="headerlink" title="MI Maximization"></a>MI Maximization</h1><p>Let $\mathcal X$ and $\mathcal Y$ be the domain and range of a continuous and (almost everywhere) differentiable parametric function, $E_{\psi}: \mathcal{X} \rightarrow \mathcal{Y}$ with parameters $\psi$ (e.g., a neural network). <strong>Mutual Information Maximization</strong> is to: find the set of parameters, $\psi$, such that the mutual information, $\mathcal{I}\left(X ; E_{\psi}(X)\right)$, is maximized. This process is often used for finding representations for a given dataset. (This part refers to <a href="https://arxiv.org/abs/1808.06670" target="_blank" rel="noopener">“Learning Deep Representations by Mutual Information Estimation and Maximization”</a>)</p><p><strong>InfoMax</strong> finds that exact KL-based formulation of MI is not necessary, for example, a simple alternative based on the Jensen-Shannon divergence (JSD) is more stable and provides better results. At a high level, we optimize $E_{\psi}$ by <strong>simultaneously estimating and maximizing</strong> $\mathcal{I}\left(X ; E_{\psi}(X)\right)$, where $G$ represents global maximization: </p><script type="math/tex; mode=display">(\hat{\omega}, \hat{\psi})_{G}=\underset{\omega, \psi}{\arg \max } \widehat{\mathcal I}_{\omega}\left(X ; E_{\psi}(X)\right)</script><p>根据fGAN中提出的框架，我们可以构造一个Jensen-Shannon MI Estimator: </p><script type="math/tex; mode=display">\widehat{\mathcal{I}}_{\omega, \psi}^{(\mathrm{JSD})}\left(X ; E_{\psi}(X)\right):=\mathbb{E}_{\mathbb{P}}\left[-\operatorname{sp}\left(-T_{\psi, \omega}\left(x, E_{\psi}(x)\right)\right)\right]-\mathbb{E}_{\mathbb{P} \times \tilde{\mathbb{P}}}\left[\operatorname{sp}\left(T_{\psi, \omega}\left(x^{\prime}, E_{\psi}(x)\right)\right)\right]</script><h2 id="Global-Local-MI-Maximization"><a href="#Global-Local-MI-Maximization" class="headerlink" title="Global/Local-MI Maximization"></a>Global/Local-MI Maximization</h2><p>The objective above can be used to maximize MI between input and output, but ultimately this may be undesirable depending on the tasks. Suppose the intermediate representation of the input preserves the characteristics of locality, which is represented as $C_{\psi}(x):=\left\{C_{\psi}^{(i)}\right\}_{i=1}^{M}$ , and $E_{\psi}(x)=f_{\psi} \circ C_{\psi}(x)$. Define our MI estimator on global/local pairs, maximizing the average estimated MI:</p><script type="math/tex; mode=display">(\hat{\omega}, \hat{\psi})_{L}=\underset{\omega, \psi}{\arg \max } \frac{1}{M} \sum_{i=1}^{M} \widehat{I}_{\omega, \psi}\left(C_{\psi}^{(i)}(X) ; E_{\psi}(X)\right)</script><h2 id="Multiview-MI-Maximization"><a href="#Multiview-MI-Maximization" class="headerlink" title="Multiview-MI Maximization"></a>Multiview-MI Maximization</h2><p><strong>Multiview-MI Maximization includes various objectives among which the MI is computed into one unified framework.</strong> (refer to <a href="https://arxiv.org/abs/1907.13625" target="_blank" rel="noopener">paper “On Mutual Information Maximization for Representation Learning”</a>)</p><p>In the image classification setting, for a given image $X$, let $X^{(1)}$ and $X^{(2)}$ be different, possibly overlapping views of $X$, for instance the top and bottom halves of the image. These are encoded using encoders $g_1$ and $g_2$ respectively, and the MI between the two representationsg $g_1(X^{(1)})$ and $g_2(X^{(2)})$ is maximized, </p><script type="math/tex; mode=display">\max _{g_{1} \in \mathcal{G}_{1}, g_{2} \in \mathcal{G}_{2}} \quad \hat I\left(g_{1}\left(X^{(1)}\right) ; g_{2}\left(X^{(2)}\right)\right)</script><p>where $\hat I$ represents the sample based MI estimator of the true MI $I(X;Y)$ and the function classes $\mathcal{G}_{1}$ and $\mathcal{G}_{2}$ can be used to specify structural constraints on the encoders. One thing to note here is that two encoders $g_1$ and $g_2$ often share parameters. </p><p>It gives us plenty of modeling flexibility, as the two <strong>views</strong> can be chosen to capture completely different aspects and modalities of the data, for example:</p><ul><li>In the basic form of <strong>DeepInfoMax</strong> $g_1$ extracts global features from the entire image $X^{(1)}$ and $g_2$ local features from image patches $X^{(2)}$, where $g_1$ and $g_2$ correspond to activations in different layers of the same convolutional network. (也就是上一节介绍的global/local-MI Maximization)</li><li><strong>Contrastive multiview coding (CMC)</strong> generalizes the objective in to consider multiple views $X^{(i)}$, where each $X^{(i)}$ corresponds to a different image modality (e.g., different color channels, or the image and its segmentation mask).</li><li><strong>Contrastive predictive coding(CPC)</strong> incorporates a sequential component of the data. Concretely, one extracts a sequence of patches from an image in some fixed order, maps each patch using an encoder, aggregates the resulting features of the first $t$ patches into a context vector, and maximizes the MI between the context and features extracted from the patch at position $t + k.$</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence&quot; target=&quot;_blank&quot; rel=&quot;no
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
  </entry>
  
  <entry>
    <title>InfoGraph - USL and Semi-SL Graph-Level Representation Learning</title>
    <link href="http://www.shihaizhou.com/2020/05/09/InfoGraph-USL-and-Semi-SL-Graph-Level-Representation-Learning/"/>
    <id>http://www.shihaizhou.com/2020/05/09/InfoGraph-USL-and-Semi-SL-Graph-Level-Representation-Learning/</id>
    <published>2020-05-09T09:33:57.000Z</published>
    <updated>2020-05-11T09:20:08.532Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/abs/1908.01000" target="_blank" rel="noopener">paper “InfoGraph - Unsupervised and Semi-Supervised Graph-Level Representation Learning via Mutual Information Maximization”</a></li><li><a href="https://openreview.net/forum?id=r1lfF2NYvH" target="_blank" rel="noopener">openreview of the paper</a></li><li><a href="https://shihaizhou.com/2020/02/17/Information-Theory-Simple-Concepts/" target="_blank" rel="noopener">my blog about JS divergence</a></li><li><a href="https://arxiv.org/abs/1808.06670" target="_blank" rel="noopener">paper “Learning Deep Representations by Mutual Information Estimation and Maximization”</a></li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>Kernel-based methods没有generalization的能力，同时无监督和半监督的setting非常promising。We maximize the <strong>mutual information</strong> between the graph-level representation and the representations of substructures of different scales (e.g., nodes, edges, triangles).</p><h2 id="Logic-of-Introduction"><a href="#Logic-of-Introduction" class="headerlink" title="Logic of Introduction"></a>Logic of Introduction</h2><ul><li>Graph is important, providing diverse structured data. </li><li>Representation learning of the entire graph is a rising field of the community.</li><li>Extant methods are mostly supervised, which is bad since labeling data is costly. <ul><li>one way to solve it is through semi-supervised learning.</li><li>or better, by unsupervised learning. </li></ul></li><li>Introducing representation learning for graphs. They are not to one’s satisfaction since:<ul><li>many don’t provide graph embedding explicitly.</li><li>kernels are handcrafted. </li></ul></li><li>Inspired by the method of maximizing mutual information, we propose InfoGraph.</li><li>Our contribution. </li></ul><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul><li>The <strong>graph kernel</strong> $K(G1, G2)$ is defined based on the frequency of each sub-structure appearing in $G_1$ and $G_2$ respectively. Namely, $K(G_1, G_2) = \left<f_{g_{s1}} , f_{g_{s2}} \right>$, where $f_{G_{s}} $ is the vector containing frequencies of $\{G_s\}$ sub-structures, and $\left&lt; , \right&gt;$ is an inner product in an appropriately normalized vector space.</f_{g_{s1}}></li><li>An important approach for unsupervised representation learning is to train an encoder to be <strong>contrastive</strong> between representations that capture statistical dependencies of interest and those that do not.</li><li><strong>Mean Teacher</strong> adds a loss term which encourages the distance between the original network’s output and the teacher’s output to be small. The teacher’s predictions are made using an exponential moving average of parameters from previous training steps.</li></ul><h1 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h1><h2 id="Problem-Definition"><a href="#Problem-Definition" class="headerlink" title="Problem Definition"></a>Problem Definition</h2><p><img src="https://i.loli.net/2020/05/10/EBzLAF1S7uOo2iG.png" alt="image.png"></p><p>该工作的Unsupervised Learning是属于transductive representation learning. 也就是说在遇到一个新的graph时没有办法生成该graph的representation. 而Semi-supervised Learning更符合inductive learning的模式：也就是学习一个模型，在遇到新的未标注数据时可以替他们生成embedding，和之前的graph-grraph proximity比较起来相对弱了一些。</p><h2 id="InfoGraph"><a href="#InfoGraph" class="headerlink" title="InfoGraph"></a>InfoGraph</h2><p>利用GNN不断aggregate neighboring node features的特性，文章将这些feature称作“patch representations”，同时GNN采用Readout function将patch representations映射成一个fixed-length graph-level representation：</p><script type="math/tex; mode=display">h_{v}^{(k)}=\mathrm{COMBINE}^{(k)}\left(h_{v}^{(k-1)}, \mathrm{AGGREGATE}^{(k)}\left(\left\{\left(h_{v}^{(k-1)}, h_{u}^{(k-1)}, e_{u v}\right): u \in \mathcal{N}(v)\right\}\right)\right)</script><p>通过K层GNN，可以将每一层的representation concat起来，这样就得到了“of different scales”的representation：</p><script type="math/tex; mode=display">\begin{aligned} h_{\phi}^{i} &=\operatorname{CONCAT}\left(\left\{h_{i}^{(k)}\right\}_{k=1}^{K}\right) \\ H_{\phi}(G) &=\operatorname{READOUT}\left(\left\{h_{\phi}^{i}\right\}_{i=1}^{N}\right) \end{aligned}</script><p>然后定义MI estimator on global/local pairs, maximizing the estimated MI over the given dataset $\mathbf{G}:=\left\{G_{j} \right\}_{j=1}^{N}$:</p><script type="math/tex; mode=display">\hat{\phi}, \hat{\psi}=\underset{\phi, \psi}{\arg \max } \sum_{G \in \mathbf{G}} \frac{1}{|G|} \sum_{u \in G} I_{\phi, \psi}\left(\vec{h}_{\phi}^{u} ; H_{\phi}(G)\right)</script><p>同时采用Jensen-Shannon MI estimator:</p><script type="math/tex; mode=display">\begin{aligned} I_{\phi, \psi}\left(h_{\phi}^{i}(G) ; H_{\phi}(G)\right) &:= \mathbb{E}_{\mathbb{P}}\left[-\operatorname{sp}\left(-T_{\phi, \psi}\left(\vec{h}_{\phi}^{i}(x), H_{\phi}(x)\right)\right)\right]-\mathbb{E}_{\mathbb{P} \times \tilde{\mathbb{P}}}\left[\operatorname{sp}\left(T_{\phi, \psi}\left(\vec{h}_{\phi}^{i}\left(x^{\prime}\right), H_{\phi}(x)\right)\right)\right] \end{aligned}</script><p>In practice, we generate negative samples using all possible combinations of global and local patch representations across all graph instances in a batch.</p><p><img src="https://i.loli.net/2020/05/11/X5hxKHUBI3ok8Mi.png" alt="image.png"></p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>For classification, 6 commonly used datasets.</p><ul><li>MUTAG</li><li>PTC</li><li>REDDIT-BINARY</li><li>REDDIT-MULTI-5K</li><li>IMDB-BINARY</li><li>IMDB-MULTI </li></ul><p>For semi-supervised setting, QM9 dataset. </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1908.01000&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;paper “InfoGraph - Unsu
      
    
    </summary>
    
    
      <category term="GNN" scheme="http://www.shihaizhou.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Graph-Level Representation Learning via Graph-Graph Proximity</title>
    <link href="http://www.shihaizhou.com/2020/05/08/Graph-Level-Representation-Learning-via-Graph-Graph-Proximity/"/>
    <id>http://www.shihaizhou.com/2020/05/08/Graph-Level-Representation-Learning-via-Graph-Graph-Proximity/</id>
    <published>2020-05-08T06:13:36.000Z</published>
    <updated>2020-05-10T06:24:34.936Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/abs/1904.01098" target="_blank" rel="noopener">paper “Unsupervised Inductive Graph-Level Representation Learning via Graph-Graph Proximity”</a></li><li><a href="https://www.quora.com/What-is-the-difference-between-inductive-and-transductive-learning" target="_blank" rel="noopener">quora “Difference between transductive/inductive learning - post of Ramashish Gaurav”</a></li></ul><p><strong>Transductive/inductive Learning</strong></p><ul><li>Transductive learning 是可以提前见到unlabelled test set并且利用这些数据来达到更好的表现。</li><li>Inductive learning 则完全看不见test data，需要在training data上训练出一个模型，将这个模型运用到test data得到结果。</li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>该工作提出了一个general framework从而使得在unsupervised data上能够做graph-level inductive embedding. 文章称“当前工作一般都隐性地假设了好的node embedding能够导致好的graph embedding”，但这只考虑了图内信息而没有考虑图间信息（intra-graph information v.s. inter-graph information），因此需要进一步改进，在最近关于graph proximity modeling工作的基础上，文章提出了一个framework: UGraphEMB. 因为保留了图拓扑结构的信息，所以在一些基于图的下游任务（classification, similarity ranking, visualization）中有比较好的效果。 </p><h1 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h1><p>在graph-level embedding生成过程中，需要首先生成node embedding，随后需要将node embedding通过聚合方式得到graph embedding，那么在这个过程中我们给一个人为的训练目标：也就是graph proximity在映射结束之后还是能够和原先的proximity保持一致，这部分预训练结束之后就可以将model运用到下游任务中。整个过程如下图所示。</p><p><img src="https://i.loli.net/2020/05/08/mBF658PQ3EJHfIX.png" alt="image.png"></p><h2 id="Node-Embedding-Generation"><a href="#Node-Embedding-Generation" class="headerlink" title="Node Embedding Generation"></a>Node Embedding Generation</h2><p>首先在Node Embedding的部分，文章采用了GIN。虽然没有在文章中明确说GIN是能够最大程度地保留图结构信息的aggregation function，但是作者直接提到使用这个模型是因为这个模型是那个时候的SOTA模型，GIN的聚合函数如下：</p><script type="math/tex; mode=display">\boldsymbol{u}_{\boldsymbol{i}}^{(k)}=\operatorname{MLP}_{\boldsymbol{W}_{\boldsymbol{k}}}^{(k)}\left(\left(1+\epsilon^{(k)}\right) \cdot \boldsymbol{u}_{\boldsymbol{i}}^{(k-1)}+\sum_{j \in \mathcal{N}(i)} \boldsymbol{u}_{j}^{(k-1)}\right)</script><h2 id="Graph-Embedding-Generation"><a href="#Graph-Embedding-Generation" class="headerlink" title="Graph Embedding Generation"></a>Graph Embedding Generation</h2><p>作者在这里表示，虽然Graph Embedding Generation可以使用一些非常简单的aggregation function，但是他们认为，因为目标是将graph映射到一个保留了他们相似度的空间中，the graph embedding generation model should:</p><ul><li>Capture structural difference <strong>at multiple scales.</strong> 文章论述这一点：”<strong>However, after many neighbor aggregation layers, the learned embeddings could be too coarse to capture the structural difference in small local regions between two similar graphs.</strong>“ </li><li>Be adaptive to different graph proximity metrics. 这一点要求aggregation中必须要含有可学习的参数。</li></ul><p>从而本文提出了一个新的Multi-Scale Node Attention (MSNA) mechanism. 首先用 $U_{\mathcal{G}} \in \mathbb{R}^{N \times D}$ 来表示graph $\mathcal{G}$ 的input node embeddings. The graph level embedding is obtained as follows:</p><script type="math/tex; mode=display">\boldsymbol{h}_{\mathcal{G}}=\operatorname{MLP}_{\boldsymbol{W}}\left(\|_{k=1}^{K} \operatorname{ATT}_{\boldsymbol{\Theta}^{(k)}}\left(\boldsymbol{U}_{\boldsymbol{G}}\right)\right)</script><p>Instead of only using the node embeddings generated by the last neighbor aggregation layer, we use the node embeddings generated by each of the K neighbor aggregation layers.</p><p>第K层的ATT函数定义式如下：</p><script type="math/tex; mode=display">\operatorname{ATT}_{\Theta}\left(\boldsymbol{U}_{\boldsymbol{G}}\right)=\sum_{n=1}^{N} \sigma\left(\boldsymbol{u}_{\boldsymbol{n}}^{T} \operatorname{ReLU}\left(\boldsymbol{\Theta}\left(\frac{1}{N} \sum_{m=1}^{N} \boldsymbol{u}_{\boldsymbol{m}}\right)\right)\right) \boldsymbol{u}_{\boldsymbol{n}}</script><p>其中$\Theta$表示第K层的参数，从而during the genera-tion of graph-level embeddings, the attention weight assigned to each node should be adaptive to the graph proximity metric. </p><h2 id="Graph-Proximity"><a href="#Graph-Proximity" class="headerlink" title="Graph Proximity"></a>Graph Proximity</h2><p>这一层简单介绍了可以用作Unsupervised Representation Learning的优化函数的graph-graph proximity，常见的有GED (Graph Edit Distance) 和 MCS (Maximum Common Subgraph) 。本文的描述非常严谨：“In this paper, we used GED as an example metric to demonstrate UGraphEMB. ”图编辑距离指的是对图进行编辑的最少步数转变成另一个图，其中<strong>一次编辑指的是”an insertion/deletion of a node/edge, or relabelling a node.”</strong></p><p><img src="https://i.loli.net/2020/05/08/AuPjLzWXGqa4Mg8.png" alt="image.png"></p><p>为了在训练时保留高维数据点（Graph）的两两距离，文章采用了<strong>Multidimensional Scaling (MDS)</strong>的数据降维方法，这个方法的核心想法是将数据集中的点map到低维空间中，同时保留数据点两两之间的距离。这一点是通过最小化以下目标函数达到的：</p><script type="math/tex; mode=display">\mathcal{L}\left(\boldsymbol{h}_{\boldsymbol{i}}, \boldsymbol{h}_{\boldsymbol{j}}, d_{i j}\right)=\left(\left\|\boldsymbol{h}_{\boldsymbol{i}}-\boldsymbol{h}_{\boldsymbol{j}}\right\|_{2}^{2}-d_{i j}\right)^{2}</script><p>用采样+数学期望的方式来表达这个函数：</p><script type="math/tex; mode=display">\begin{aligned} \mathcal{L} &=\mathbb{E}_{(i, j) \sim \mathcal{D}}\left(\hat{d}_{i j}-d_{i j}\right)^{2} \\ &=\mathbb{E}_{(i, j) \sim \mathcal{D}}\left(\left\|\boldsymbol{h}_{\mathcal{G}_{i}}-\boldsymbol{h}_{\mathcal{G}_{j}}\right\|_{2}^{2}-d_{i j}\right)^{2} \end{aligned}</script><p>如果是图之间的距离是它们的相似度，那么我们可以直接用内积的形式来计算两个图向量的距离：</p><script type="math/tex; mode=display">\begin{aligned} \mathcal{L} &=\mathbb{E}_{(i, j) \sim \mathcal{D}}\left(\hat{s_{i j}}-s_{i j}\right)^{2} \\ &=\mathbb{E}_{(i, j) \sim \mathcal{D}}\left(\boldsymbol{h}_{\mathcal{G}_{i}}^{T} \boldsymbol{h}_{\mathcal{G}_{j}}-s_{i j}\right)^{2} \end{aligned}</script><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>文章在3个任务的5个数据集上进行了该方法验证。对于这个预训练方法，作为读者我们想要问的问题是：</p><ul><li>pre-trained graph embeddings 能给下游任务带来多少提升？是否能够加快下游任务的训练速度？</li><li>和当前的graph representation learning algorithms相比是否有显著的提升？</li><li>Ablation Study需要涵盖：<ul><li>不同的Node Embedding Module有什么差别？</li><li>如果不采用Multihead-Attention机制会有怎样的变化？</li></ul></li><li>数据集中有没有一些Same structure, Disparate meaning的graph-pair？尤其对于小图来说？这样来说graph-graph proximity是不是就不make sense了？</li></ul><h2 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h2><p>汇报了classification accuracy，如下表。</p><p><img src="https://i.loli.net/2020/05/08/eRxvcrZuQgpUIlk.png" alt="image.png"></p><h2 id="Similarity-Ranking"><a href="#Similarity-Ranking" class="headerlink" title="Similarity Ranking"></a>Similarity Ranking</h2><p>汇报了Kendall’s Rank Correlation Coefficient ($\tau$) 和 precision at 10 (p@10)，如下表。</p><p><img src="https://i.loli.net/2020/05/08/lyoYXcbgRMAHhz9.png" alt="image.png"></p><h2 id="Ebedding-Visualization"><a href="#Ebedding-Visualization" class="headerlink" title="Ebedding Visualization"></a>Ebedding Visualization</h2><p>将graph embedding learned by all the methods into the visualization tool t-SNE, 得到了以下可视化结果，拥有相同的子图结构的图被更好地分在了同一个cluster中，这大概率是使用graph-graph proximity的结果。</p><p><img src="https://i.loli.net/2020/05/08/cbWoBjJ6sh7UCiY.png" alt="image.png"></p><h2 id="MSNA-Ablation-Study"><a href="#MSNA-Ablation-Study" class="headerlink" title="MSNA Ablation Study"></a>MSNA Ablation Study</h2><p><img src="https://i.loli.net/2020/05/08/9EM5OvqeH4Flkjx.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1904.01098&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;paper “Unsupervised Ind
      
    
    </summary>
    
    
      <category term="GNN" scheme="http://www.shihaizhou.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>BERT</title>
    <link href="http://www.shihaizhou.com/2020/05/03/BERT/"/>
    <id>http://www.shihaizhou.com/2020/05/03/BERT/</id>
    <published>2020-05-03T13:35:18.000Z</published>
    <updated>2020-05-09T05:33:02.597Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">paper “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”</a></li><li><a href="https://www.youtube.com/watch?v=OR0wfP2FD3c" target="_blank" rel="noopener">youtube “BERT Explained”</a></li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. <strong>As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks.</strong> </p><p>预训练模型在sentence-level和token-level tasks上都有很不错的表现。现存的预训练方法可以分为feature-based和fine-tuning：</p><ul><li>ELMo作为前者 uses task-specific architectures that include the pre-trained representations as additional features. </li><li>The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pretrained parameters.</li></ul><p>The two approaches share the same objective function during pre-training, <strong>where they use unidirectional language models to learn general language representations.</strong></p><p>BERT采用“<strong>masked language model</strong>” (MLM) pre-training objective. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. </p><ul><li>We showed the importance of the bidirectional pre-training for language representations. </li><li>We show that pre-trained representations reduce the need for many heavily-engineered task-specific architectures </li><li>BERT advances the state of the art for <strong>eleven NLP tasks.</strong> </li></ul><h1 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h1><p>BERT’s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation. </p><h2 id="Input-Output-Representations"><a href="#Input-Output-Representations" class="headerlink" title="Input/Output Representations"></a>Input/Output Representations</h2><p>因为BERT是一种预训练模型，所以需要匹配各种不同形式的输入输出。在BERT的训练中，一个sentence不是一个语义上的sentence，而只需要是arbitrary span of contiguous text. 而一个sequence可以是一个sentence也可以是两个放置在一起的sentence（例如在QA问题中的Question-Answer pair需要用到两个并置的sentence）. </p><ul><li>The first token of every sequence is always a special classification token ([CLS]). </li><li>Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. <ul><li>We separate them with a special token ([SEP]). </li><li>We add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. </li><li>最终，将最后一层[CLS] token得到的hidden state标记为C，其余中间结果的final hidden states标记为$T_i \in \mathbb R^H$. 见下图左边pre-training部分。</li></ul></li></ul><p><img src="https://i.loli.net/2020/05/08/I14vnXO6uJtpiBM.png" alt="image.png"></p><h2 id="Pre-training-BERT"><a href="#Pre-training-BERT" class="headerlink" title="Pre-training BERT"></a>Pre-training BERT</h2><p>采用两个Unsupervised Tasks对BERT进行训练。</p><h3 id="Task-1-Masked-LM"><a href="#Task-1-Masked-LM" class="headerlink" title="Task #1: Masked LM"></a>Task #1: Masked LM</h3><p>In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a “masked LM” (MLM), although it is often referred to as a <strong>Cloze</strong> task in the literature. 在BERT中，被mask掉的位置会送入一个mask token，然后在输出层输出softmax的概率分布。训练时BERT采取的是随机mask掉15%的token，但是这样会造成所谓”mismatch”：因为mask token不会在fine-tuning阶段出现。为了缓解这个问题，文章将已经选中的token再进行：</p><ul><li>80%几率真实替换为mask token</li><li>10%几率替换为random token</li><li>10%几率保留原token</li></ul><h3 id="Task-2-Next-Sentence-Prediction-NSP"><a href="#Task-2-Next-Sentence-Prediction-NSP" class="headerlink" title="Task #2: Next Sentence Prediction (NSP)"></a>Task #2: Next Sentence Prediction (NSP)</h3><p>这个任务的动机在于对于很多的任务，例如Question Answering / Natural Language Inference等，需要探究句子之间的关系。选择进入模型的sentence来进行NSP任务时，句子B有50%的概率选择非A的next，同时标注为NonNext，在这个任务下，最后的C就是用来判断是否为next sentence的标志。</p><h3 id="Pre-training-Data"><a href="#Pre-training-Data" class="headerlink" title="Pre-training Data"></a>Pre-training Data</h3><p>一共使用了3个大数据集进行非监督的训练，包括BooksCorpus (800M words), English Wikipedia (2,500M words), Billion Word Benchmark to extract long contiguous sequences. </p><h2 id="Fine-tuning-BERT"><a href="#Fine-tuning-BERT" class="headerlink" title="Fine-tuning BERT"></a>Fine-tuning BERT</h2><p>For each task, we simply plug in the task-specific inputs and outputs into BERT and fine-tune all the parameters end-to-end. (A, B) sentence input可以适应各种样式的input: </p><ul><li>形如QA任务的输入就是question-passage pair</li><li>如果是单一语句输入就是degenerate text-$\empty$ pair</li></ul><p>At the output, the token representations are fed into an output layer for token-level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as entailment or sentiment analysis.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;paper “BERT: Pre-tr
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
      <category term="NLP" scheme="http://www.shihaizhou.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>ELMo - Deep Contextualized Word Representations</title>
    <link href="http://www.shihaizhou.com/2020/05/03/ELMo-Deep-Contextualized-Word-Representations/"/>
    <id>http://www.shihaizhou.com/2020/05/03/ELMo-Deep-Contextualized-Word-Representations/</id>
    <published>2020-05-03T13:25:04.000Z</published>
    <updated>2020-05-04T05:01:18.897Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><p><a href="https://arxiv.org/abs/1802.05365" target="_blank" rel="noopener">paper: Deep Contextualized Word Representations</a></p><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>提出新的deep contextualized word representation，能够同时model：</p><ul><li>Complex characteristics of word use (syntax &amp; semantics)</li><li>how these uses vary across linguistic contexts (polysemy)</li></ul><p>该工作的词表示是一个parameterized function：从一个language model内部状态映射到向量。通过在large corpus上预训练得到的词表示能够在现有的模型中达到更好的提升。We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.</p><h1 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h1><p>Unlike most widely used word embeddings, ELMo word representations are functions of the entire input sentence, as described in this section. </p><h2 id="Bidirectional-Language-Model"><a href="#Bidirectional-Language-Model" class="headerlink" title="Bidirectional Language Model"></a>Bidirectional Language Model</h2><p>将token映射到向量的参数$\Theta_x$以及最后softmax层的参数$\Theta_s$在$L$层的双向LSTM中参数共享的，除此之外前向LSTM和后向LSTM的参数是独立的。Our formulation jointly maximizes the log likelihood of the forward and backward directions:</p><script type="math/tex; mode=display">\begin{array}{l}\sum_{k=1}^{N}\left(\log p\left(t_{k} | t_{1}, \ldots, t_{k-1} ; \Theta_{x}, \overrightarrow{\Theta}_{L S T M}, \Theta_{s}\right)\right. \\ \left.\quad+\log p\left(t_{k} | t_{k+1}, \ldots, t_{N} ; \Theta_{x}, \overleftarrow{\Theta}_{L S T M}, \Theta_{s}\right)\right)\end{array}</script><h2 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h2><p>对于一个token $t_k$，经过L-layer biLM之后会得到$2L+1$层的representation $R_k$：</p><script type="math/tex; mode=display">\begin{aligned} R_{k} &=\left\{\mathbf{x}_{k}, \overrightarrow{\mathbf{h}}_{k, j}, \overleftarrow{\mathbf{h}}_{k, j} | j=1, \ldots, L\right\}\end{aligned}</script><p>For inclusion in a downstream model, ELMo collapses all layers in $R$ into a single vector：</p><script type="math/tex; mode=display">\mathbf{E L M o}_{k}=E\left(R_{k} ; \mathbf{\Theta}_{e}\right)</script><p>可以对于不同的任务学习task-specific weighting：</p><script type="math/tex; mode=display">\mathbf{E} \mathbf{L} \mathbf{M} \mathbf{o}_{k}^{t a s k}=E\left(R_{k} ; \Theta^{t a s k}\right)=\gamma^{t a s k} \sum_{j=0}^{L} s_{j}^{t a s k} \mathbf{h}_{k, j}^{L M}</script><p>其中的$s_j^{task}$是softmax-normalized weights，$\gamma^{task}$能够scale the entire ELMo vector.</p><h2 id="biLMs-for-supervised-NLP-tasks"><a href="#biLMs-for-supervised-NLP-tasks" class="headerlink" title="biLMs for supervised NLP tasks"></a>biLMs for supervised NLP tasks</h2><p>给定一个pre-trained biLM和一个supervised architecture for a target NLP task. 只需要用这个biLM来做一次前向传播就可以拿到一个词L层的context-dependent representation。</p><p>文章概括了普遍的supervised NLP task的解决方式：首先将word token通过预训练得到的词向量（context-independent）进行映射，然后forms context-sensitive representation $h_k$。为了将ELMo加入到supervised model中：</p><ul><li>freeze the weights of the biLM and then </li><li>concatenate the ELMo vector $\mathbf{E L M o}_{k}^{\text {task}}$  with $x_k$ and pass the ELMo enhanced representation $[x_k ; \mathbf{E L M o}_{k}^{\text {task}}]$ into the task RNN. </li></ul><h2 id="Final-Model"><a href="#Final-Model" class="headerlink" title="Final Model"></a>Final Model</h2><ul><li>The final model uses L = 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from the first to second layer.</li><li>The context insensitive type representation uses 2048 character n-gram convolutional filters followed by two highway layers and a linear projection down to a 512 representation.</li></ul><p>In contrast, traditional word embedding methods only provide one layer of representation for tokens in a fixed vocabulary.</p><p>Once pretrained, the biLM can compute representations for any task. In some cases, fine tuning the biLM on domain specific data leads to significant drops in perplexity and an increase in downstream task performance.</p><h1 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h1><p>在以下的任务中测试ELMo的有效性：</p><ul><li><strong>Question Answering</strong></li><li><strong>Textual Entailment</strong>: Textual entailment is the task of determining whether a “hypothesis” is true, given a “premise”.</li><li><strong>Semantic Role Labeling</strong>: A semantic role labeling (SRL) system models the predicate-argument structure of a sentence, and is often described as answering “Who did what to whom”.</li><li><strong>Coreference Resolution</strong>: Coreference resolution is the task of clustering mentions in text that refer to the same underlying real world entities.</li><li><strong>Named Entity Extraction</strong></li><li><strong>Sentiment Analysis</strong></li></ul><p>进行的实验：</p><ul><li>Alternate layer weighting schemes. </li><li>Where to include ELMo? （在output也可以将ELMo和internal state进行连接，而不仅仅是在input上）</li><li>What information is captured by the biLM’s representations? （Intuitively, the biLM must be disambiguating the meaning of words using their context.）</li></ul><p><img src="https://i.loli.net/2020/05/03/miTJoxrI9cVSW1k.png" alt="image.png"></p><ul><li>Sample efficiency (In addition, ELMo-enhanced models use smaller training sets more efficiently than models without ELMo.)</li><li>Visualization of learned weights</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.05365&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;paper: Deep Contextualized Wo
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
      <category term="NLP" scheme="http://www.shihaizhou.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>RSA Algorithm</title>
    <link href="http://www.shihaizhou.com/2020/03/06/RSA-Algorithm/"/>
    <id>http://www.shihaizhou.com/2020/03/06/RSA-Algorithm/</id>
    <published>2020-03-06T06:27:00.000Z</published>
    <updated>2020-03-06T07:19:58.523Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://zh.wikipedia.org/wiki/RSA%E5%8A%A0%E5%AF%86%E6%BC%94%E7%AE%97%E6%B3%95" target="_blank" rel="noopener">RSA wikipedia</a></li><li><a href="https://www.ruanyifeng.com/blog/2013/06/rsa_algorithm_part_one.html" target="_blank" rel="noopener">阮一峰网络日志</a></li><li><a href="https://en.wikipedia.org/wiki/Euler%27s_totient_function" target="_blank" rel="noopener">Euler’s totient function</a></li><li><a href="https://en.wikipedia.org/wiki/Euler%27s_theorem" target="_blank" rel="noopener">Euler’s theorem</a></li></ul><h1 id="Euler’s-totient-function"><a href="#Euler’s-totient-function" class="headerlink" title="Euler’s totient function"></a>Euler’s totient function</h1><p><strong>Definition</strong> (Euler’s totient function). In number theory, <strong>Euler’s totient function</strong> $\phi(n)$ counts the positive integers up to a given integer $n$ that are relatively prime to $n$. The Euler’s totient function is computed by the following formula:</p><script type="math/tex; mode=display">\varphi (n)=n\prod _{p\mid n}\left(1-{\frac {1}{p}}\right)</script><p>where the product is over the distinct prime numbers dividing $n$.</p><h2 id="Derivation"><a href="#Derivation" class="headerlink" title="Derivation"></a>Derivation</h2><p>When $(n=1)$：</p><script type="math/tex; mode=display">\varphi(1)=1</script><p>When $(n=p)$：</p><script type="math/tex; mode=display">\varphi(n)=p-1</script><p>When $(n=p^k)$：</p><script type="math/tex; mode=display">\varphi(n)=p^k-p^{k-1}</script><p>When $(n=p_1p_2)$：</p><script type="math/tex; mode=display">\varphi(n)=\varphi(p_1)\phi(p_2)</script><p>因为任意一个整数$n$都能够表示成$n=p_1^{k_1}p_2^{k_2}\cdots p_m^{k_m}$的形式，所以根据以上四种情况的结论我们有：</p><script type="math/tex; mode=display">\begin{align}\varphi(n) &= \varphi(p_1^{k_1}p_2^{k_2}\cdots p_m^{k_m}) \\&= \varphi(\prod_{i=1}^mp_i^{k_i}) \\&= \prod_{i=1}^m \varphi(p_i^{k_i}) \\&= \prod_{i=1}^m (p_i^{k_i}-p_i^{k_i-1}) \\&= n\prod _{p\mid n}\left(1-{\frac {1}{p}}\right)\end{align}</script><h1 id="Euler’s-theorem"><a href="#Euler’s-theorem" class="headerlink" title="Euler’s theorem"></a>Euler’s theorem</h1><p>In number theory, <strong>Euler’s theorem</strong> (also known as the <strong>Fermat–Euler theorem</strong> or <strong>Euler’s totient theorem</strong>) states that if $n$ and $a$ are coprime positive integers, then</p><script type="math/tex; mode=display">a^{\varphi (n)} \equiv 1 \pmod{n}</script><h1 id="RSA-Algorithm"><a href="#RSA-Algorithm" class="headerlink" title="RSA Algorithm"></a>RSA Algorithm</h1><p>The RSA algorithm involves four steps: <strong>key generation, key distribution, encryption and decryption.</strong></p><p>A basic principle behind RSA is the observation that it is practical to find three very large positive integers $e$, $d$ and $n$ such that with modular exponentiation for all integers $ m$ (with 0 ≤ <em>m</em> &lt; <em>n</em>):</p><script type="math/tex; mode=display">{\displaystyle (m^{e})^{d}\equiv m{\pmod {n}}}</script><p>上面的式子里，$e$ 是进行加密的public key, $d$ 是用来解密的private key。</p><h2 id="Key-generation"><a href="#Key-generation" class="headerlink" title="Key generation"></a>Key generation</h2><p>假设Alice和Bob是需要加密通信的两个主人公。Alice可以通过以下步骤生成public key &amp; private key：</p><ul><li>选取两个大质数 $p$ 和 $q$，计算$N=pq$</li><li>计算 $N$ 的欧拉函数值 $r=\varphi(N)=(p-1)(q-1)$</li><li>找到一个小于 $r$ 并与 $r$ 互质的整数 $e$，同时求出 $e$ 关于 $r$ 的模的逆元，即 $ed \equiv 1 \pmod r$</li><li>将两个大质数 $p, q$ 的记录销毁</li><li>得到公钥$(N,e)$和私钥$(N,d)$。</li><li>将公钥发送给Bob，Alice自己保管私钥不被公开。</li></ul><h2 id="Encryption"><a href="#Encryption" class="headerlink" title="Encryption"></a>Encryption</h2><p>Bob对明文 $m$ 通过公钥$(N,e)$进行加密并发送给Alice：</p><script type="math/tex; mode=display">c\equiv m^e\pmod N</script><h2 id="Decryption"><a href="#Decryption" class="headerlink" title="Decryption"></a>Decryption</h2><p>Alice将收到的密文 $c$ 通过私钥 $(N,d)$ 进行解密：</p><script type="math/tex; mode=display">\begin{align}m&=c^d \\&\equiv m^{ed}\pmod N \\&\equiv m^{1+k\varphi(N)} \pmod N \\&\equiv m \pmod N\end{align}</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://zh.wikipedia.org/wiki/RSA%E5%8A%A0%E5%AF%86%E6%BC%94%E7%AE%97%E6%B3%95&quot; target=&quot;
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
  </entry>
  
  <entry>
    <title>Subgraph Related</title>
    <link href="http://www.shihaizhou.com/2020/03/03/Graph-Problems/"/>
    <id>http://www.shihaizhou.com/2020/03/03/Graph-Problems/</id>
    <published>2020-03-03T15:51:40.000Z</published>
    <updated>2020-03-16T14:15:36.395Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://en.wikipedia.org/wiki/Graph_theory/" target="_blank" rel="noopener">Wikipedia Graph Theory</a></li><li><a href="https://www.annualreviews.org/doi/pdf/10.1146/annurev-ecolsys-102209-144718" target="_blank" rel="noopener">paper “From Graphs to Spatial Graphs”</a></li></ul><h2 id="Subgraph-isomorphism-problem"><a href="#Subgraph-isomorphism-problem" class="headerlink" title="Subgraph isomorphism problem"></a>Subgraph isomorphism problem</h2><p>In theoretical computer science, the <a href="https://en.wikipedia.org/wiki/Subgraph_isomorphism_problem" target="_blank" rel="noopener"><strong>subgraph isomorphism</strong></a> problem is a computational task in which two graphs $G$ and $H$ are given as input, and one must determine whether $G$ contains a subgraph that is isomorphic to $H$. </p><p>One reason to be interested in such a question is that many graph properties are hereditary for subgraphs, which means that a graph has the property if and only if all subgraphs have it too. Unfortunately, finding maximal subgraphs of a certain kind is often an NP-complete problem.</p><h2 id="Induced-subgraph"><a href="#Induced-subgraph" class="headerlink" title="Induced subgraph"></a>Induced subgraph</h2><p>In graph theory, an <a href="https://en.wikipedia.org/wiki/Induced_subgraph" target="_blank" rel="noopener"><strong>induced subgraph</strong></a> of a graph is another graph, formed from a subset of the vertices of the graph and all of the edges connecting pairs of vertices in that subset. </p><p>Formally, let $G = (V, E)$ be any graph, and let $S\in V$ be any subset of vertices of $G$. Then the induced subgraph $G[S]$ is the graph whose vertex set is $S$ and <strong>whose edge set consists of all of the edges in $E$ that have both endpoints in $S$.</strong> The same definition works for undirected graphs, directed graphs, and even multigraphs.</p><h2 id="Edge-contraction"><a href="#Edge-contraction" class="headerlink" title="Edge contraction"></a>Edge contraction</h2><p>In graph theory, an <a href="https://en.wikipedia.org/wiki/Edge_contraction" target="_blank" rel="noopener">edge contraction</a> is an operation which removes an edge from a graph while simultaneously merging the two vertices that it previously joined. Edge contraction is a fundamental operation in the theory of graph minors.</p><p>Contractions are useful in structures where we wish to simplify a graph by identifying vertices that represent essentially equivalent entities. One of the most common examples is the reduction of a general directed graph to an acyclic directed graph by contracting all of the vertices in each strongly connected component. If the relation described by the graph is transitive, no information is lost as long as we label each vertex with the set of labels of the vertices that were contracted to form it.</p><h2 id="Graph-minor"><a href="#Graph-minor" class="headerlink" title="Graph minor"></a>Graph minor</h2><p>In graph theory, an undirected graph $H$ is called a <strong><a href="https://en.wikipedia.org/wiki/Graph_minor" target="_blank" rel="noopener">minor</a></strong> of the graph $G$ if $H$ can be formed from $G$ by deleting edges and vertices and by contracting edges. Many graph properties are hereditary for minors, which means that a graph has a property if and only if all minors have it too. </p><h2 id="Graph-clique"><a href="#Graph-clique" class="headerlink" title="Graph clique"></a>Graph clique</h2><p>In the mathematical area of graph theory, a <strong>clique</strong>is a subset of vertices of an undirected graph such that every two distinct vertices in the clique are adjacent; that is, its induced subgraph is complete.</p><p>A <strong>maximal clique</strong> is a clique that cannot be extended by including one more adjacent vertex, that is, a clique which does not exist exclusively within the vertex set of a larger clique. Some authors define cliques in a way that requires them to be maximal, and use other terminology for complete subgraphs that are not maximal.</p><p>A <strong>maximum clique</strong> of a graph, $G$, is a clique, such that there is no clique with more vertices. Moreover, the <strong>clique number</strong> $\omega (G)$ of a graph $G$ is the number of vertices in a maximum clique in $G$.</p><h2 id="Graphlets"><a href="#Graphlets" class="headerlink" title="Graphlets"></a>Graphlets</h2><p><a href="https://en.wikipedia.org/wiki/Graphlets#Graphlet_degree_vectors_(signatures" target="_blank" rel="noopener"><strong>Graphlets</strong></a>_and_signature_similarities) are small connected non-isomorphic induced subgraphs of a large network. Graphlets differ from <a href="https://en.wikipedia.org/wiki/Network_motif" target="_blank" rel="noopener">network motifs</a>, since they must be induced subgraphs, whereas motifs are partial subgraphs. Graphlets were first introduced by Nataša Pržulj, when they were used as a basis for designing two new highly sensitive measures of network local structural similarities: <strong>the relative graphlet frequency distance (RGF-distance)</strong> and the <strong>graphlet degree distribution agreement (GDD-agreement)</strong>.</p><ul><li>Relative graphlet frequency distance</li><li>Graphlet degree distribution agreement</li><li>Graphlet degree vectors (signatures) and signature similarities</li></ul><h2 id="GRAph-ALigner-GRAAL"><a href="#GRAph-ALigner-GRAAL" class="headerlink" title="GRAph ALigner (GRAAL)"></a>GRAph ALigner (GRAAL)</h2><p><a href="https://en.wikipedia.org/wiki/GRAph_ALigner_(GRAAL" target="_blank" rel="noopener"><strong>GRAaph ALigner (GRAAL)</strong></a>) is an algorithm for global network alignment that is based solely on network topology. GRAAL matches pairs of nodes originating in different networks based on their <a href="https://en.wikipedia.org/wiki/Graphlets#Graphlet_degree_vectors_(signatures" target="_blank" rel="noopener">graphlet degree signature similarities</a>_and_signature_similarities), where a higher similarity between two nodes corresponds to a higher topological similarity between their extended neighborhoods (out to distance 4). GRAAL produces global alignments, i.e., <strong>it aligns each node in the smaller network to exactly one node in the larger network</strong>. The matching proceeds using a technique analogous to the “seed and extend” approach of the popular <a href="https://en.wikipedia.org/wiki/BLAST" target="_blank" rel="noopener">BLAST</a> algorithm for <a href="https://en.wikipedia.org/wiki/Sequence_alignment" target="_blank" rel="noopener">sequence alignment</a>: it first chooses a single “seed” pair of nodes (one node from each network) with high graphlet degree signature similarity. It then expands the alignment radially outward around the seed as far as practical using a greedy algorithm for details).</p><h2 id="How-to-Form-Subgraphs"><a href="#How-to-Form-Subgraphs" class="headerlink" title="How to Form Subgraphs"></a>How to Form Subgraphs</h2><p>From a graph, one can obtain subgraphs, or modules, by removing edges (cut edges) between nodes. Modularity is the divisibility of a graph into highly connected subgraphs with few edges between them. This may reflect the substructure of any spatial graph. Newman (2006) provides an algorithm for dividing a univariate graph into such subgraphs based on the eigenvectors of a characteristic matrix for the graph. This process of dividing the graph into modules is much like spatially constrained clustering (Fortin &amp; Dale 2005), where the most similar nodes are joined into clusters, but only if they are adjacent in space (as defined by some algorithm; see sidebar, How to Join Nodes).</p><h2 id="How-to-Compare-Graph-Structures"><a href="#How-to-Compare-Graph-Structures" class="headerlink" title="How to Compare Graph Structures"></a>How to Compare Graph Structures</h2><p>To compare graph structures, many metrics have been developed (Albert &amp; Baraba ́si 2002, Estrada &amp; Bodin 2008, Freeman 1977, Fortin 1994, Oden et al. 1993, Pascual-Hortal &amp; Saura 2006, Urban &amp; Keitt 2001, Wasserman &amp; Faust 1994) to characterize different graph properties based on nodes [e.g., <strong>number of edges per node, degree of node assortativeness</strong> (Newman 2002), <strong>node importance to overall connectivity, and centrality of a node</strong>], based on edges (e.g., <strong>shortest path, path diameter</strong>), or based on the entire graph [e.g., <strong>diameter of a graph, network centrality metrics</strong> (Wasserman &amp; Faust 1994)]. Each of these metrics assume that all the nodes have been included, and the metrics are sensitive to missing nodes to different degrees depending of the characteristics being measured; <strong>this sensitivity is referred to as the sampling issue</strong> (Clauset et al. 2008, Corson 2010, Lee et al. 2005). This sampling issue creates serious limitations for the interpretation of graph metrics when there is no assurance that <strong>there is a complete census of the objects depicted in the graph</strong>. Further comparisons of these metrics have been conducted by B. Rayfield, M.J. Fortin, and A. Fall (submitted), who proposed a classification framework for them based on the component of connectivity quantified and the structural level (element, neighborhood, cluster, and network) to which they can be applied.</p><h2 id="Graph-of-graphs"><a href="#Graph-of-graphs" class="headerlink" title="Graph of graphs"></a>Graph of graphs</h2><p>A graph of graphs is a spatial graph where each node has its own associated graph representing some other structure (such as a food web). For example, Fortuna &amp; Bascompte (2008) showed a graph of a metacommunity consisting of nodes, each of which contains a graph of multispecies interactions. Melia ́n et al. (2005) compared two complete graphs of five nodes representing habitats, with edges’ thickness depicting the number of shared trophic modules. The node representing each habitat has an associated tritrophic digraph showing a food chain with or without omnivory. As more spatial studies are completed, the number of examples of graphs of graphs will undoubtedly increase.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Graph_theory/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Wikipedia Gr
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
      <category term="GNN" scheme="http://www.shihaizhou.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>How Powerful are GNNs?</title>
    <link href="http://www.shihaizhou.com/2020/02/29/How-Powerful-are-GNNs/"/>
    <id>http://www.shihaizhou.com/2020/02/29/How-Powerful-are-GNNs/</id>
    <published>2020-02-29T05:31:29.000Z</published>
    <updated>2020-03-03T14:34:44.491Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/abs/1810.00826" target="_blank" rel="noopener">paper “How powerful are GNNs?”</a></li><li><a href="https://www.davidbieber.com/post/2019-05-10-weisfeiler-lehman-isomorphism-test/" target="_blank" rel="noopener">blog “WL test”</a></li><li><a href="https://openreview.net/forum?id=ryGs6iA5Km" target="_blank" rel="noopener">open review of the paper</a></li><li><a href="https://www.youtube.com/watch?v=USfNJNePDKQ" target="_blank" rel="noopener">youtube lecture including this paper</a></li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>Despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present <strong>a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures.</strong> We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test.</p><p>Similar to GNNs, the WL test iteratively updates a given node’s feature vector by aggregating feature vectors of its network neighbors. What makes the WL test so powerful is <strong>its injective aggregation update that maps different node neighborhoods to different feature vectors.</strong></p><p><strong>Main contributions</strong></p><ul><li>We show that GNNs are at most as powerful as the WL test in distinguishing graph structures. </li><li>We establish conditions on the neighbor aggregation and graph readout functions under which the resulting GNN is as powerful as the WL test.</li><li>We identify graph structures that cannot be distinguished by popular GNN variants, such as GCN and GraphSAGE, and we precisely characterize the kinds of graph structures such GNN-based models can capture. </li><li>We develop a simple neural architecture, Graph Isomorphism Network (GIN), and show that its discriminative/representational power is equal to the power of the WL test. </li></ul><h1 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h1><h2 id="GNN-Structure"><a href="#GNN-Structure" class="headerlink" title="GNN Structure"></a>GNN Structure</h2><p>本文总结了当前主流的GNN结构：Modern GNNs follow a <strong>neighborhood aggregation strategy</strong>, where we iteratively update the representation of a node by aggregating representations of its neighbors. After k iterations of aggregation, a node’s representation <strong>captures the structural information within its k-hop network neighborhood.</strong> 文章将neighborhood aggregation总结成两步：首先需要将中心节点neighborhood的表示通过一个函数进行聚合：</p><script type="math/tex; mode=display">a_{v}^{(k)}=\operatorname{AGGREGATE}^{(k)}\left(\left\{h_{u}^{(k-1)}: u \in \mathcal{N}(v)\right\}\right)</script><p>接着将中心节点的表示进行更新：</p><script type="math/tex; mode=display">h_{v}^{(k)}=\mathrm{COMBINE}^{(k)}\left(h_{v}^{(k-1)}, a_{v}^{(k)}\right)</script><p>对于graph-level的分类问题，我们需要一个readout function将所有的node-level representation进行aggregation，从而得到关于整个图的representation。readout function should be a permutation invariant function such as summation or graph-level pooling function.</p><script type="math/tex; mode=display">h_{G}=\operatorname{READOUT}\left(\left\{h_{v}^{(K)} | v \in G\right\}\right)</script><h2 id="Weisfeiler-Lehman-Test"><a href="#Weisfeiler-Lehman-Test" class="headerlink" title="Weisfeiler-Lehman Test"></a>Weisfeiler-Lehman Test</h2><p><strong>The graph isomorphism problem asks whether two graphs are topologically identical.</strong> This is a challenging problem: no polynomial-time algorithm is known for it yet. </p><p>The Weisfeiler-Lehman (WL) test of graph isomorphism is an effective and computationally efficient test that distinguishes a broad class of graphs. Its 1-dimensional form, “naïve vertex refinement”, is analogous to neighbor aggregation in GNNs. The WL test iteratively (1) aggregates the labels of nodes and their neighborhoods, and (2) hashes the aggregated labels into unique new labels. The algorithm decides that two graphs are non-isomorphic if at some iteration the labels of the nodes between the two graphs differ.</p><p>关于WL test的具体算法，详见<a href="https://www.davidbieber.com/post/2019-05-10-weisfeiler-lehman-isomorphism-test/" target="_blank" rel="noopener">blog “WL test”</a>. </p><h1 id="Theoretical-Framework"><a href="#Theoretical-Framework" class="headerlink" title="Theoretical Framework"></a>Theoretical Framework</h1><p><img src="https://i.loli.net/2020/03/02/qwKYmVN23Lo9lu5.png" alt="image.png"></p><p>A GNN recursively updates each node’s feature vector to capture the network structure and features of other nodes around it, i.e., its <strong>rooted subtree structures</strong>. For notational simplicity, we can assign each feature vector a unique label in $ \{ a, b, c,  \cdots \} $. Then, feature vectors of a set of neighboring nodes form a multiset: the same element can appear multiple times since different nodes can have identical feature vectors.</p><p>Since subtree structures are defined recursively via node neighborhoods, we can reduce our analysis to the question whether a GNN maps two neighborhoods (i.e., two multisets) to the same embedding or representation. <strong>A maximally powerful GNN would never map two different neighborhoods, i.e., multisets of feature vectors, to the same representation.</strong> This means its aggregation scheme must be <strong>injective</strong>.</p><p>总结一下理论推导的思路：</p><ul><li>首先定义GNN的aggregation函数是一个作用在由neighborhood定义的multiset上的函数</li><li>多层的GNN不断地将周围的neighborhood展开变成子树的形式，因为这一递归的定义，我们只需要关注在每一层树的映射上</li><li>最有鉴别力的GNN的上界就是WL test，这一条件只在aggregation方法是injective时实现，因为这意味着GNN不会将不同的multiset映射到同一个representation上。</li></ul><h1 id="Building-Powerful-GNN"><a href="#Building-Powerful-GNN" class="headerlink" title="Building Powerful GNN"></a>Building Powerful GNN</h1><p><img src="https://i.loli.net/2020/03/03/uaJFxjKdsMvW1PS.png" alt="image.png"></p><p>A natural follow-up question is whether there exist GNNs that are, in principle, as powerful as the WL test? Our answer, in Theorem 3, is yes: if the neighbor aggregation and graph-level readout functions are injective, then the resulting GNN is as powerful as the WL test.</p><p><img src="https://i.loli.net/2020/03/03/eufvlm7iCjrdoFY.png" alt="image.png"></p><p><strong>For countable sets, injectiveness well characterizes whether a function preserves the distinctness of inputs.</strong> Uncountable sets, where node features are continuous, need some further considerations. In addition, it would be interesting to characterize how close together the learned features lie in a function’s image.</p><p>上面虽然讨论了GNN和WL test在鉴别不同拓扑结构的图时的能力比较，但是GNN<strong>还需要将相似的neighborhood映射到相似的embedding上，这是为了GNN的泛化能力</strong>。In contrast, a GNN satisfying the criteria in Theorem 3 generalizes the WL test by learning to embed the subtrees to low-dimensional space. This enables GNNs to not only discriminate different structures, but also to learn to map similar graph structures to similar embeddings and capture dependencies between graph structures.</p><h2 id="Graph-Isomorphism-Network"><a href="#Graph-Isomorphism-Network" class="headerlink" title="Graph Isomorphism Network"></a>Graph Isomorphism Network</h2><p>本文提出的GIN网络结构和WL test有着同样的鉴别不同网络结构的能力。Our next lemma states that <strong>sum aggregators can represent injective</strong>, in fact, universal functions over multisets.</p><p><img src="https://i.loli.net/2020/03/03/SafujK4kymWeAVL.png" alt="image.png"></p><p>这条引理告诉我们存在一个作用在可数元素的函数，使得经过该函数变换后再加权的函数是一个单射。拿GNN网络来做类比，也就是函数$f$作用在节点feature上，$h(X)$就是中心节点的聚合函数，那么我们知道存在这样的一个函数$f$能够让GNN的聚合函数为单射。更进一步地，任何一个定义在multiset上的函数$g$都能够被解构成关于某个函数的和的形式：</p><script type="math/tex; mode=display">g(X)=\phi\left(\sum_{x \in X} f(x)\right)</script><p><img src="https://i.loli.net/2020/03/03/INO9LvUP1XeYEHw.png" alt="image.png"></p><p>以上的引理实际上也是根据GNN中心节点获得neighborhood aggregation之后的combine函数进行设计的。从而我们有GIN节点表示的递推关系。 Generally, there may exist many other powerful GNNs. GIN is one such example among many maximally powerful GNNs, while being simple.</p><script type="math/tex; mode=display">h_{v}^{(k)}=\operatorname{MLP}^{(k)}\left(\left(1+\epsilon^{(k)}\right) \cdot h_{v}^{(k-1)}+\sum_{u \in \mathcal{N}(v)} h_{u}^{(k-1)}\right)</script><h2 id="Mean-Max-Pooling"><a href="#Mean-Max-Pooling" class="headerlink" title="Mean/Max Pooling"></a>Mean/Max Pooling</h2><p>What happens if we replace the sum in $h(X)=\sum_{x \in X} f(x)$ with mean or max-pooling as in GCN? Mean and max-pooling aggregators are still well-defined multiset functions because they are permutation invariant. But, <strong>they are not injective</strong>.</p><p><img src="https://i.loli.net/2020/03/03/fsvM2R14LAicjwD.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/03/03/9VCag32ZJBORvn1.png" alt="image.png"></p><p>第一张图表示了三种aggregator表达能力的不同，其表达能力的强弱顺序依次为sum&gt;mean&gt;max。而第二张图给出了一些简单的mean/max失败的案例。</p><h3 id="MEAN-LEARNS-DISTRIBUTIONS"><a href="#MEAN-LEARNS-DISTRIBUTIONS" class="headerlink" title="MEAN LEARNS DISTRIBUTIONS"></a>MEAN LEARNS DISTRIBUTIONS</h3><p>很容易想到，当一个multiset包括了另一个multiset若干份copy时，mean aggregation会无法分辨，从而得出结论：Mean learns distributions. The mean aggregator may perform well if, for the task, the statistical and distributional information in the graph is more important than the exact structure. Moreover, when node features are diverse and rarely repeat, the mean aggregator is as powerful as the sum aggregator. This may explain why, despite the limitations identified, GNNs with mean aggregators are effective for node classification tasks, such as classifying article subjects and community detection, where node features are rich and the distribution of the neighborhood features provides a strong signal for the task. </p><h3 id="MAX-POOLING-LEARNS-SETS-WITH-DISTINCT-ELEMENTS"><a href="#MAX-POOLING-LEARNS-SETS-WITH-DISTINCT-ELEMENTS" class="headerlink" title="MAX-POOLING LEARNS SETS WITH DISTINCT ELEMENTS"></a>MAX-POOLING LEARNS SETS WITH DISTINCT ELEMENTS</h3><p>Max-pooling captures neither the exact structure nor the distribution. However, it may be suitable for tasks where it is important to identify representative elements or the “skeleton”, rather than to distinguish the exact structure or distribution. Qi et al. (2017) empirically show that the max-pooling aggregator learns to identify the skeleton of a 3D point cloud and that it is robust to noise and outliers. </p><h1 id="OpenReview-Discussion"><a href="#OpenReview-Discussion" class="headerlink" title="*OpenReview Discussion"></a><a href="https://openreview.net/forum?id=ryGs6iA5Km" target="_blank" rel="noopener">*OpenReview Discussion</a></h1><h3 id="Review1"><a href="#Review1" class="headerlink" title="Review1."></a>Review1.</h3><p>My chief concern is equating the Weisfeiler-Lehman test (WL-test) with Weisfeiler-Lehman-type GNNs (WL-GNNs). The WL-test relies on countable set inputs and injective hash functions. Here, the paper is oversimplifying the WL-GNN problem. After the first layer, a WL-GNN is operating on uncountable sets. On uncountable sets, saying that a function is injective does not tells us much about it; we need a measure of how closely packed we find the points in the function’s image (a measure in measure theory, a density in probability). On countable sets, saying a function is injective tells us much about the function. Moreover, the WL-test hash function does not even need to operate over sets with total or even partial orders. As a neural network, the WL-GNN “hash” ( in the paper) must operate over a totally ordered set (\mathbb{R}^n, n &gt; 0). Porting the WL-test argument of “convergence to unique isomorphic fingerprints” to a WL-GNN requires a <strong>measure-theoretic analysis of the output</strong> of the WL-GNN layers, and careful analysis if the total order of the set does not create attractors when they are applied recursively. </p><p><strong>Reply. Validity of equating the WL test operating on countable sets to the WL-GNN operating on uncountable sets.</strong> The reviewer makes a great observation that countability of node features is essential and necessary for our theory, and we acknowledge that our current Theorem 3 and Lemma 5 are built on the common assumption that input node features are from a countable universe. We have now made this clear in our paper. We also filled in a technical gap/detail to address R1’s concern that after the first iteration, we are in an uncountable universe: this actually does not happen. We can show that for a fixed aggregation function, hidden node features also form a countable universe, because the countability of input node features recursively propagates into deeper layers. We also added a rigorous proof for this (Lemma 4 in our revised paper). As the reviewer nicely suggests, for the uncountable setting, it would be useful to have measure-theoretic analysis, which we leave for future work. <strong>Often input node features in graph classification applications (e.g., chemistry, bioinformatics, social)</strong> come from a countable (in fact, finite) universe, so our assumption is realistic. In the revised version, we clearly stated our assumptions at the beginning of Section 3 and have added further discussion on the relation between the WL test and WL-GNN after Theorem 3.</p><h3 id="Review2"><a href="#Review2" class="headerlink" title="Review2."></a>Review2.</h3><p>The matrix analysis of the last paragraph also points to another <strong>potential problem with the sum aggregator.</strong> GIN needs to be shallow. With ReLU activations the reason is simple: for an adjacency matrix $A$, the value of $A^j$ grows very quickly with  (diverges). With sigmoid activations, GIN would experience vanishing gradients in graphs with high variance in node degrees.</p><p><strong>Reply</strong>. Furthermore, the reviewer is concerned <strong>with a possibly exploding value due to the sum aggregation</strong>, but this can be avoided because <strong>we have different learnable neural networks</strong> at each layer that can scale down the summed output (also, in practice, we did not observe such explosion).</p><h3 id="Review3"><a href="#Review3" class="headerlink" title="Review3."></a>Review3.</h3><p>I understand that GIN provably has more discriminative power than other variants of GNN. But the ability to differentiate non-isomorphic graphs does not necessarily imply better graph classification accuracy, right? Would it be possible to strong discriminative power will backfire for the graph classification? After all, we don’t need to solve graph isomorphism here.</p><p><strong>Reply</strong>. As we have pointed out in the experiment section, although stronger discriminative power does not directly imply better generalization, it is reasonable to expect that models that can accurately capture graph structures of interest also perform well on test set. In particular, with many existing GNNs, the discriminative power may not be enough to capture graph substructures that are important for classifying graphs. Therefore, we believe strong discriminative power is generally advantageous for graph classification. In our experiments, we empirically demonstrated that our powerful GIN has better generalization as well as better fitting to training datasets compared to other GNN variants. GINs performed the best in general, and achieved state-of-the-art test accuracy. We leave further theoretical investigation of generalization to our future work.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1810.00826&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;paper “How powerful are
      
    
    </summary>
    
    
      <category term="GNN" scheme="http://www.shihaizhou.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Visual Exploration</title>
    <link href="http://www.shihaizhou.com/2020/02/28/Visual-Exploration/"/>
    <id>http://www.shihaizhou.com/2020/02/28/Visual-Exploration/</id>
    <published>2020-02-28T08:59:35.000Z</published>
    <updated>2020-02-28T16:15:04.581Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/pdf/2001.02192.pdf" target="_blank" rel="noopener">paper “An Exploration of Embodied Visual Exploration”</a></li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>本文提出了Embodied Visual Exploration这一概念：how might a robot equipped with a camera scope out a new environment? 关于Visual Exploration，有三个问题需要解决：</p><ul><li>What does it mean for an agent to explore its environment well?</li><li>Which methods work well, and under which assumptions and environmental settings?</li><li>Where do current approaches fall short, and where might future work seek to improve?</li></ul><p>首先，CV中的问题分为三个层次：第一层是模型被动地学习人为收集和标注好的数据；第二层是embodied active perception，学习task-specific controls；第三层是embodied visual exploration, the goal is inherently more open-ended and task-agnostic: how does an agent learn to move around in an environment to gather information that will be useful for a variety of tasks that it may have to perform in the future?</p><p>Embodied Visual Exploration的算法比较难以横向进行比较，因为不同的工作之间的重心不同，因此测试的evaluation metric也选择不同：overcoming sparse rewards, pixelwise reconstruction of environments, area covered in the environment, object interactions,information gathering for solving downstream tasks such as navigation/recognition/pose estimation. 因此这篇工作提出了一个unified view of exploration algorithms for visually rich 3D environments, and a common evaluation framework to understand their strengths andweaknesses.</p><h1 id="Problem-Setting"><a href="#Problem-Setting" class="headerlink" title="Problem Setting"></a>Problem Setting</h1><p>首先Embodied Visual Exploration定义为：agent在环境中进行一定轮数的observe-action-update循环，其中<strong>action的选择目标是最大化information gain</strong>。本质上就是一个Partially Observable Markov Decision Process (POMDP)。</p><h2 id="POMDP"><a href="#POMDP" class="headerlink" title="POMDP"></a>POMDP</h2><p>A discrete-time <a href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process" target="_blank" rel="noopener">POMDP</a> models the relationship between an agent and its environment. Formally, a POMDP is a 7-tuple $(S,A,T,R,\Omega ,O,\gamma )$:</p><ul><li>$S$ is a set of states,</li><li>$A$ is a set of actions,</li><li>$T$ is a set of conditional transition probabilities between states,</li><li>$R:S\times A\to \mathbb {R} $ is the reward function,</li><li>$\Omega$ is a set of observations,</li><li>$O$ is a set of conditional observation probabilities, and</li><li>$\gamma \in [0,1]$ is the discount factor.</li></ul><p>POMDP和MDP唯一不同的地方在于，PDMDP多了agent接受到的observation. 对于当前世界的state $s\in S$，agent采取了action $a\in A$，接着世界的state进入到了$s^\prime$，agent接受到的observation 遵从分布$O(o|s^\prime ,a)$. </p><h1 id="Exploration-Paradigms"><a href="#Exploration-Paradigms" class="headerlink" title="Exploration Paradigms"></a>Exploration Paradigms</h1><h2 id="Curiosity"><a href="#Curiosity" class="headerlink" title="Curiosity"></a>Curiosity</h2><p>In the curiosity paradigm, the agent is encouraged to visit states where its predictive model of the environment is uncertain. Dynamics-based formulation of curiosity指的是通过学习一个forward-dynamics model $\mathcal F$，模型每一次选择和当前state最大差异的下一个state。也就是该模型预测下一个state会是怎样的表示： $\hat{\boldsymbol{s}}_{t+1}=\mathcal{F}\left(\boldsymbol{s}_{t}, a_{t}\right)$ ，通过定义reward function $R$即可：</p><script type="math/tex; mode=display">r_{t} \propto\left\|\hat{s}_{t+1}-s_{t+1}\right\|_{2}^{2}</script><p>这个forward-dynamics model是通过online training的方式进行学习的，只需要在两个state之间最小化$\left|F\left(s_{t}, a_{t}\right)-s_{t+1}\right|_{2}^{2}$. </p><h2 id="Novelty"><a href="#Novelty" class="headerlink" title="Novelty"></a>Novelty</h2><p>Novelty reward直接硬记录到达每一个state的次数，然后让reward function和该次数成负相关即可。其中需要对3D环境的平面进行网格化建模，含义是是同一个地点不要来很多次。</p><script type="math/tex; mode=display">r_{t} \propto \frac{1}{\sqrt{n\left(s_{t}\right)}}</script><h2 id="Coverage"><a href="#Coverage" class="headerlink" title="Coverage"></a>Coverage</h2><p>Coverage则认为Novelty的判断过于武断，因为在3D环境中不同位置的信息量是不一样的，这和该位置的周围structure相关。Whereas novelty encourages explicitly visiting all locations, coverage encourages <strong>observing</strong> all of the environment. 换句话说，到的地方越多不等价于observe到的信息越大。</p><p>The coverage reward consists of the increment in some observed quantity of interest: </p><script type="math/tex; mode=display">r_{t} \propto I_{t}-I_{t-1}</script><p>其中$I_t$指的是在时间步$t$是观测到interesting object的数目。可供选择的”things”可以为aera/object/landmark/random view. 其中random view指的是我们可以随机在这个环境中指定viewpoint作为奖励，如果agent看到了这个viewpoint，那么就可以获得相应的reward. This method is similar to the <a href="https://arxiv.org/abs/1803.00653" target="_blank" rel="noopener">“goal agnostic” baseline.</a></p><h2 id="Reconstruction"><a href="#Reconstruction" class="headerlink" title="Reconstruction"></a>Reconstruction</h2><p>Reconstruction-based methods use the objective of active observation completion to learn exploration policies. The reconstruction reward scores the quality of the predicted outputs:</p><script type="math/tex; mode=display">r_{t} \propto-d\left(V(\mathcal{P}), \hat{V}_{t}(\mathcal{P})\right)</script><p>其中$V(\mathcal P)$是camera view在pose $\mathcal P$处的true query view，$\hat V_t(\mathcal P)$是agent在时间步$t$时的view reconstructions，$d$ 是定义在view上的distance function. Whereas curiosity rewards views that are individually surprising, reconstruction rewards views that bolster the agent’s correct hallucination of all other views. </p><h1 id="Evaluation-Framework"><a href="#Evaluation-Framework" class="headerlink" title="Evaluation Framework"></a>Evaluation Framework</h1><p>第一种Evaluation Metric比较简单，可以计算Model在Exploration期间访问了多少”interesting things”，其中如上文所述，interesting things可以是area, objects, and landmarks。</p><p>第二种Evaluation Metric测试Exploration在Downstream task transfer上的性能。最近被拿来广泛测试的主要downstream task包括了以下三种：</p><ul><li><strong>PointNav</strong>: how to quickly navigate from point A to point B?</li><li><strong>View localization</strong>: where was this photo taken?</li><li><strong>Reconstruction</strong>: what can I expect to see at point B?</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2001.02192.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;paper “An Explorati
      
    
    </summary>
    
    
      <category term="CV" scheme="http://www.shihaizhou.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>Graph Neural Networks - Survey</title>
    <link href="http://www.shihaizhou.com/2020/02/28/Graph-Neural-Networks/"/>
    <id>http://www.shihaizhou.com/2020/02/28/Graph-Neural-Networks/</id>
    <published>2020-02-28T08:43:17.000Z</published>
    <updated>2020-02-28T08:50:34.323Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong>:</p><ul><li><a href="https://github.com/thunlp/GNNPapers" target="_blank" rel="noopener">github repo “GNN papers one must read”</a></li><li><a href="https://arxiv.org/abs/1901.00596" target="_blank" rel="noopener">paper “A Comprehensive Survey on Graph Neural Networks”</a></li><li><a href="https://shihaizhou.com/2020/02/22/Graph-Neural-Networks-Basics/" target="_blank" rel="noopener">Previous post introducing basic knowledge about GNNs.</a></li></ul><h1 id="Backgrounds"><a href="#Backgrounds" class="headerlink" title="Backgrounds"></a>Backgrounds</h1><h2 id="Basics"><a href="#Basics" class="headerlink" title="Basics"></a>Basics</h2><p><strong>History</strong> 早期的工作可以被归结为Recurrent GNNs (RecGNN)，节点在每个iteration和周围的节点传递信息直到节点的表示拟合，这种方法在计算上开销比较大，后来的研究者为了解决这一问题也有工作。CNN在视觉领域取得成功之后，研究者在GNN上重新定义了convolution操作并且有很多的工作围绕ConvGNN展开。在ConvGNN中可以分为两个小的子方向：spatial-based ConvGNNs以及spectral-based ConvGNNs。除此之外，还有很多工作关注在Graph AutoEncoders以及Spatial-Temporal GNNs，这些工作都是在前两个方向的基础上进行的。</p><p><strong>GNN vs Graph Kernel</strong> 后者是以往解决graph classification的主流方法，这个方法通过核函数将图结构映射到一个低维的向量空间，然后利用ML方法例如SVM进行分类。和GNN不同的是，Graph Kernel方法里的核函数是人为定义的而不是可学习的。GNN则将Graph Representation直接进行端到端的训练学习。</p><h2 id="Training-Frameworks"><a href="#Training-Frameworks" class="headerlink" title="Training Frameworks"></a>Training Frameworks</h2><p><strong>Semi-supervised learning for node-level classification.</strong> 在节点上进行的<strong>半监督分类问题</strong>。半监督意味着在一个图中有一些节点进行了标注，而另外一些节点没有标注。通常使用ConvGNN进行未标注节点的分类。</p><p><strong>Supervised learning for graph-level classification.</strong> Graph-level的有监督分类。通常使用graph convolution + graph pooling/readout layer进行。graph pooling layer通常被用来进行高维信息的聚合，readout layer将每个节点的representation映射到graph-level的representation。</p><p><strong>Unsupervised learning for graph embedding.</strong> 在没有标注的情况下学习图像的embedding。一种方法采用autoencoder；另一种方法采用negative sampling：采样一些互相没有连接的node pair作为负样本，有连接的node pair作为正样本。</p><h1 id="RecGNNs"><a href="#RecGNNs" class="headerlink" title="RecGNNs"></a>RecGNNs</h1><p>RecGNNs是最早开始在图结构上采用神经网络的，可以说是GNNs方法的先驱。早期的工作因为计算条件的限制，只能在directed acyclic graphs上进行。</p><p>由<a href="http://persagen.com/files/misc/scarselli2009graph.pdf" target="_blank" rel="noopener">Scarselli et al.提出的GNN*</a>算是图神经网络的开山之作，它采用了一种information diffusion mechanism - it updates nodes’ states by exchanging neighborhood information recurrently until a stable equilibrium is reached: </p><script type="math/tex; mode=display">h_v^{(t)} = \sum_{u\in N(v)} f(x_v, x_{(v,u)}^e,x_u, h_u^{(t-1)})</script><p>To ensure convergence, the recurrent function $f(\cdot)$ <strong>must be a contraction mapping</strong>, which shrinks the distance between two points after projecting them into a latent space. In the case of $f(\cdot)$ being a neural network, a penalty term has to be imposed on the Jacobian matrix of parameters.</p><p><a href="https://ieeexplore.ieee.org/document/5596796" target="_blank" rel="noopener">Graph Echo State Network (GraphESN)</a> 在GCN*的基础上进行了改进从而达到了更好的训练效率 - It implements a contractive state transition function to recurrently update node states until the global graph state reaches convergence. Afterward, the output layer is trained by taking the fixed node states as inputs. （该工作的引用不是很高）</p><p><a href="https://arxiv.org/pdf/1511.05493.pdf" target="_blank" rel="noopener">Gated Graph Neural Network (GGNN)</a> 采用了GRU作为recurrent function. </p><script type="math/tex; mode=display">h_v^{(t)}=\text{GRU}(h_v^{(t-1)}, \sum_{u\in N(v)}Wh_u^{(t-1)})</script><p>同时GGNN不再将convergence作为recurrence停止的信号，而是将其限制在一个最大步数里。这样做的好处是不再需要对参数进行额外的约束来保证收敛，同时它采用了BP进行参数更新：这在graph很大的时候可能会导致问题。</p><p><a href="http://proceedings.mlr.press/v80/dai18a/dai18a.pdf" target="_blank" rel="noopener">Stochastic Steady-state Embedding (SSE)</a>对于更大的图上有比较好的表现。他对于节点hidden state的更新采用的是随机且异步(stochastic and asynchronous)的方式：它会sample一部分的节点进行更新，并且sample另一部分的节点进行梯度计算。为了让算法更加稳定，在进行hidden state的更新时还会采用smoothing的方法。</p><script type="math/tex; mode=display">h_v^{(t)} = (1-\alpha)h_v^{(t-1)} + \alpha W_1 \sigma(W_2[x_v, \sum_{u\in N(v)}[h_u^{(t-1)}, x_u]])</script><p>注意：SSE并没有在理论上证明通过以上的recurrence能够收敛。</p><h1 id="ConvGNNs"><a href="#ConvGNNs" class="headerlink" title="ConvGNNs"></a>ConvGNNs</h1><p>首先我们会介绍关于<a href="https://shihaizhou.com/2020/02/22/Graph-Neural-Networks-Basics/" target="_blank" rel="noopener">Graph Laplacian Operator的intuition和推导</a>过程，因为这是一个非常重要的概念，理解它有助于理解图结构的问题应当遵循怎样的解决思路。接着分别介绍Spectral-based ConvGNNs和Spatial-based ConvGNNs.</p><h2 id="Spectral-based-ConvGNNs"><a href="#Spectral-based-ConvGNNs" class="headerlink" title="Spectral-based ConvGNNs"></a>Spectral-based ConvGNNs</h2><p>Spectral-based approaches define graph convolutions by introducing filters from the perspective of graph signal processing where the graph convolutional operation is interpreted as removing noises from graph signals. </p><p>首先回顾graph convolution的定义：The <strong>graph convolution</strong> of the input signal $x$ with a filter $g\in \mathbb R^{|V|}$ is defined as</p><script type="math/tex; mode=display">\begin{aligned} \mathbf{x} *_{G} \mathbf{g} &=\mathscr{F}^{-1}(\mathscr{F}(\mathbf{x}) \odot \mathscr{F}(\mathbf{g})) \\ &=\mathbf{U}\left(\mathbf{U}^{T} \mathbf{x} \odot \mathbf{U}^{T} \mathbf{g}\right) \\&= \mathbf{U} \mathbf{g}_\theta \mathbf{U}^T \mathbf{x}\end{aligned}</script><p><a href="https://arxiv.org/abs/1312.6203" target="_blank" rel="noopener">Spectral Convolutional Neural Network (Spectral CNN)</a> 假设了多层的可训练multi-channel filter $\mathbf{g}_{\theta}=\mathbf{\Theta}_{i, j}^{(k)}$. Spectral CNN的图卷积操作定义为：</p><script type="math/tex; mode=display">\mathbf{H}_{:, j}^{(k)}=\sigma\left(\sum_{i=1}^{f_{k-1}} \mathbf{U} \Theta_{i, j}^{(k)} \mathbf{U}^{T} \mathbf{H}_{:, i}^{(k-1)}\right) \quad\left(j=1,2, \cdots, f_{k}\right)</script><p>where $k$ is the layer index, $\mathbf{H}^{(k-1)} \in \mathbf{R}^{n \times f_{k-1}}$ is the input graph signal, $\mathbf{H}^{(0)}=\mathbf{X}$, $f_{k-1}$ is the number of input channels and $f_k$ is the number of output channels, $\Theta_{i, j}^{(k)}$ is a diagonal matrix filled with learnable parameters. </p><p>因为需要特征分解，所以Spectral CNN方法有以下三点不足：首先，任何图结构的变化（边权变化或者节点增加）都会导致eigenbasis的变化；其次，训练得到的fitler是domain dependent，是不能运用在不同结构的图上的；第三，特征分解需要$O(|V|^3)$的计算复杂度。以下的两篇工作ChebNet和GCN通过一些近似和简化将其计算复杂度降低。</p><p><a href="https://arxiv.org/abs/1606.09375" target="_blank" rel="noopener">Chebyshev Spectral CNN (ChebNet)</a> 认为non-parametric filters有两点不足：(i) they are not localized in space; (ii) their learning complexity is in $O(n)$. 根据<a href="https://shihaizhou.com/2020/02/22/Graph-Neural-Networks-Basics/" target="_blank" rel="noopener">上一篇博客</a>中的Lemma 1，我们可以通过高次$L_n$来得到具有局部连通性的filter. 所以我们可以如下设计一个多channel的filter函数：</p><script type="math/tex; mode=display">g_{\theta}(\Lambda)=\sum_{k=0}^{K-1} \theta_{k} \Lambda ^k</script><p>高次$L_n$的计算需要$O(N^3)$的复杂度，而我们最后期待计算的结果是类似于将filter函数作用到输入的graph signal $x$上。据此联想到Chebyshev Polynomials（详细见博客”Approximation Theory”中描述Chebyshev Polynomials的定义及性质）在方阵上的推广。为了满足Chebyshev Polynomials的定义域问题，需要将$\Lambda$以及$L$归一化到区间$[-1,1]$上：</p><script type="math/tex; mode=display">\begin{align}\tilde {\Lambda} &= \frac {2\Lambda} {\lambda_\max} - I \\\tilde {L} &= \frac {2 L} {\lambda_\max} - I\end{align}</script><p>Chebyshev Polynomials的递归定义如下：</p><script type="math/tex; mode=display">T_{i}(\mathbf{x})=2 \mathbf{x} T_{i-1}(\mathbf{x})-T_{i-2}(\mathbf{x})</script><p>其中$T_{0}(\mathbf{x})=1$, $T_{1}(\mathbf{x})=\mathbf{x}$, $\mathbf{x}$为向量。根据以上我们能够给出更快计算filter的定义式：</p><script type="math/tex; mode=display">\mathbf{x} *_{G} \mathbf{g}_{\theta}=\sum_{i=0}^{K} \theta_{i} T_{i}(\tilde{\mathbf{L}}) \mathbf{x}</script><p>As an improvement over Spectral CNN, the filters defined by ChebNet are localized in space, which means filters can extract local features independently of the graph size. </p><p><a href="https://arxiv.org/pdf/1609.02907.pdf" target="_blank" rel="noopener">Graph Convolutional Network (GCN)</a> 对ChebNet进行了一阶近似，通过固定$K=1$，$\lambda_\max=2$，我们能够将上式改写成：</p><script type="math/tex; mode=display">\mathbf{x} *_{G} \mathbf{g}_{\theta}=\theta_{0} \mathbf{x}-\theta_{1} \mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}} \mathbf{x}</script><p>To restrain the number of parameters and avoid over-fitting, GCN further assume $\theta =\theta_0=-\theta_1$, leading to the following definition of a graph convolution, </p><script type="math/tex; mode=display">\mathbf{x} *_{G} \mathbf{g}_{\theta}=\theta\left(\mathbf{I}_{\mathbf{n}}+\mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}}\right) \mathbf{x}</script><p>为了能够处理多通道的input/output，GCN将上式改成了compositional layer, defined as,</p><script type="math/tex; mode=display">\mathbf{H}=\mathbf{X} *_{G} \mathbf{g}_{\Theta}=f(\overline{\mathbf{A}} \mathbf{X} \Theta)</script><p>其中$\overline{\mathbf{A}}=\mathbf{I}_{\mathbf{n}}+\mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}}$, $f(\cdot)$是activation function，使用这样定义的$\overline A$会导致numerical instability. To address this problem, GCN applies a normalization trick: $\overline{\mathbf{A}}=\tilde{\mathbf{D}}^{-\frac{1}{2}} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-\frac{1}{2}}$ with $\tilde{\mathbf{A}}=\mathbf{A}+\mathbf{I}_{\mathbf{n}}$ and $\tilde{\mathbf{D}}_{i i}=\sum_{j} \tilde{\mathbf{A}}_{i j}$. 虽然GCN是一种Spectral-based的方法，但是它也能够被看作aggregating feature information from a node’s neighborhood的空间图卷积网络,</p><script type="math/tex; mode=display">\mathbf{h}_{v}=f(\boldsymbol{\Theta}^{T}(\sum_{u \in\{N(v) \cup v\}} \bar{A}_{v, u} \mathbf{x}_{u})) \quad \forall v \in V</script><p>在GCN的基础上，有很多的工作进行了一些增量式的改动。Several recent works made incremental improvements over GCN by exploring alternative symmetric matrices. <a href="https://arxiv.org/pdf/1801.03226.pdf" target="_blank" rel="noopener">Adaptive Graph Convolutional Network (AGCN)</a> learns hid-den structural relations unspecified by the graph adjacency matrix. It constructs a so-called residual graph adjacency matrix through a learnable distance function which takes two nodes’ features as inputs. <a href="https://arxiv.org/pdf/1706.02216.pdf" target="_blank" rel="noopener">Dual Graph Convolutional Network (DGCN)</a> introduces a dual graph convolutional architecture with two graph convolutional layers in parallel.</p><h2 id="Spatial-based-ConvGNNs"><a href="#Spatial-based-ConvGNNs" class="headerlink" title="Spatial-based ConvGNNs"></a>Spatial-based ConvGNNs</h2><p>Spatial-based approaches inherit ideas from RecGNNs to define graph convolutions by information propagation. The spatial graph convolutional operation essentially propagates node information along edges.</p><p>在二维图像上的convolution可以看作是graph convolution的特殊情况（如下图所示）：图像中，红点的representation是weighted sum of the nearby representation；同样的，基于这个想法我们可以定义一个图的convolution为他的weighted sum of the neighborhood。</p><p><img src="https://i.loli.net/2020/02/25/Kwto9fgQyN7r6XH.png" alt="image.png"></p><p><a href="https://ieeexplore.ieee.org/document/4773279" target="_blank" rel="noopener">Neural Network for Graphs (NN4G)</a> 是和GNN*同一时间提出来的模型。和RecGNNs不同的是，NN4G层之间不共享参数，除此之外，层与层之间还有residual connection。这和之前的GCN有很强的相似性，只是NN4G没有采用normalization的技术，这会导致层与层之间的表示在scale上有非常大的差别。从这里我们可以想到，<strong>一定要注意最简单的常识性问题。例如算法是否收敛，设计的模型是否会导致参数的爆炸，每一层的scale是多少，都要有一些意识。</strong>深度学习虽然像是在很多基本块中找合适的零件搭积木，但是关于模型设计的技术细节，还是需要多加注意。</p><script type="math/tex; mode=display">\mathbf{H}^{(k)}=f\left(\mathbf{X} \mathbf{W}^{(k)}+\sum_{i=1}^{k-1} \mathbf{A} \mathbf{H}^{(k-1)} \mathbf{\Theta}^{(k)}\right)</script><ul><li><a href="https://arxiv.org/abs/1511.02136" target="_blank" rel="noopener">Diffusion Convolutional Neural Network (DCNN)</a></li><li><a href="https://arxiv.org/abs/1707.01926" target="_blank" rel="noopener">Diffusion Graph Convolution (DGC)</a></li><li><a href="https://arxiv.org/abs/1811.10435" target="_blank" rel="noopener">PGC-DGCNN</a></li><li><a href="https://arxiv.org/abs/1801.07455" target="_blank" rel="noopener">Partition Graph Convolution (PGC)</a></li></ul><p><a href="https://arxiv.org/abs/1704.01212" target="_blank" rel="noopener">Message Passing Neural Network (MPNN)</a> outlines a general framework of spatial-based ConvGNNs. 这篇工作将图卷积看作是message passing的一种方式，节点信息通过边进行传递。message passing机制定义为以下：</p><script type="math/tex; mode=display">\mathbf{h}_{v}^{(k)}=U_{k}\left(\mathbf{h}_{v}^{(k-1)}, \sum_{u \in N(v)} M_{k}\left(\mathbf{h}_{v}^{(k-1)}, \mathbf{h}_{u}^{(k-1)}, \mathbf{x}_{v u}^{e}\right)\right)</script><p>其中第零层的隐层表示为节点输入，$U_k(\cdot)​$ 以及 $M_k(\cdot)​$ 是具有可学习参数的函数。经过几段的message passing，$h_v^{(K)}​$能够直接传进output layer进行node-level prediction或者进入一个readout function进行graph-level prediction. readout function基于节点表示生成图表示：</p><script type="math/tex; mode=display">\mathbf{h}_{G}=R\left(\mathbf{h}_{v}^{(K)} | v \in G\right)</script><p><a href="https://arxiv.org/abs/1810.00826" target="_blank" rel="noopener">Graph Isomorphism Network (GIN)</a> finds that previous MPNN-based <strong>methods are incapable of distinguishing different graph structures</strong> based on the graph embedding they produced. 为了弥补这个缺点，GIN将中心节点的权重用一个可学习参数来代替：</p><script type="math/tex; mode=display">\mathbf{h}_{v}^{(k)}=M L P\left(\left(1+\epsilon^{(k)}\right) \mathbf{h}_{v}^{(k-1)}+\sum_{u \in N(v)} \mathbf{h}_{u}^{(k-1)}\right)</script><p><a href="https://arxiv.org/abs/1706.02216" target="_blank" rel="noopener">GraphSage</a> 不直接让所有的邻接节点进行message passing，因为一个节点可能有非常不同的邻接节点数，如果直接全部计算会有较大的计算开销。因此这篇工作采取了从一个节点的neighborhodd中采样固定数目节点的方法，其中$S_{\mathcal{N}(v)}$表示采样，其中的aggregation function $f_k$必须是一个顺序无关的函数例如mean, sum or max：</p><script type="math/tex; mode=display">\mathbf{h}_{v}^{(k)}=\sigma\left(\mathbf{W}^{(k)} \cdot f_{k}\left(\mathbf{h}_{v}^{(k-1)},\left\{\mathbf{h}_{u}^{(k-1)}, \forall u \in S_{\mathcal{N}(v)}\right\}\right)\right)</script><p><a href="https://arxiv.org/abs/1710.10903" target="_blank" rel="noopener">Graph Attention Network (GAT)</a>  和之前的模型不同，它认为message passing中的不同节点应该有不同的权重。这权重也是通过训练学习得到的。GAT中的图卷积定义为：</p><script type="math/tex; mode=display">\mathbf{h}_{v}^{(k)}=\sigma\left(\sum_{u \in \mathcal{N}(v) \cup v} \alpha_{v u}^{(k)} \mathbf{W}^{(k)} \mathbf{h}_{u}^{(k-1)}\right)</script><p>其中的$\alpha _{vu}^k$表示的第$k$层的节点$v$应当对其邻接节点$u$所给予的关注。这里和原文的表述”connective strength”不同是因为我认为连通性更像是一个对称的度量方式，而其实这里关注度的给出是可以非对称的。GAT中计算该关注度的方法为：</p><script type="math/tex; mode=display">\alpha_{v u}^{(k)}=\operatorname{softmax}\left(g\left(\mathbf{a}^{T}\left[\mathbf{W}^{(k)} \mathbf{h}_{v}^{(k-1)} \| \mathbf{W}^{(k)} \mathbf{h}_{u}^{(k-1)}\right)\right)\right.</script><p>更进一步地，GAT采用了Multi-head Attention来提取更多不同方面的节点特征。而GAT将多个head之间的结果拼接起来就以为它预设每个head特征之间是相同重要的。</p><p><a href="https://arxiv.org/pdf/1803.07294.pdf" target="_blank" rel="noopener">Gated Attention Network (GAAN)</a> introduces a self-attention mechanism which computes an additional attention score for each attention head. There are other graph attention models which might be of interest. However, they do not belong to the ConvGNN framework.</p><h3 id="Improvement-in-terms-of-training-efficiency"><a href="#Improvement-in-terms-of-training-efficiency" class="headerlink" title="Improvement in terms of training efficiency"></a>Improvement in terms of training efficiency</h3><p><a href="https://arxiv.org/abs/1801.10247" target="_blank" rel="noopener">Fast Learning with Graph Convolutional Network (Fast-GCN)</a> 和GraphSage不同，它在每一层sample固定数目个node来计算图卷积，而不是对每个node sample固定数目个neighbor。It interprets graph convolutions as integral transforms of embedding functions of nodes under probability measures. Monte Carlo approximation and variance reduction techniques are employed to facilitate the training process. </p><p><a href="https://arxiv.org/pdf/1710.10568.pdf" target="_blank" rel="noopener">Stochastic Training of Graph Convolutional Networks (StoGCN)</a></p><p><img src="https://i.loli.net/2020/02/27/jQ9NpSg3H67TFqC.png" alt="image.png"></p><h3 id="Comparison-between-spectral-and-spatial-models"><a href="#Comparison-between-spectral-and-spatial-models" class="headerlink" title="Comparison between spectral and spatial models"></a>Comparison between spectral and spatial models</h3><ul><li>首先，spectral-based models的效率不如spatial-based models。spectral-based model要么需要执行特征向量计算，要么需要同时处理整个图形。空间模型则更可扩展到大型图，因为它们通过信息传播直接在图域中执行卷积。可以在一批节点而不是整个图中执行计算。</li><li>依赖于图傅立叶基础的spectral-based model无法很好地推广到新图。因为图拉普拉斯矩阵因不同的图而不同，一旦产生变化需要进行costly的谱分解运算。spatial-based model在每个节点上局部执行图卷积，可以在不同位置和结构之间轻松共享权重。</li><li>Third, spectral-based models are limited to operate on undirected graphs. Spatial-based models are more flexible to handle multi-source graph inputs such as edge inputs, directed graphs, signed graphs, and heterogeneous graphs, because these graph inputs can be incorporated into the aggregation function easily.</li></ul><h2 id="Graph-Pooling"><a href="#Graph-Pooling" class="headerlink" title="Graph Pooling"></a>Graph Pooling</h2><p>在GNN方法产生出了node feature之后，我们需要将这些node feature用于最后的下游任务。Pooling主要是通过<strong>down-sampling</strong>从而减少参数的数量来防止overfitting, permutation invariance and computational complexity issues；Readout则是从节点表示产生出图表示。这两者的原理非常相近，我们指称的pooling是这两种方法的总和。</p><p>现在最流行的pooling方法主要是mean/max/sum pooling，因为计算这些方法非常快。一些工作甚至采用了基于attention的pooling方法，但是这些reduction operation表现还是不尽如人意，因为这些方法无论图的结构/大小，最后生成的永远是fixed-length vector。<a href="https://arxiv.org/abs/1511.06391" target="_blank" rel="noopener">Set2Set</a>方法产生随着input变大而产生更大的memory，然后使用LSTM来结合所有的embedding，最后才使用reduction method。</p><p><a href="https://arxiv.org/abs/1606.09375" target="_blank" rel="noopener">Chebyshev Spectral CNN (ChebNet)</a> 中提出了一种有趣的方法：他们会首先对图的节点进行重新排列。Input graphs are first coarsened into multiple levels by the Graclus. After coarsening, the nodes of the input graph and its coarsened version are rearranged into a balanced binary tree. Arbitrarily aggregating the balanced binary tree from bottom to top will arrange similar nodes together. Pooling such a rearranged signal is much more efficient than pooling the original. <a href="https://www.cse.wustl.edu/~muhan/papers/AAAI_2018_DGCNN.pdf" target="_blank" rel="noopener">SortPooling</a>通过另一种机制来对节点进行排序。</p><p>The aforementioned pooling methods mainly consider graph features and ignore the structural information of graphs. <a href="https://arxiv.org/abs/1806.08804" target="_blank" rel="noopener">DiffPool</a> 提出了一种可微的pooling方法。</p><h2 id="Discussion-of-Theoretical-Aspects"><a href="#Discussion-of-Theoretical-Aspects" class="headerlink" title="Discussion of Theoretical Aspects"></a>Discussion of Theoretical Aspects</h2><p><strong>Receptive Field</strong>. 指的是中心节点能够“看”到多远的邻接节点。对于GCN类似的方法，每多一层graph convolution，receptive field的半径就增大1. 所以对于一个连通图来说，至少存在一个最大层数，能够让任意一个节点作为中心节点看到全局的图结构信息。</p><p><strong>VC Dimension</strong>. <a href="https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension" target="_blank" rel="noopener">Vapnik–Chervonenkis (VC) dimension</a> is a measure of the capacity (complexity, expressive power, richness, or flexibility) of a space of functions that can be learned by a statistical classification algorithm. It is defined as the cardinality of the largest set of points that the algorithm can shatter. 有研究者推导出GNN*如果使用sigmoid or tangent hyperbolic activation，则VC dimension为$O(p^4n^2)$，这意味着如果使用这些非线性函数，那么随着Model parameter number $p$还有节点数$n$的增加，模型的复杂度会快速升高。</p><p><strong>Graph Isomorphism</strong>. 如果两个图topologically identical，那我们称这两个图是同构的。<a href="https://arxiv.org/abs/1810.00826" target="_blank" rel="noopener">Graph Isomorphism Network (GIN)</a> 工作证明如果一个GNN将两个图$G_1$和$G_2$分别映射到两个不同的embedding上，那么这两个图可以通过WL-test被识别成两个异构图。该工作同时证明了GCN以及GraphSage是没有办法辨别不同的图结构的。同时该工作更进一步证明了如果一个GNN的aggregation function以及readout function都是单射（injective），那么这个GNN在辨别两个异构图的能力上和WL-test是没有差别的。</p><p><strong>Equivariance and Invariance</strong>. GNN在处理node-level task时必须是equivariant function，在处理graph-level task时必须是invariant function. 若用数学语言表达，假设$f(\mathbf{A}, \mathbf{X}) \in R^{n \times d}$ 是一个GNN函数，$\mathbf Q$为任意的置换矩阵。如果$f\left(\mathbf{Q} \mathbf{A} \mathbf{Q}^{T}, \mathbf{Q X}\right)=\mathbf{Q} f(\mathbf{A}, \mathbf{X})$，则为equivariant；如果$f\left(\mathbf{Q} \mathbf{A} \mathbf{Q}^{T}, \mathbf{Q X}\right)= f(\mathbf{A}, \mathbf{X})$，则为invariant。这也就是说，对于任意的节点顺序，GNN函数必须给出相对同节点的相同的结果。</p><h1 id="Spatial-Temporal-GNNs"><a href="#Spatial-Temporal-GNNs" class="headerlink" title="Spatial-Temporal GNNs"></a>Spatial-Temporal GNNs</h1><p>Graphs in many real-world applications are dynamic both in terms of graph structures and graph inputs. Spatial-temporal graph neural networks (STGNNs) occupy important positions <strong>in capturing the dynamicity of graphs</strong>. Methods under this category aim to model the dynamic node inputs while assuming interdependency between connected nodes. STGNNs capture spatial and temporal dependencies of a graph simultaneously. The task of STGNNs can be forecasting future node values or labels, or predicting spatial-temporal graph labels. STGNNs follow two directions, <strong>RNN-based methods</strong> and <strong>CNN-based methods</strong>.</p><p>假设图上的简单RNN采取以下公式进行第$t$步的隐藏层计算：</p><script type="math/tex; mode=display">\mathbf{H}^{(t)}=\sigma\left(\mathbf{W} \mathbf{X}^{(t)}+\mathbf{U H}^{(t-1)}+\mathbf{b}\right)</script><p>在RNN上添加graph convolution操作即有：</p><script type="math/tex; mode=display">\mathbf{H}^{(t)}=\sigma\left(\operatorname{Gconv}\left(\mathbf{X}^{(t)}, \mathbf{A} ; \mathbf{W}\right)+\operatorname{Gconv}\left(\mathbf{H}^{(t-1)}, \mathbf{A} ; \mathbf{U}\right)+\mathbf{b}\right)</script><p><strong>RNN-based approaches suffer from time-consuming iterative propagation and gradient explosion/vanishing issues.</strong> As alternative solutions, CNN-based approaches tackle spatial-temporal graphs in a non-recursive manner with the advantages of parallel computing, stable gradients, and low memory requirements.</p><h1 id="Future-Directions"><a href="#Future-Directions" class="headerlink" title="Future Directions"></a>Future Directions</h1><p><strong>Model Depth</strong>. CNN在视觉领域证实了网络层越多表现越好。然而研究工作显示GNN正相反。因为GNN中的节点表示不断地和周围进行传递，在理论上，足够多层的网络会让所有的节点表示拟合到一个single point. This raises the question of whether going deep is still a good strategy for learning graph data.</p><p><strong>Salability Trade-off</strong>. The scalability of GNNs is gained at the price of corrupting graph completeness. Whether using sampling or clustering, a model will lose part of the graph information. 采样会导致一个节点可能错过一个非常重要的节点；而聚类则导致一个节点可能失去它独特的pattern。如何平衡算法的可扩展性和图完整性是未来的一大方向。</p><p><strong>Heterogenity</strong>. 现在的GNN直接假设同质的图，但对于异质图来说，节点、边可能有不同的表示和输入（图像或文字）。</p><p><strong>Dynamicity</strong>. Graphs are in nature dynamic in a way that nodes or edges may <strong>appear or disappear</strong>, and that node/edge inputs may change time by time. New graph convolutions are needed to adapt to the dynamicity of graphs. Although the dynamicity of graphs can be partly addressed by STGNNs, few of them consider how to perform graph convolutions in the case of dynamic spatial relations.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/thunlp/GNNPapers&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;github repo “GNN pa
      
    
    </summary>
    
    
      <category term="GNN" scheme="http://www.shihaizhou.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Graph Attention Networks</title>
    <link href="http://www.shihaizhou.com/2020/02/28/Graph-Attention-Networks/"/>
    <id>http://www.shihaizhou.com/2020/02/28/Graph-Attention-Networks/</id>
    <published>2020-02-28T04:57:44.000Z</published>
    <updated>2020-02-28T05:55:11.223Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/abs/1710.10903" target="_blank" rel="noopener">paper “Graph Attention Networks”</a></li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>之前的GCN方法将中心节点的信息和K-th neighborhood进行传播，但是认为每个邻接节点的权重是相同的。这篇工作enables (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to <strong>inductive as well as transductive</strong> problems. </p><p>其中的inductive learning problem 包括了 tasks where the model has to generalize to completely unseen graphs. 广义的inductive learning和deductive learning相对：前者指的是给出例子进行归纳从而找到规律；后者指的是给出规则，让学习者运用规则到特定的例子上。</p><h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><p>在GAT中，这章描述的Graph Attention Layer会从始至终被使用。首先文章采用了一个在节点空间中共享的映射矩阵$W$，两个节点之间的相关性$e_{ij}$通过这个矩阵加上一个函数$a$进行度量，其中$a$是一个参数化的函数：</p><script type="math/tex; mode=display">e_{i j}=a\left(\mathbf{W} \vec{h}_{i}, \mathbf{W} \vec{h}_{j}\right)</script><p>需要注意的是，我们可以将这个相关性计算apply到图中的任意两个节点上而不去考虑图本身的结构。但是一般来说Attention机制被应用在1st-order neighborhood（需要注意的是<strong>中心节点在本文也被定义在1st-order  neighborhood内</strong>）上，经过归一化我们有：</p><script type="math/tex; mode=display">\alpha_{i j}=\operatorname{softmax}_{j}\left(e_{i j}\right)=\frac{\exp \left(e_{i j}\right)}{\sum_{k \in \mathcal{N}_{i}} \exp \left(e_{i k}\right)}</script><p>最后输出层的节点表示$\vec{h}_{i}^{\prime}$为：</p><script type="math/tex; mode=display">\vec{h}_{i}^{\prime}=\sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j} \mathbf{W} \vec{h}_{j}\right)</script><p>为了让self-attention在训练的时候更加稳定，可以使用Multi-head的技术。最后的表示就是多个头结果的concatenation：</p><script type="math/tex; mode=display">\vec{h}_{i}^{\prime}=\|_{k=1}^{K} \sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \vec{h}_{j}\right)</script><p>特别地，在整个网络的最后一层预测层再进行concatenation是不make sense的。通过average和延后的non-linear层我们有：</p><script type="math/tex; mode=display">\vec{h}_{i}^{\prime}=\sigma\left(\frac{1}{K} \sum_{k=1}^{K} \sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \vec{h}_{j}\right)</script><p><img src="https://i.loli.net/2020/02/28/dWKEa3iwVjNFpJh.png" alt="image.png"></p><p><strong>Discussion and Future Work</strong></p><p>The time complexity of a <strong>single GAT attention head</strong> computing $F^\prime$ features may be expressed as $O(|V|FF^\prime + |E|F^\prime)​$, where F is the number of input features. 因为这仅仅是一个head的计算复杂度，可以看到还是相对较高的。</p><p>未来可以关注的方向有：</p><ul><li>overcoming the practical problems to be able to handle larger batch sizes.</li><li>take advantage of the attention mechanism to perform a thorough analysis on the model interpretability.</li><li>extend the model to incorporate edge features (possibly indicating relationship among nodes) . </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1710.10903&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;paper “Graph Attention 
      
    
    </summary>
    
    
      <category term="GNN" scheme="http://www.shihaizhou.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Graph Convolutional Neural Networks</title>
    <link href="http://www.shihaizhou.com/2020/02/25/Graph-Convolutional-Neural-Networks/"/>
    <id>http://www.shihaizhou.com/2020/02/25/Graph-Convolutional-Neural-Networks/</id>
    <published>2020-02-25T02:14:12.000Z</published>
    <updated>2020-02-27T11:55:55.308Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/abs/1609.02907" target="_blank" rel="noopener">paper “Semi-Supervised Classification with Graph Convolutional Networks”</a></li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>本文提出了广泛流行的GCN，通过对ChebNet进行一阶近似，从而达到了以下效果：首先是一个scalable的方法框架；第二是计算复杂度和图的边集大小呈线性关系。</p><p>在本文之前的工作处理半监督节点分类问题时，大多在 loss function 上加上图的正则项$\mathcal L_{reg}$： </p><script type="math/tex; mode=display">\mathcal{L}_{\mathrm{reg}}=\sum_{i, j} A_{i j}\left\|f\left(X_{i}\right)-f\left(X_{j}\right)\right\|^{2}=f(X)^{\top} \Delta f(X)</script><p>其中$\Delta = D-A$代表的是未经过归一化的graph laplacian. The formulation of this euation relies on the assumption that connected nodes in the graph are likely to share the same label. This assumption, however, might restrict modeling capacity, as graph edges need not necessarily encode node similarity, but could containa dditional information.</p><h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><p>基于以上的动机本文提出了一个神经网络架构$f(X,A)$从而避免正则项的使用。</p><script type="math/tex; mode=display">H^{(l+1)}=\sigma\left(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)}\right)</script><p>其中 $\tilde A$ 表示加上self-loop的邻接矩阵，$\tilde D$ 为其对应的度矩阵；$\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$ 则为经过归一化的邻接矩阵。$H^{(l)}\in \mathbb R^{N\times D}$ 为第$l$ 层的的图节点表示，对应的$W^{(l)} \in \mathbb R ^{D\times D}$为第$l$层的参数矩阵，从这里我们可以看出参数矩阵和图的结构是无关的（因为和节点维数无关），从而给了GCN scalability。以上的等式优美的地方在于它是可以从Spectral-based方法推导得到的。</p><h2 id="Derivation-from-ChebNet"><a href="#Derivation-from-ChebNet" class="headerlink" title="Derivation from ChebNet"></a>Derivation from ChebNet</h2><p>ChebNet是利用矩阵上的切比雪夫多项式进行K阶近似的方法。简单来说是将图傅里叶变换中的$g_\theta$进行近似。</p><script type="math/tex; mode=display">g_{\theta^{\prime}}(\Lambda) \approx \sum_{k=0}^{K} \theta_{k}^{\prime} T_{k}(\tilde{\Lambda})</script><p>Note that this expression is now K-localized since it is a Kth-order polynomial in the Laplacian, i.e. it depends only on nodes that are at maximum K steps away from the central node (Kth-order neighborhood). </p><p>K阶近似意味着中心节点的表示仅会被周围距离为K的节点影响。本文的一个重要想法是：我们不需要在单个filter上去定义K阶近似，相反我们限制一个filter就是一阶近似，<strong>将这个一阶近似层叠K层也能够达到Kth-order neighborhood的效果</strong>。同时估计ChebNet中$\lambda_{\max}\approx 2$。一阶（线性）近似的两个自由变量分别取为相反数$\theta = \theta^ \prime_0 = -\theta^ \prime_1$。这样我们就得到最开始定义的GCN上单通道的卷积操作：</p><script type="math/tex; mode=display">\begin{align}g_{\theta^{\prime}} \star x &\approx \theta_{0}^{\prime} x+\theta_{1}^{\prime}\left(L-I_{N}\right) x \\&=\theta_{0}^{\prime} x-\theta_{1}^{\prime} D^{-\frac{1}{2}} A D^{-\frac{1}{2}} x \\&\approx \theta\left(I_{N}+D^{-\frac{1}{2}} A D^{-\frac{1}{2}}\right) x\end{align}</script><p>注意到中间的变换矩阵所有的特征值都在$[0,2]$的区间内，这会导致数值计算上的不稳定。运用renormalization trick $I_{N}+D^{-\frac{1}{2}} A D^{-\frac{1}{2}} \rightarrow \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$ 即可。</p><p>将以上的单通道卷积操作扩展到$X\in \mathbb R^{N\times C}$，即有现在的公式。该公式的计算复杂度为$O(|E|FC)$，因为矩阵$A$是一个sparse matrix。</p><script type="math/tex; mode=display">Z=\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} X \Theta</script><h2 id="Semi-Supervised-Node-Classification"><a href="#Semi-Supervised-Node-Classification" class="headerlink" title="Semi-Supervised Node Classification"></a>Semi-Supervised Node Classification</h2><p>用一个简单的两层模型来解释这个问题中GCN的forward model：</p><script type="math/tex; mode=display">Z=f(X, A)=\operatorname{softmax}\left(\hat{A} \operatorname{ReLU}\left(\hat{A} X W^{(0)}\right) W^{(1)}\right)</script><p>用以下XE loss function下进行训练：</p><script type="math/tex; mode=display">\mathcal{L}=-\sum_{l \in \mathcal{Y}_{L}} \sum_{f=1}^{F} Y_{l f} \ln Z_{l f}</script><p>示意图为以下：</p><p><img src="https://i.loli.net/2020/02/27/1mzRJDTO9cGBdZX.png" alt="image.png"></p><p><strong>Limitations and Future Work</strong></p><ul><li>For very large and densely connected graph datasets, further approximations might be necessary.</li><li>Our framework currently does not naturally support edge features and is limited to undirected graphs (weighted oo unweighted).</li><li>we implicitly assume locality (dependence on the Kth-order neighborhood for a GCN with K layers) and equal importance of self-connections vs. edges to neighboring nodes. It might be beneficial to introduce a trade-off parameter $\lambda $ in the definition of $\tilde A$: $\tilde A  = A + \lambda I_N$. </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1609.02907&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;paper “Semi-Supervised 
      
    
    </summary>
    
    
      <category term="GNN" scheme="http://www.shihaizhou.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Transformer Basics</title>
    <link href="http://www.shihaizhou.com/2020/02/24/Transformer-Basics/"/>
    <id>http://www.shihaizhou.com/2020/02/24/Transformer-Basics/</id>
    <published>2020-02-23T17:22:40.000Z</published>
    <updated>2020-05-09T06:23:34.462Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">paper: “Attention Is All You Need”</a></li><li><a href="https://zhuanlan.zhihu.com/p/44121378" target="_blank" rel="noopener">知乎文章 “Transformer详解”</a></li><li><a href="http://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">blog “The Illustrated Transformer”</a>, which is extremely helpful!!!</li></ul><h1 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h1><p>Sequence Modeling问题简述为以下：</p><blockquote><p>The encoder maps an input sequence of symbol representations $x = (x_1,\cdots,x_n)$ to a sequence of continuous representations $z = (z_1,\cdots,z_n)$. Given $z$, the decoder then generates an output sequence $y=(y_1, …, y_m)$ of symbols one element at a time. </p></blockquote><p>encoder-decoder framework是auto-regressive的，也就是说当前生成的词是conditioned on the previously generated word (previously generated symbols as additional input when generating the next). 注意：虽然我们传统意义上使用encoder-decoder时，encoder往往将variable-length的input映射成fixed-length的向量（RNN中为最后一个hidden state），但是encoder在编码时已经生成了很多的中间结果构成$z = (z_1,\cdots,z_n)$ .</p><p>Transformer的模型如下图所示。</p><p><img src="https://i.loli.net/2020/02/23/ZNT194ipKYeAzv6.png" alt="image.png"></p><p>下面是blog中更加简洁但是能看得清flow的示意图，可以看到encoder最后一层的结果会作为每一层decoder的input。</p><p><img src="https://i.loli.net/2020/05/09/c4ROe6ZKEzLbiop.png" alt="image.png"></p><h2 id="Encoder-and-Decoder-Stacks"><a href="#Encoder-and-Decoder-Stacks" class="headerlink" title="Encoder and Decoder Stacks"></a>Encoder and Decoder Stacks</h2><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><ul><li><p>由6层相同的layer组成，每层layer由3层sublayer组成。</p></li><li><p>第一层sublayer是multi-head self-attention；第二层是position-wise fully connected feed-forward network。</p></li><li><p>在两层之间加入了residual network的设计：</p><script type="math/tex; mode=display">\text{layer_norm}(x+\text{sublayer}(x))</script></li><li><p>为了使residual成为可能，model的所有向量长度都是对齐的$d_\text{model} = 512$.</p></li><li><p>Encoder是可以完全并行化处理的，因为所有的input vector是全局可见的。</p></li></ul><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><ul><li>由6层相同的layer组成，每层layer由3层sublayer组成。</li><li>在Encoder结构的基础上中间插入了一层multi-head self-attention.</li></ul><blockquote><p> We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.</p></blockquote><p>这一段话是理解Decoder过程的关键。首先要明确的是Transformer的encoding可以并行化，但是decoding确是auto-regressive的。而在Decoder中有两层不同的attention机制，包括了第一层的self-attention和第二层加入了encoder信息的attention机制。</p><p><img src="https://i.loli.net/2020/05/09/nKkf6t72AxU9aIB.png" alt="image.png"></p><p>然后是最关键的，通常来说我们认为两个模块之间是以embedding的形式来进行信息的传递的，但是注意encoder产生出的结果是一个三元组(Query, Key, Value)，其中Value就是我们通常意义上的embedding。而由于在decoder中也需要使用attention机制，所以自然地，将(Key, Value)都作为输出传给decoder。</p><p>假设当前为$i^{th}$ time step，从input一直到产生softmax输出分布的流程为：</p><ul><li>input为前$i-1$个output token embeddings, 这一点是通过mask机制来实现的</li><li>input经过positional encoding得到embedding with positional signal</li><li>input经过self-attention得到中间结果$z_1$（其中的QKV也是含参重新生成，而不是使用encoder产生的QKV）</li><li>$z_1$利用encoder产生的$(K, V)$通过encoder-decoder attention得到$o_1$, which serves as input of the next decoder block.</li><li>经过若干个decoder模块之后，通过softmax产生输出的分布，取得$i$号位置的输出</li></ul><p><img src="https://i.loli.net/2020/05/09/xdjPuXefbAU6oO2.png" alt="image.png"></p><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>Attention机制可以被看作是given a set of key-value pairs，将query映射到一个output上。output通常是weighted sum of the values, based on the similarity of the query-key pairs. </p><p><img src="https://i.loli.net/2020/02/24/8icua43VCtygQHO.png" alt="image.png"></p><p>一层Attention运行流程如下：</p><ul><li>假设input token sequence为$X = \{x_1, x_2, \cdots, x_n\}$ ，</li><li>将每一个token vector分别通过参数矩阵$W^Q, W^K, W^V$，得到以下：<ul><li>Query: $Q=\{q_1, q_2, \cdots, q_n\}$</li><li>Key: $K=\{k_1, k_2, \cdots, k_n\}$</li><li>Value: $V=\{v_1, v_2, \cdots, v_n\}$</li></ul></li><li>$Q,K$之间的dot product表示了词之间的关联系数，也就是attention机制赋予的权重</li><li>将权重和$V$进行加权平均，就得到了这一层encoding结果</li></ul><h3 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h3><p><strong>Scaled Dot-Product Attention</strong> 是本文使用的attention机制。In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$. The keys and values are also packed together into matrices $K$ and $V$. We compute the matrix of outputs as: </p><script type="math/tex; mode=display">\text { attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V</script><h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p>通过实验发现如果将$Q,K,V$这些向量分别线性映射（映射也是参数化的）到不同的空间，然后并行地计算Attention，最后concat并映射回原来的空间，将会非常有提升（见图）。这样的技术叫做<strong>Multi-Head Attention</strong>.</p><p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</p><script type="math/tex; mode=display">\begin{aligned} \text { multi_head }(Q, K, V) &=\text { concat }\left(\text { head }_{1}, \ldots, \text { head }_{\mathrm{h}}\right) W^{O} \\ \text { where head }_{\mathrm{i}} &=\text { attention }\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \end{aligned}</script><p>在本文中采用了 $h=8$ parallel attention layers，在不同head的空间中数据维数显著减小，提升了运算表现。</p><h2 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h2><p><strong>Feed-Forward Networks</strong> 定义为线性映射+RELU+线性映射：</p><script type="math/tex; mode=display">\mathrm{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}</script><h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h2><p>如何加入当前位置的位置信息，作者给出了一个sine+cosine的函数：</p><script type="math/tex; mode=display">\begin{array}{c}{P E_{(p o s, 2 i)}=\sin \left(\text {pos} / 10000^{2 i / d_{\text {model }}}\right)} \\ {P E_{(\text {pos, }, 2 i+1)}=\cos \left(\text {pos} / 10000^{2 i / d_{\text {model }}}\right)}\end{array}</script><p>其中$pos$为当前位置而$i$为当前embedding的dimension。这样的函数<strong>能够帮助编码模型的相对位置信息</strong>，因为对于一个固定的relative offset $k$，$PE_{pos+k}$能够被表示成$PE_{pos}$的线性函数（$\sin(\lambda k)$以及$\cos(\lambda k)$为常数，根据三角展开式得到）。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;paper: “Attention I
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
  </entry>
  
  <entry>
    <title>Grammar of Java</title>
    <link href="http://www.shihaizhou.com/2020/02/23/Algorithms-and-Grammar-of-java/"/>
    <id>http://www.shihaizhou.com/2020/02/23/Algorithms-and-Grammar-of-java/</id>
    <published>2020-02-23T12:37:01.000Z</published>
    <updated>2020-02-23T12:37:54.138Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Grammar"><a href="#Grammar" class="headerlink" title="Grammar"></a>Grammar</h1><h3 id="BufferedReader-比较快"><a href="#BufferedReader-比较快" class="headerlink" title="BufferedReader (比较快)"></a>BufferedReader (比较快)</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.*;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> BufferedReader reader = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(System.in));</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">String[] line = reader.nextLine().split();</span><br><span class="line">        <span class="keyword">int</span> num = Integer.parseInt(line[<span class="number">0</span>]);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Scanner-输入字符串"><a href="#Scanner-输入字符串" class="headerlink" title="Scanner 输入字符串"></a>Scanner 输入字符串</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Scanner scanner = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"><span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">        System.out.print(<span class="string">"Fuck you."</span>);</span><br><span class="line">        String input = scanner.nextLine();</span><br><span class="line">        System.out.println(<span class="string">"input = "</span> + input);</span><br><span class="line">        System.out.printf(<span class="string">"%d"</span>, <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Input-Integer"><a href="#Input-Integer" class="headerlink" title="Input Integer"></a>Input Integer</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">String input = scanner.nextLine();</span><br><span class="line"><span class="keyword">int</span> num = Integer.parseInt(input);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> num = Scanner.nextInt(); <span class="comment">// very slow</span></span><br><span class="line"><span class="keyword">int</span> num = Integer.parseInt(Scanner.next())</span><br></pre></td></tr></table></figure><h3 id="Convert-Integer-to-String"><a href="#Convert-Integer-to-String" class="headerlink" title="Convert Integer to String"></a>Convert Integer to String</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">str = Integer.toString(num);</span><br><span class="line"></span><br><span class="line">str = Integer.toBinaryString(num);</span><br><span class="line">str = Integer.toHexString(num);</span><br><span class="line">str = Integer.toOctalString(num);</span><br><span class="line"></span><br><span class="line">str = Integer.toString(num, radix=<span class="number">10</span>);</span><br></pre></td></tr></table></figure><h3 id="Convert-Double-to-String"><a href="#Convert-Double-to-String" class="headerlink" title="Convert Double to String"></a>Convert Double to String</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">str = Double.toString(num);</span><br><span class="line"></span><br><span class="line">str = Double.toBinaryString(num);</span><br><span class="line">str = Double.toHexString(num);</span><br><span class="line">str = Double.toOctalString(num);</span><br><span class="line"></span><br><span class="line">str = Double.toString(num, radix=<span class="number">10</span>);</span><br></pre></td></tr></table></figure><h3 id="String-Methods"><a href="#String-Methods" class="headerlink" title="String Methods"></a>String Methods</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">char</span>[] charArray = &#123;<span class="string">'0'</span>, <span class="string">'f'</span>, <span class="string">'u'</span>&#125;;</span><br><span class="line">String str = <span class="keyword">new</span> String(charArray);</span><br><span class="line"></span><br><span class="line"><span class="comment">// length of the string</span></span><br><span class="line">length = str.length();</span><br><span class="line"></span><br><span class="line"><span class="comment">// index</span></span><br><span class="line"><span class="keyword">char</span> a = str.charAt(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// substring</span></span><br><span class="line">str.substring(<span class="number">0</span>, str.length()-<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// reverse a string</span></span><br><span class="line">str = <span class="keyword">new</span> StringBuilder(str).reverse().toString();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 格式化输出 double, int 等</span></span><br><span class="line">String.format(<span class="string">"%d"</span>, num);</span><br><span class="line">String.format(<span class="string">"%.1f"</span>, num);</span><br><span class="line"></span><br><span class="line"><span class="comment">// string的分开，和python差不多</span></span><br><span class="line">String[] splited = str.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// string的头尾去掉空格</span></span><br><span class="line">String trimed = str.trim();</span><br><span class="line"></span><br><span class="line"><span class="comment">// replace: 非正则表达式，返回一个新的字符串，只能够替换char</span></span><br><span class="line">str.replace(<span class="keyword">char</span> oldChar, <span class="keyword">char</span> newChar);</span><br><span class="line"></span><br><span class="line"><span class="comment">// replaceAll</span></span><br><span class="line">str.repalceAll(<span class="string">"^\+"</span>, <span class="string">""</span>); <span class="comment">// replace all the starting "</span></span><br><span class="line"></span><br><span class="line">str = <span class="string">"fuck"</span>;</span><br><span class="line"><span class="comment">// compareTo, compareToIgnoreCase</span></span><br><span class="line">str.compareTo(<span class="string">"fuck"</span>);</span><br><span class="line">str.compareToIgnoreCase(<span class="string">"FUCk"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// endsWith, returns boolean</span></span><br><span class="line">str.endsWith(<span class="string">"ck"</span>);</span><br><span class="line"><span class="comment">// startsWith, returns boolean</span></span><br><span class="line">str.startsWith(<span class="string">"wh"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// equals, equalsIgnoreCase</span></span><br><span class="line">str.equalsIgnoreCase(<span class="string">"Fuck"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 找到第一个出现的位置</span></span><br><span class="line">indexOf(<span class="keyword">char</span> chr);</span><br><span class="line">indexOf(<span class="keyword">char</span> chr, <span class="keyword">int</span> fromIndex);</span><br><span class="line">indexOf(String str, <span class="keyword">int</span> fromIndex);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 找到最后一个出现的位置</span></span><br><span class="line">lastIndexOf(<span class="keyword">char</span> chr);</span><br><span class="line"></span><br><span class="line"><span class="comment">// regionMatches, 测试两个字符串区域是否相等</span></span><br><span class="line">str.regionMatches(<span class="keyword">int</span> offset, String otherString, <span class="keyword">int</span> offset_hat, <span class="keyword">int</span> length);</span><br><span class="line"></span><br><span class="line"><span class="comment">// concat, 在仅仅只有两个string的时候是最快的</span></span><br><span class="line">str.concat(<span class="string">"you"</span>)</span><br></pre></td></tr></table></figure><h3 id="Arrays-and-ArrayList"><a href="#Arrays-and-ArrayList" class="headerlink" title="Arrays and ArrayList"></a>Arrays and ArrayList</h3><p>Java中的array是在heap中创建的，每个array的元素都会被自动初始化，该值根据不同的类型而定。</p><ul><li>int - 初始化为0</li><li>boolean - 初始化为false</li><li>double - 初始化为0.0</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// array</span></span><br><span class="line"><span class="keyword">int</span>[] nums = &#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>&#125;;</span><br><span class="line"><span class="keyword">int</span>[] nums = <span class="keyword">new</span> nums[<span class="number">6</span>];</span><br><span class="line"><span class="keyword">int</span>[] nums = <span class="keyword">new</span> nums[]&#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// iterate</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.length; i++)</span><br><span class="line">    System.out.println(nums[i]);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i: nums)</span><br><span class="line">    System.out.println(i);</span><br><span class="line"></span><br><span class="line"><span class="comment">// size</span></span><br><span class="line">len = nums.length;</span><br><span class="line"></span><br><span class="line"><span class="comment">// array的填充(常作为初始化的方法)</span></span><br><span class="line"><span class="keyword">int</span>[] nums = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">5</span>];</span><br><span class="line">Arrays.fill(nums, Integer.MAX_VALUE);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 二维数组的填充</span></span><br><span class="line"><span class="keyword">int</span>[][] nums = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">5</span>][<span class="number">5</span>];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span>[] row : nums)</span><br><span class="line">    Arrays.fill(row, Integer.MAX_VALUE);</span><br><span class="line"></span><br><span class="line"><span class="comment">// ArrayList</span></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"></span><br><span class="line"><span class="comment">// creation, 不能用正常数据类型(int, double, char),只能用Integer, Double, </span></span><br><span class="line">ArrayList&lt;Integer&gt; arr = <span class="keyword">new</span> ArrayList&lt;Integer&gt;();</span><br><span class="line"><span class="comment">// Arrays.asList() to initialize a list：</span></span><br><span class="line">List&lt;String&gt; tmp = Arrays.asList(<span class="string">"1"</span>, <span class="string">"2"</span>, <span class="string">"4"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 添加对象进入list</span></span><br><span class="line"><span class="comment">// 在index i处添加对象，i之后的对象自动往后移</span></span><br><span class="line">arr.add(i, obj);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在list末尾添加对象</span></span><br><span class="line">arr.add(obj);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 移去index i处的元素</span></span><br><span class="line">arr.remove((<span class="keyword">int</span>)i);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 移去对象</span></span><br><span class="line">arr.remove(obj);</span><br><span class="line"></span><br><span class="line"><span class="comment">// List转换为array</span></span><br><span class="line">arr.toArray();</span><br></pre></td></tr></table></figure><h3 id="Map的使用"><a href="#Map的使用" class="headerlink" title="Map的使用"></a>Map的使用</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Map 创建</span></span><br><span class="line">Map&lt;Character, Integer&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">// size</span></span><br><span class="line">map.size()</span><br><span class="line"></span><br><span class="line"><span class="comment">// clear</span></span><br><span class="line">map.clear();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 含有key/value</span></span><br><span class="line">map.containsKey();</span><br><span class="line">map.containsValue();</span><br><span class="line"></span><br><span class="line"><span class="comment">// get, put, remove</span></span><br><span class="line">map.get(k);</span><br><span class="line">map.put(k, v);</span><br><span class="line">map.remove(k);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 是否空</span></span><br><span class="line">map.isEmpty();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 键值集合</span></span><br><span class="line">map.keySet();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 快速加入或者自增</span></span><br><span class="line">map.put(key, map.getOrDefault(key, <span class="number">0</span>)+<span class="number">1</span>);</span><br></pre></td></tr></table></figure><h3 id="Queue的使用"><a href="#Queue的使用" class="headerlink" title="Queue的使用"></a>Queue的使用</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//声明</span></span><br><span class="line">Queue&lt;Integer&gt; queue = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">//添加弹出元素</span></span><br><span class="line">queue.offer(<span class="number">5</span>); <span class="comment">// if full return false</span></span><br><span class="line">queue.poll();<span class="comment">// if empty return null</span></span><br><span class="line">queue.peek();<span class="comment">// if empty return null</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//vevtor中继承的方法</span></span><br><span class="line">queue.size();</span><br><span class="line">queue.isEmpty();</span><br><span class="line"></span><br><span class="line"><span class="comment">// PriorityQueue</span></span><br><span class="line">Queue&lt;Integer&gt; queue = <span class="keyword">new</span> PriorityQueue&lt;&gt;((a,b) -&gt; b-a); <span class="comment">// maxHeap</span></span><br></pre></td></tr></table></figure><h3 id="Stack"><a href="#Stack" class="headerlink" title="Stack"></a>Stack</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 初始化</span></span><br><span class="line">Stack&lt;Integer&gt; st = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line"></span><br><span class="line">st.peek();</span><br><span class="line">st.push();</span><br><span class="line">st.pop();</span><br><span class="line">st.isEmpty();</span><br></pre></td></tr></table></figure><p>array用Arrays.sort()，如果是普通的基本数据类型，则没有lambda表达式。</p><h3 id="List-数组"><a href="#List-数组" class="headerlink" title="List 数组"></a>List 数组</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 注意后面的ArrayList不带尖括号</span></span><br><span class="line">List&lt;Integer&gt;[] arr = <span class="keyword">new</span> ArrayList[<span class="number">10</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment">// 数组排序, use new Comparator statement</span></span><br><span class="line">Arrays.sort(arr, <span class="keyword">new</span> Comparator&lt;List&lt;Integer&gt;&gt;() &#123;</span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(List&lt;Integer&gt; a, List&lt;Integer&gt;)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> a.size() - b.size();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 数组每个的初始化</span></span><br><span class="line"><span class="keyword">for</span> (i=<span class="number">0</span>;i&lt;arr.lengh;i++) arr[i] = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br></pre></td></tr></table></figure><h3 id="Arrays"><a href="#Arrays" class="headerlink" title="Arrays"></a>Arrays</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span>[] arr = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">10</span>];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) arr[i] = i;</span><br><span class="line"><span class="comment">// 1. sort, 适用基本数据类型</span></span><br><span class="line">Arrays.sort(arr); <span class="comment">// ascending</span></span><br><span class="line">Collections.sort(arr, Collections.reverseOrder()); <span class="comment">// descending</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// sort, 非基本数据类型：array of lists 为例 (new Comparator&lt;class&gt;() &#123;@Override ...&#125; ) </span></span><br><span class="line">Arrays.sort(lists, <span class="keyword">new</span> Comparator&lt;List&lt;Integer&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(List&lt;Integer&gt; a1, List&lt;Integer&gt; a2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> -a1.get(<span class="number">0</span>) + a2.get(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. binarySearch, 如果不在数组中返回(-insertIndex-1)</span></span><br><span class="line">Arrays.binarySearch(arr, <span class="number">10</span>); <span class="comment">// return </span></span><br><span class="line">Arrays.binarySearch(arr, fromIndex, toIndex, key);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. copyOfRange</span></span><br><span class="line"><span class="keyword">int</span>[] copy = Arrays.copyOfRange(arr, <span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4. equals: shallow equal 用在单个元素上</span></span><br><span class="line">Arrays.equals(arr, copy);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5. deepEquals: deep equal 用在元素也是数组上，递归进行比较</span></span><br><span class="line">Arrays.deepEquals(arr, copy);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 6. toString (deepToString)</span></span><br><span class="line">System.out.println(Arrays.toString(arr));</span><br></pre></td></tr></table></figure><h3 id="Functional-Programming-in-Java-OJ系统中不一定快"><a href="#Functional-Programming-in-Java-OJ系统中不一定快" class="headerlink" title="Functional Programming in Java (OJ系统中不一定快)"></a>Functional Programming in Java (OJ系统中不一定快)</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// WRONG!!!!!!</span></span><br><span class="line">String[] strings = Array.stream(arr).map(x-&gt;String.valueOf(x)).toArray(String[]::<span class="keyword">new</span>);</span><br></pre></td></tr></table></figure><p><strong>以上代码会报错，</strong>这是由于Java实现了IntStream, LongStream, DoubleStream三个基本类型的stream，这些基本类型的stream有更好的性能，但是需要注意的是这些基本类型的stream的map函数只支持自身类型-&gt;自身类型(int-&gt;int/long-&gt;long/double-&gt;double)的映射，如果需要进行String类型的转换那么我们需要使用mapToObj()函数。</p><p>三个基本类型的Stream相互转化的函数为mapToDouble(), mapToInt(), mapToLong().</p><p><strong>应当优先使用基本类型的stream</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// List, Array</span></span><br><span class="line">Stream&lt;Integer&gt; stream = Arrays.stream(arr);</span><br><span class="line">Stream&lt;Integer&gt; stream = list.stream();</span><br><span class="line"></span><br><span class="line"><span class="comment">// map (Integer / int)</span></span><br><span class="line">arr = Arrays.stream(arr).map(x-&gt;x*<span class="number">2</span>).toArray(); <span class="comment">// no function passed to toArray() call.</span></span><br><span class="line">String[] strings = Array.stream(arr).mapToObj(x-&gt;String.valueOf(x)).toArray(String[]::<span class="keyword">new</span>); </span><br><span class="line">String[] strings = Array.stream(arr).mapToObj(x-&gt;String.valueOf(x)).toArray(n-&gt;<span class="keyword">new</span> String[n]);</span><br><span class="line"></span><br><span class="line"><span class="comment">// filter</span></span><br><span class="line">arr = Arrays.stream(arr).filter(x-&gt;x&gt;<span class="number">10</span>).toArray();</span><br><span class="line">String[] strings = list.stream().filter(x-&gt;x&gt;<span class="number">10</span>).toArray(String[]::<span class="keyword">new</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// reduce</span></span><br><span class="line"><span class="keyword">int</span> sum = Arrays.stream(arr).sum(); <span class="comment">// 只能够用在基本类型stream上</span></span><br><span class="line"><span class="keyword">int</span> sum = Arrays.stream(arr).reduce(<span class="number">0</span>, (a,b)-&gt;a+b);</span><br><span class="line"></span><br><span class="line"><span class="comment">// forEach 可以很帅地打印</span></span><br><span class="line">Arrays.stream(arr).forEach(x-&gt;System.out.println(x));</span><br><span class="line"></span><br><span class="line"><span class="comment">// IntStream的很帅的用法</span></span><br><span class="line"><span class="comment">// 快速初始化</span></span><br><span class="line">IntStream stream = IntStream.of(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>);</span><br><span class="line">IntStream range = IntStream.range(<span class="number">1</span>, <span class="number">10</span>);</span><br><span class="line">IntStream rangeClosed = IntStream.rangeClosed(<span class="number">1</span>, <span class="number">10</span>); <span class="comment">// 包含10的range</span></span><br><span class="line">IntStream infinite = IntStream.iterate(<span class="number">1</span>, operand-&gt;oprand + <span class="number">2</span>); <span class="comment">// 生成一个无限生成奇数的generator</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span>[] arr = <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, -<span class="number">1</span>&#125;;</span><br><span class="line">String[] strs = <span class="keyword">new</span> String[]&#123;<span class="string">"what"</span>, <span class="string">"the"</span>, <span class="string">"fuck"</span>&#125;;</span><br><span class="line"><span class="comment">// 求最大、最小、求和 (如果没有 .get() / .orElse() 则返回的是optional)</span></span><br><span class="line"><span class="keyword">int</span> max = Arrays.stream(arr).max().orElse(Integer.MIN_VALUE);</span><br><span class="line"><span class="keyword">int</span> min = Arrays.stream(arr).min().orElse(Integer.MAX_VALUE);</span><br><span class="line"><span class="keyword">int</span> sum = Arrays.stream(arr).sum().orElse(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">String max = Arrays.stream(strs).max((a,b)-&gt;a.compareTo(b)).orElse(<span class="string">""</span>);</span><br><span class="line">String sum = Arrays.stream(strs).reduce(<span class="string">""</span>, (a,b)-&gt;a.concate(b));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 另一种有趣写法</span></span><br><span class="line"><span class="keyword">int</span> max = Arrays.stream(arr).reduce(Integer.MIN_VALUE, Math::max);</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Grammar&quot;&gt;&lt;a href=&quot;#Grammar&quot; class=&quot;headerlink&quot; title=&quot;Grammar&quot;&gt;&lt;/a&gt;Grammar&lt;/h1&gt;&lt;h3 id=&quot;BufferedReader-比较快&quot;&gt;&lt;a href=&quot;#BufferedReader-
      
    
    </summary>
    
    
      <category term="reality" scheme="http://www.shihaizhou.com/tags/reality/"/>
    
  </entry>
  
  <entry>
    <title>Graph Neural Networks - Basics</title>
    <link href="http://www.shihaizhou.com/2020/02/22/Graph-Neural-Networks-Basics/"/>
    <id>http://www.shihaizhou.com/2020/02/22/Graph-Neural-Networks-Basics/</id>
    <published>2020-02-22T03:28:57.000Z</published>
    <updated>2020-02-29T04:55:04.756Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong>:</p><ul><li><a href="https://arxiv.org/abs/1901.00596" target="_blank" rel="noopener">paper “A Comprehensive Survey on Graph Neural Networks”</a></li><li><a href="https://csustan.csustan.edu/~tom/Clustering/GraphLaplacian-tutorial.pdf" target="_blank" rel="noopener">Graph Laplacian Tutorial</a></li><li><a href="https://www.quora.com/Whats-the-intuition-behind-a-Laplacian-matrix-Im-not-so-much-interested-in-mathematical-details-or-technical-applications-Im-trying-to-grasp-what-a-laplacian-matrix-actually-represents-and-what-aspects-of-a-graph-it-makes-accessible" target="_blank" rel="noopener">Graph Laplacian explanation on Quora</a></li><li><a href="https://www.zhihu.com/question/26822364" target="_blank" rel="noopener">Laplacian explanation on Zhihu</a></li><li><a href="http://www.norbertwiener.umd.edu/Research/lectures/2014/MBegue_Prelim.pdf" target="_blank" rel="noopener">Graph Fourier Transformation Tutorial</a></li><li><a href="https://zhuanlan.zhihu.com/p/67336297" target="_blank" rel="noopener">拉普拉斯算子和拉普拉斯矩阵</a></li><li><a href="https://arxiv.org/abs/1606.09375" target="_blank" rel="noopener">paper “Chebyshev Spectral CNN (ChebNet)”</a> </li><li><a href="https://arxiv.org/abs/0912.3848" target="_blank" rel="noopener">paper “Wavelets on Graphs via Spectral Graph Theory”</a></li></ul><h1 id="Notations"><a href="#Notations" class="headerlink" title="Notations"></a>Notations</h1><p><strong>Definition 1</strong> (Graph). A <strong>graph</strong> is represented as $G=(V,E)$, where $V$ is the set of vertices or nodes, and $E$ is the set of edges. In the graph, let $v_i \in V$ to denote a node and $e_{ij}=(v_i, v_j) \in E$ to denote an edge pointing from $v_i$ to $v_j$. （文章中写的是$v_j$ to $v_i$）</p><p><strong>Definition 2</strong> (Adjacency matrix &amp; degree matrix). The <strong>adjacency matrix</strong> $A$ of a graph is a $n\times n$ matrix with</p><script type="math/tex; mode=display">A_{ij} = \left\{    \begin{array}{lr}    1, \quad (e_{ij} \in E)\\     0. \quad (e_{ij} \notin E)    \end{array}\right.</script><p>The <strong>degree matrix</strong> $D$ is a $n\times n$ diagonal matrix with</p><script type="math/tex; mode=display">D_{ii}=\sum_{j=1}^n A_{ij} .</script><p><strong>Lemma 1</strong> (Power of adjacency matrix). Let $G$ be a weighted graph, with binary adjacency matrix $A$.  Let $\tilde A$ be the adjacency matrix with unit loops added on every vertex, e.g. $A_{m,n} =\tilde A_{m,n}$ for $m\neq n$ and $\tilde A_{m,n} =1$ for $m=n$. Then for each $s &gt; 0$, $(\tilde A^s)_{m,n}$ equals the number of paths of length s connecting $m$ and $n$, and $(\tilde A^s)_{m,n}$ equals the number of all paths of length $r \leq s$ connecting m and n. <strong>（注意：这条引理对于GNN之后推导Locality非常重要）</strong></p><p><strong>Definition 3</strong> (Neighborhood of a node). The <strong>neighborhood of a node</strong> $v$ is defined as </p><script type="math/tex; mode=display">N(v)=\{u \in V | (u,v) \in E\}</script><p><strong>Definition 4</strong> (Node attributes). The <strong>node attributes of the graph</strong> is represented as $X$, where $X \in \mathbb R ^{n\times d}$ is called as node feature matrix. $x_v \in \mathbb R^d$ represents the feature vector of node $v$. </p><p><strong>Definition 5</strong> (Edge attributes). The <strong>edge attributes of the graph</strong> is represented as $X^e$, where $X^e \in \mathbb R^{m\times c}$ is an edge feature matrix with $x_{v,u}^e ∈ \mathbb R^c$ representing the feature vector of an edge $(v, u)$.</p><p><strong>Definition 6</strong> (Spatial-Temporal Graph). A <strong>spatial-temporal graph</strong> is an attributed graph where the node attributes change dynamically over time. The spatial-temporal graph is defined as $G^{(t)} =(V,E,X^{(t)})$ with $X^{(t)} ∈ \mathbb R^{n\times d}$. </p><p>常用的Notation表示一览：</p><p><img src="https://i.loli.net/2020/02/19/iSPnINotEzhWHxA.png" alt="image.png"></p><h1 id="Graph-Laplacian"><a href="#Graph-Laplacian" class="headerlink" title="Graph Laplacian"></a>Graph Laplacian</h1><p>这里我们主要讨论的是undirected graph，对于directed graph来说有其他更多的工作。</p><h2 id="How-to-Define-Graph-Laplacian"><a href="#How-to-Define-Graph-Laplacian" class="headerlink" title="How to Define Graph Laplacian?"></a>How to Define Graph Laplacian?</h2><p><strong>Definition 7</strong> (Euclidian Laplacian). Laplacian operator in Euclidian space is defined by</p><script type="math/tex; mode=display">\Delta f = \nabla^2 f = \nabla \cdot \nabla f</script><p>Equivalently, the Laplacian of $f$ is the sum of all the <em>unmixed</em> second partial derivatives in the Cartesian coordinates $x_i$:</p><script type="math/tex; mode=display">\Delta f = \sum_{i=1}^n \frac {\partial^2 f}{\partial x^2_i}</script><p>在欧式空间的拉普拉斯算子就是一个实值函数“梯度的散度”。从物理上来理解拉普拉斯算子的重要性：<strong>标量函数的梯度往往是一种“驱动力”（或者叫“势”），而针对“驱动力”求散度就可以知道空间中“源”的分布。</strong></p><p>将欧式空间中的Laplacian operator推广到Graph上，需要回答以下几个问题：</p><ul><li>What is the meaning of a function over a graph?</li><li>How do we define the gradient of a graph function?</li><li>How do we define the divergence of the graph gradient?</li></ul><h2 id="Graph-Laplacian-Defintion-amp-Intuition"><a href="#Graph-Laplacian-Defintion-amp-Intuition" class="headerlink" title="Graph Laplacian Defintion &amp; Intuition"></a>Graph Laplacian Defintion &amp; Intuition</h2><p>首先为了定义图函数，我们将图在 $xOy$ 平面上延展开，将节点看成自变量（也就是说节点集合是 $xOy$ 平面上的一个子集），定义函数$ f:V\rightarrow \mathbb R$ 是一个从节点到实数的映射。</p><p>接着为了定义图函数上的gradient，回忆起在数值分析中我们学过的difference的概念，也就是在离散空间中梯度的近似近似。同样的我们可以定义：在图上，两个连通节点函数值的difference就是图上的gradient。Gradient of the function along the edge $e=(u,v)$ is given by $\text{grad}(f)|_e=f(u)−f(v)$. </p><p>在这个基础上我们先给出graph laplacian的定义：</p><p><strong>Definition 8</strong> (Graph Laplacian operator). <strong>Graph Laplacian</strong> operator $\Delta$ with given graph function $f$ and given node $x$ is defined as</p><script type="math/tex; mode=display">\Delta f(x) = \sum_{y \sim x} f(x)-f(y)</script><p>其中$y \sim x$表示节点$y$是$x$的邻接点，也可以表示成$y \in N(x)$. 这条式子定义了一个节点的Laplacian等于该节点所有的incident edge上gradient的和。这就导致了一些疑问：<strong>Laplacian是梯度的散度，按照道理应当是gradient的导数的和，为什么这里少了一次求导呢？</strong></p><p>为了解决这个疑问，我们回顾在离散空间中的Laplacian是怎样定义的。在一维离散空间上的Laplacian是二阶差分（second-order difference）：</p><script type="math/tex; mode=display">\Delta f(x) = \frac{f(x+1)-2f(x)+f(x-1)}{1^2}</script><p>而在二维离散空间上的Laplacian算子是矩阵：</p><script type="math/tex; mode=display">L=\left[\matrix{0 & 1 & 0 \\1 & -4 & 1 \\0 & 1 & 0 \\}\right]</script><p>该矩阵可以看作是两个正交方向（$x$方向和$y$方向）上二阶差分的叠加。通过对比以上的定义，我们需要解决一个问题：如何在graph上计算二阶差分？事实上laplacian在直观上没有办法计算二阶差分，因为<strong>在同一个维度（边）上找不到3个节点</strong>。为了直观地理解graph laplacian，我们需要重新回到向量场散度的定义。</p><p><strong>Definition 9</strong> (Divergence). The <strong>divergence of a vector field</strong> $F(x)$ at a point $x_0$ is defined as the limit of the ratio of the surface integral of $F$ out of the surface of a closed volume $V$ enclosing $x_0$ to the volume of $V$, as $V$ shrinks to zero. </p><script type="math/tex; mode=display">\left.\operatorname{div} \mathbf{F}\right|_{\mathbf{x}_{0}}=\lim _{V \rightarrow 0} \frac{1}{|V|} \int_{S(V)} \mathbf{F} \cdot \hat{\mathbf{n}} d S</script><p>直观上来讲，散度是计算一点附近邻域内通量的极限。为了将这个概念推广到图结构的离散空间中，我们可以将图结构重新映射到离散的三维空间，我们做以下几个假设：</p><ul><li>空间中体积的单位元（最小元）是$1\times 1\times 1$的小立方块；</li><li>图节点是一个单位元；</li><li>相邻节点有一个面（边）紧贴在一起，在这个面上根据边的梯度定义梯度向量。</li></ul><p>在这个空间中，节点、边、梯度向量场都是离散的。考察具体例子：图$G=(V,E)$以及在图上定义的实值函数 $f:V \rightarrow \mathbb R$，其中$V=\{v_1, v_2, v_3\}$，$E=\{(v_1,v_2), (v_1,v_3)\}$。将这个图映射到我们以上定义的离散空间中，有以下：</p><p><img src="https://i.loli.net/2020/02/21/tSsVPIxkZH6EfnN.png" alt="image.png"></p><p>该图中的两个箭头表示的梯度向量场分别为 $f(v_1)-f(v_2)$ 以及 $f(v_1)-f(v_3)$. 除此之外的空间中的向量均为$0$向量。那么根据定义，关于节点$v_1$的Laplacian为：</p><script type="math/tex; mode=display">\begin{align}\left.\operatorname{div} \mathbf{F}\right|_{\mathbf{v}_{1}} &=\lim _{V \rightarrow 0} \frac{1}{|V|} \int_{S(V)} \mathbf{F} \cdot \hat{\mathbf{n}} d S \\&= \frac{1}{|1^3|} [1^2(f(v_1)-f(v_2)) + 1^2(f(v_1)-f(v_2))] \\&= \sum_{v_j \sim v_1}f(v_1)-f(v_j)\end{align}</script><p>对于更高维度的空间（节点度数$&gt;$8），因为我们认为不同的边是相互正交的，故而也是适用的。从而我们给出了graph laplacian一个更加直观的、符合原定义的解释。</p><h2 id="Incidence-Matrix"><a href="#Incidence-Matrix" class="headerlink" title="Incidence Matrix"></a>Incidence Matrix</h2><p>更进一步，我们可以引入关联矩阵（Incidence Matrix）定义一个图函数对于每一条边的gradient：</p><script type="math/tex; mode=display">\text{grad}(f) = K^Tf</script><p>where $K$ is an $V\times E$ matrix called the <strong>incidence matrix</strong>. $K$ is constructed as follows: For every edge $e=(u,v)$ in the graph, we assign $K_{u,e}=+1$ and $K_{v,e}=-1$. 我们可以将graph gradient看成是一个新的定义在edge上的函数：$g=\text{grad}(f)$.</p><p>关于incidence matrix，有两点需要注意。首先在图论中有一个非常重要的性质：我们认为<strong>图上的边相互正交</strong>。接着我们可以把一条边看作是空间中的一个维度，所有的边就构成了一个$|E|$维的单位正交基，随意指定维度坐标轴的正向（也就是指定incidence matrix上+1和-1的位置），这样用边集作为基表示的节点集就是incidence matrix.</p><p><img src="https://i.loli.net/2020/02/20/6cSpKACveBZmXYJ.png" alt="image.png"></p><p>（注意：graph laplacian可以作为母定义来推导出网格状的二阶差分，但是和网格状的二阶差分相差一个负号）</p><p>在graph gradient的基础上定义散度：$\text{div}(g)=Kg$. 而根据定义梯度的散度就是Laplacian，由此我们得到以下定义</p><p><strong>Definition 8*</strong> (Graph Laplacian operator). <strong>Graph Laplacian</strong> operator $\Delta$ with given graph function $f$ is defined as</p><script type="math/tex; mode=display">\Delta f = \text{div}(\text{grad}(f)) = KK^Tf</script><p><strong>Definition 10</strong> (Standard Graph Laplacian Matrix). <strong>Graph Laplacian Matrix</strong> $L$ is a $|V|\times |V|$ matrix defined by</p><script type="math/tex; mode=display">L = KK^T=D-A</script><p>where $K$, $A$ and $D$ is the incidence matrix, adjacency matrix and degree matrix respectively of the given graph. </p><p>Graph Laplacian和smoothness of graph有一些关联。<strong>Dirichlet Energy</strong> 定义了图的某种势能，它和variance的定义有些类似：相邻节点的function value应当和他们的edge weight成负相关。Dirichlet Energy可以由Graph Laplacian表示：</p><script type="math/tex; mode=display">E(f)=\frac{1}{2}\sum_{u∼v}w_{uv}(f(u)-f(v))^2=\|K^Tf\|^2=f^TLf</script><p>在单位球面（$f^Tf=1​$）上能够minimize Dirichlet energy的 $f​$ 就是 $L​$ 的smallest nonzero eigenvector。关于这部分的内容在<a href="https://www.quora.com/Whats-the-intuition-behind-a-Laplacian-matrix-Im-not-so-much-interested-in-mathematical-details-or-technical-applications-Im-trying-to-grasp-what-a-laplacian-matrix-actually-represents-and-what-aspects-of-a-graph-it-makes-accessible" target="_blank" rel="noopener">Graph Laplacian explanation on Quora</a>有更多的解释。</p><h1 id="Graph-Convolution"><a href="#Graph-Convolution" class="headerlink" title="Graph Convolution"></a>Graph Convolution</h1><p><strong>Definition 11</strong> (Normalized Graph Laplacian Matrix). The <strong>normalized graph laplacian matrix</strong> $L_n$ is defined as</p><script type="math/tex; mode=display">L_n =I-D^{- \frac{1}{2} } A D^{- \frac{1}{2} }</script><p><strong>Lemma 1*</strong> (Locality of High Power of Laplacian matrix). Let $G$ be a weighted graph, $L$ the graph Laplacian (normalized or non-normalized) and $s &gt; 0$ an integer. For any two vertices $m$ and $n$, if $d_G(m, n) &gt; s$ then $(L^s)_{m,n} = 0​$. 这条引理告诉我们高次的graph laplacian具有高次的局部连通性，具体的证明参考<a href="https://arxiv.org/abs/0912.3848" target="_blank" rel="noopener">paper “Wavelets on Graphs via Spectral Graph Theory”</a>。</p><p><strong>Lemma 2</strong> (Spectral Decomposition). If $A$ is a real symetric matrix, then it can be decomposed as</p><script type="math/tex; mode=display">A =Q\Lambda Q^T</script><p>where $Q$ is an orthogonal matrix whose columns are the eigenvectors of $A$, and $\Lambda$ is a diagonal matrix whose entries are the eigenvalues of $A$.</p><p>利用<strong>Lemma 2</strong>，我们可以将normalized graph Laplacian $L_n$分解为</p><script type="math/tex; mode=display">L_n=U\Lambda U^T</script><p>其中 $U$ 是由$L_n$的特征向量作为列向量构成的$n\times n$方阵，$U$构成了一个orhonormal space，即$U^TU=I$。$\Lambda$是对角元素为$U$对应的特征值的对角阵，$\Lambda$也被称为谱（spectrum）。</p><p><strong>Definition 12</strong> (Graph Fourier Transform). The <strong>graph Fourier transform</strong> is defined by</p><script type="math/tex; mode=display">\mathscr F(x)=U^Tx</script><p>where $x\in \mathbb R^{|V|}$ is a feature vector (graph signal) of the graph, which means the i-th value of $x$ is the value of the i-th node. </p><p><strong>Definition 12*</strong> (Inverse Graph Fourier Transform). The <strong>inverse graph Fourier transform</strong> is defined by</p><script type="math/tex; mode=display">\mathscr F^{-1}(\hat x)=U\hat x</script><p>where $\hat x$ is a the result of the graph Fourier transform. </p><blockquote><p>The graph Fourier transform projects the input graph signal to the orthonormal space where the basis is formed by eigenvectors of the nor-malized graph Laplacian. </p></blockquote><p><strong>Definition 13</strong> (Graph Convolution). The <strong>graph convolution</strong> of the input signal $x$ with a filter $g\in \mathbb R^{|V|}$ is defined as</p><script type="math/tex; mode=display">\begin{aligned} \mathbf{x} *_{G} \mathbf{g} &=\mathscr{F}^{-1}(\mathscr{F}(\mathbf{x}) \odot \mathscr{F}(\mathbf{g})) \\ &=\mathbf{U}\left(\mathbf{U}^{T} \mathbf{x} \odot \mathbf{U}^{T} \mathbf{g}\right) \end{aligned}</script><p>If we denote a filter as $g_\theta = \text{diag}(U^Tg)$, then the spectral graph convolution is simplified as</p><script type="math/tex; mode=display">\begin{align}\mathbf{x} *_{G} \mathbf{g} _{\theta}&= \mathbf{U}\left(\mathbf{U}^{T} \mathbf{x} \odot \mathbf{U}^{T} \mathbf{g}\right) \\&= \mathbf{U}\left(\mathbf{U}^{T} \mathbf{g} \odot \mathbf{U}^{T} \mathbf{x}\right) \\&= \mathbf{U}\left(\text{diag}(\mathbf{U}^{T} \mathbf{g}) (\mathbf{U}^{T} \mathbf{x}\right)) \\&= \mathbf{U} \mathbf{g}_{\theta} \mathbf{U}^{T} \mathbf{x}\end{align}</script><p>所有的Spectral-based ConvGNNs都遵循这个convolution的定义，主要的区别就在于选取不同的filter $g$. 这里我们可以看到，因为$U$是orthonormal basis，所以$U^T$是一个可逆矩阵，也是一个可逆映射。因此当我们参数化$g$的时候可以将这一步矩阵乘法省略，用一个简单的对角矩阵来代替即可。</p><p>如果我们将spectrum $\Lambda$作为$g_\theta$的参数，那么我们称满足以下条件的filter称为non-parametric filter，也就是这个filter的自由度就是自身的维度。</p><script type="math/tex; mode=display">g_{\theta}(\Lambda)=\operatorname{diag}(\theta)</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1901.00596&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;paper “A Comprehensive
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
      <category term="GNN" scheme="http://www.shihaizhou.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Information Theory</title>
    <link href="http://www.shihaizhou.com/2020/02/17/Information-Theory-Simple-Concepts/"/>
    <id>http://www.shihaizhou.com/2020/02/17/Information-Theory-Simple-Concepts/</id>
    <published>2020-02-17T07:59:39.000Z</published>
    <updated>2020-05-14T05:28:44.471Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><p><a href="https://arxiv.org/pdf/1708.07459.pdf" target="_blank" rel="noopener">arxiv文章 “Divergence, Entropy, Information”</a></p></li><li><p><a href="http://colah.github.io/posts/2015-09-Visual-Information/" target="_blank" rel="noopener">博客文章 “Visual Information Theory”</a></p></li><li><p><a href="http://www.cs.cmu.edu/~aarti/Class/10704/lecs.html" target="_blank" rel="noopener">CMU课程</a></p></li></ul><h1 id="Divergence"><a href="#Divergence" class="headerlink" title="Divergence"></a>Divergence</h1><p>散度divergence $d(p,q)$通常被用来表征”surprise”的程度：你相信一个分布为$q$，经过试验却发现它实际上是$p$的惊讶程度。为了引出KL-Divergence，我们首先定义概率单纯型(Probability Simplex).</p><p><strong>Definition</strong>  (Probability Simplex). For any finite alphabet $\mathcal{X}$ with $|\mathcal{X}|=m$, the probability simplex $\mathcal P^{\mathcal X}$ is the set</p><script type="math/tex; mode=display">\mathcal{P}^{\mathcal X} \triangleq\left\{q \in \mathbb{R}^{m} | \sum_{i} q_{i}=1, q_{i} \geq 0 \quad \forall i\right\}</script><p>注意：Probability Simplex是一个维度为$m-1$的空间，因为这个空间上的向量需要满足$\sum_{i} q_{i}=1$，从而导致其少了一个自由度。</p><p><strong>Definition</strong> (Kullback-Leibler Divergence). For $p, q \in \mathcal{P}^{\mathcal X}$, the <strong>Kullback-Leibler (KL) divergence</strong> of $q$ from $p$ is</p><script type="math/tex; mode=display">d(p, q) \triangleq \sum_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)}</script><p>where $\log$ represents $\ln$, and we are using the conventions that $\log \infty=\infty$, $\log 0=-\infty$, $0/0=0$, and $0\times \infty=0$.</p><p>注意：同entropy不同，KL divergence不仅仅局限于离散型变量，对于连续型变量也是适用的：</p><script type="math/tex; mode=display">D_{K L}(p(x) \| q(x))=\int_{-\infty}^{\infty} p(x) \log \frac{p(x)}{q(x)} d x</script><p>以上也是为什么作者会从KL divergence开始介绍Information Theory，因为KL divergence是一个更加广泛的定义。在实际计算两个分布的的KL divergence时，因为采样会不完全，所以需要用到以上数值的习惯性定义，或者使用function approximation来拟合观察到的分布；另一种做法是在计算的时候使用平滑策略：为两个变量missing $x$值赋予一个小的频率常数$\epsilon$，这部分多出来的频率由其他采样过的$x$支付，这样就保证了所有的频率都是非零的。</p><p><strong>Theorem</strong> (Gibbs’ Inequality). For all $p, q \in \mathcal{P}^{\mathcal{X}}$ , it holds that $d(p, q) \geq 0$, with equality iff $p = q$.</p><p>证明这个定理的思路：</p><ul><li>等价于对于任意的真实分布$p$，$\min d(p,q) = 0$，且等号成立条件为$q=p$.</li><li>使用拉格朗日乘数法计算梯度为0的点即可.</li></ul><p>注意：这条定理告诉我们为什么KL divergence和距离的定义非常相似，因为它满足了距离中的正则性。而同时我们需要注意的是它并不满足距离的对称性以及距离的三角不等式性质。</p><p>在机器学习中，我们经常需要做极大似然估计：</p><script type="math/tex; mode=display">\theta^{*}=\underset{\theta}{\operatorname{argmax}} \prod_{i=1}^{n} p_{X}^{\theta}\left(x_{i}\right)</script><p>其中，$p_{X}^{\theta}​$表示的是由参数$\theta​$决定的在随机变量$X​$上的概率分布。而其实我们可以证明，极大似然估计的等价就是极小KL divergence函数的优化：</p><script type="math/tex; mode=display">\theta^{*} = \arg\min_\theta d(\hat{p}_X,p^\theta_X)</script><p><strong>证明</strong>：Maximum Likelihood Estimation = KL Divergence Minimization. ($P_G$为估计的分布，$P_\text{data}$为真实分布)</p><script type="math/tex; mode=display">\begin{aligned} \theta^{*} &=\arg \max _{\theta} \prod_{i=1}^{m} P_{G}\left(x^{i} ; \theta\right) \\&=\arg \max _{\theta} \log \prod_{i=1}^{m} P_{G}\left(x^{i} ; \theta\right) \\ &=\arg \max _{\theta} \sum_{i=1}^{m} \log P_{G} \\ & \approx \arg \max _{\theta} E_{x \sim P_{\text {data }}}\left[\log P_{G}(x ; \theta)\right] \\&= \arg \max _{\theta} \int_{x} P_{\text {data }}(x) \log P_{G}(x ; \theta) d x-\int_{x} P_{\text {data }}(x) \log P_{\text {data }}(x) d x \\&= \arg \min _{\theta} \operatorname{KL}(P_{\text{data}}\|P_G)\end{aligned}</script><h1 id="Entropy"><a href="#Entropy" class="headerlink" title="Entropy"></a>Entropy</h1><p><strong>Definition</strong> (Shannon Entropy). The <strong>Shannon entropy</strong> of $p \in \mathcal{P}^{\mathcal X}$ is</p><script type="math/tex; mode=display">H(p) \triangleq-\sum_{x \in \mathcal{X}} p(x) \log p(x)</script><p>我们也使用 $H(X)$ 来表示随机变量$X$的熵。</p><p><strong>Theorem</strong> The Shannon entropy is related to the divergence according to the formula</p><script type="math/tex; mode=display">H(p) = \log |X| - d(p,u)</script><p>其中$u$表示在集合$X$上的均匀分布。通过这条定理我们可以知道一个分布的熵和你相信这个分布是均匀分布时得到的surprise是负相关的，同时因为Gibbs’ Inequality，我们知道当一个分布是均匀分布的时候熵取到最大。另外，这条定理也告诉我们熵是不能够在连续型随机变量上定义的，因为在$\mathbb R$上是没有“均匀分布的”.</p><p>而对于更加常规的信息论学习，通常是先介绍Entropy的定义，然后转而介绍KL divergence等。Entropy翻译作信息熵，也就是一个离散型随机变量的不确定性。不确定性的定义可以被表示成一个编码问题：一个alphabet中的每个事件已知其发生的概率分布，该如何对每个事件进行编码从而能够复出最小的代价？以下为结论：如果事件$x$发生的概率为$p(x)$，最优编码位数为$-\log_2{p(x)}$，则该事件的总代价为$-p(x)\log_2{p(x)}$。从而有了以上的Entropy定义。均匀分布的时候拥有最大的不确定性，也就是对其编码的代价最大。</p><h2 id="Conditional-Entropy"><a href="#Conditional-Entropy" class="headerlink" title="Conditional Entropy"></a>Conditional Entropy</h2><p>概率论中最美妙的就是引入了Conditional Probability $P(A|B)$，也就是$B$事件发生的前提下$A$事件发生的概率。同样的在信息论中我们也会关注到多变量分布的情况。为了表述清晰，以下是我们会用到的符号及其意义：</p><ul><li>$p_{X} \in \mathcal{P}^{\mathcal X}$: 一个在alphabet $\mathcal X$上的随机变量$X$的分布$p$</li><li>$p_{Y} \in \mathcal{P}^{\mathcal Y}$: 一个在alphabet $\mathcal Y$上的随机变量$Y$的分布$p$</li><li>$p_{X,Y} \in \mathcal{P}^{\mathcal X \times \mathcal Y}$: 一个在他们上面的联合分布$p$</li><li>$p_{X} \otimes p_{Y} \in \mathcal{P}^{\mathcal{X} \times \mathcal Y}$: 定义为边缘分布的乘积，也即$\left(p_{X} \otimes p_{Y}\right)(x, y)= p_{X}(x) p_{Y}(y)​$.</li></ul><p><strong>Definition</strong> (Conditional Entropy). The <strong>conditional entropy</strong> of $X$ given $Y$ is</p><script type="math/tex; mode=display">H(X | Y) \triangleq \sum_{x, y \in \mathcal{X} \times \mathcal{Y}} p_{X, Y}(x, y) \log p_{X | Y}(x | y)</script><p>注意：有人可能会觉得为了维持对称性，需要将条件熵定义为：</p><script type="math/tex; mode=display">\tilde{H}(X | Y)=\sum_{x, y \in \mathcal{X} \times \mathcal{Y}} p_{X | Y}(x | y) \log p_{X | Y}(x | y)</script><blockquote><p>However, a quick think makes clear that this definition isn’t appropriate, because it doesn’t include any information about the distribution of $Y$. If $Y$ is concentrated around some very informative (or uninformative) values, then $\tilde H$ won’t notice that some values are more valuable than others. </p></blockquote><p><strong>Theorem</strong> The conditional entropy is related to the unconditional entropy as</p><script type="math/tex; mode=display">H(X|Y)= H(X,Y)-H(Y)</script><p>where $H(X,Y)$ is the entropy of the joint distribution $p_{X,Y}$. </p><p>这一条定理非常容易记住，只需要和条件概率公式进行比对：</p><script type="math/tex; mode=display">p_{X|Y}(x|y)={p_{X,Y}(x,y)\over p_Y(y)}</script><p>或者用更加通俗易懂的语言来解释它：</p><blockquote><p>The uncertainty you have about $X$ given that you’ve been told $Y$ is equal to the uncertainty you had about both $X$ and $Y$ , less the uncertainty that was resolved when you learned $Y$.</p></blockquote><p><strong>Theorem</strong> (Side Information reduces uncertainty). </p><script type="math/tex; mode=display">H(X|Y) \leq H(X)</script><p>该定理：额外的信息不会增加一个随机变量的不确定性。提供额外的信息$Y$造成的增益$H(X)-H(X|Y) \geq 0$，增益为0时就是$Y$和$X$互不相关(Y carries no information about X)。</p><h1 id="Information"><a href="#Information" class="headerlink" title="Information"></a>Information</h1><p><strong>Definition</strong> (Mutual Information). The <strong>mutual information</strong> $I(X,Y)$ in $Y$ about $X$ is </p><script type="math/tex; mode=display">I(X,Y) \triangleq H(X) - H(X|Y).</script><p><strong>Theorem</strong> The mutual information may also be written as</p><script type="math/tex; mode=display">\begin{align}I(X,Y) &= d(p_{X,Y}, p_{X} \otimes p_{Y}) \\&= \mathbb E _Y[d(p_{X|Y}, p_X)]\end{align}</script><p><strong>Mutual Information在很多情况下也叫做Information Gain（信息增益），是用来衡量两个随机变量之间的dependence的：Mutual Information越大，则两个随机变量间的关联度越高。</strong>而根据第一条等式推知：the larger the divergence between the joint and the product of the marginals, the stronger the dependence between the two variables. 而同时我们互换第一条等式$XY$变量可以得到以下：</p><p><strong>Corrolary</strong> The mutual information is symmetric:</p><script type="math/tex; mode=display">I(X,Y) = I(Y,X)</script><p>而第二条等式的含义则是如果你选择无视$Y$给出的side information并还是认为分布是按照$p_X$的，那么mutual information或者就等于你选择忽视和接受Y之间的surprise。</p><p><strong>Theorem</strong> (Data Processing Inequality). Let $X$ and $Y$ be random variables, and let $Z = g(Y)$, where $g$ is any function $g : Y \rightarrow Z$. Then,</p><script type="math/tex; mode=display">I(X,Z) \leq I(X,Y)</script><p>这条定理告诉我们，对于一个作为side information的随机变量，对这个变量（或者该变量的估计）进行任意的函数映射操作，所得到的结果作为新的side information，其提供的mutual information一定不会超过原来的随机变量。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1708.07459.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;arxiv文章 “Diverge
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
  </entry>
  
  <entry>
    <title>Metric Space</title>
    <link href="http://www.shihaizhou.com/2020/02/11/Some-Maths/"/>
    <id>http://www.shihaizhou.com/2020/02/11/Some-Maths/</id>
    <published>2020-02-11T07:58:43.000Z</published>
    <updated>2020-02-13T11:52:20.483Z</updated>
    
    <content type="html"><![CDATA[<h1 id="度量空间"><a href="#度量空间" class="headerlink" title="度量空间"></a>度量空间</h1><p>度量空间也叫做距离空间（metric space）。这是一种拓扑空间，其上的拓扑由指定的距离决定。在一个空间中引入距离是为了刻画“收敛”这个概念，同时距离也能够定义在向量空间中的（准）范数。</p><h3 id="距离空间"><a href="#距离空间" class="headerlink" title="距离空间"></a>距离空间</h3><p>距离空间$\mathscr{X}$是指非空集$\mathscr{X}$上定义了一个双变量的实值函数$\rho(x,y)$，该函数满足以下三个条件：</p><ul><li>$\rho(x,y) \geq 0$，等号成立条件当且仅当$x=y$ （非负）</li><li>$\rho(x,y)=\rho(y,x)$ （可交换）</li><li>$\rho(x,y)\leq \rho(x,z) + \rho(z,y)$ （三角不等式）</li></ul><p>以$\rho$为距离的度量空间记作$(\mathscr{X}, \rho)$。“距离”的概念是对欧式空间距离的抽象，不难发现欧式空间两点间的距离满足以上的条件。</p><h3 id="空间-C-a-b"><a href="#空间-C-a-b" class="headerlink" title="空间$C[a,b]$"></a>空间$C[a,b]$</h3><p>空间$C[a,b]$定义为在区间$[a.b]$上的连续函数全体（即这个空间内的点就是函数）。在其上定义距离</p><script type="math/tex; mode=display">\rho(x,y) \triangleq \max_{a<t<b}|x(t)-y(t)|</script><p>距离空间$(C[a,b], \rho)$在以后简记为$C[a,b]$.</p><h3 id="收敛"><a href="#收敛" class="headerlink" title="收敛"></a>收敛</h3><p>距离空间$(\mathscr{X}, \rho)$上的点列$\{x_n\}$叫做收敛到$x_0$指的是：</p><script type="math/tex; mode=display">\rho\left(x_{n}, x_{0}\right) \rightarrow 0(n \rightarrow \infty)</script><p>这时也记作$\lim _{n \rightarrow \infty} x_{n}=x_{0}$，或者简单记作$x_n \rightarrow x_0$.</p><p>而在函数空间$C[a,b]$中的点列$\{x_n\}$收敛到$x_0$指的是$\{x_n(t)\}$一致收敛到$x_0(t)$.</p><h3 id="完备"><a href="#完备" class="headerlink" title="完备"></a>完备</h3><p>基本列（柯西列）：点列$\{x_n\}$是基本列，如果 $\rho\left(x_{n}, x_{m}\right) \rightarrow 0(n, m \rightarrow \infty)$。</p><p>如果空间中的所有基本列都是收敛列，那么这个空间就是完备的。</p><h3 id="连续映射"><a href="#连续映射" class="headerlink" title="连续映射"></a>连续映射</h3><p>设$T:(\mathscr{X}, \rho) \rightarrow(\mathscr{Y}, r)$ 是一个映射。如果对于$\mathscr{X}$中的任意点列$\{x_n\}$和点$x_0$，都有：</p><script type="math/tex; mode=display">\rho\left(x_{n}, x\right) \rightarrow 0 \Rightarrow r\left(T x_{n}, T x_{0}\right) \rightarrow 0 \quad(n \rightarrow \infty)</script><p>则称映射$T$是连续的。</p><h3 id="压缩映射"><a href="#压缩映射" class="headerlink" title="压缩映射"></a>压缩映射</h3><p>设$T:(\mathscr{X}, \rho) \rightarrow(\mathscr{X}, \rho)$ 是一个到自身的映射。如果存在$0&lt;\alpha&lt;1$，使得对于任意的$x,y \in \mathscr{X}$，都有：</p><script type="math/tex; mode=display">\rho(T x, T y) \leqslant \alpha \rho(x, y)</script><p>则称映射$T$为一个压缩映射。</p><h3 id="Banach不动点定理（压缩映射原理）"><a href="#Banach不动点定理（压缩映射原理）" class="headerlink" title="Banach不动点定理（压缩映射原理）"></a>Banach不动点定理（压缩映射原理）</h3><p>假设$(\mathscr{X}, \rho)$是一个完备的距离空间，$T$是这个距离空间上到自身的一个压缩映射，则在该空间上存在唯一的不动点$x^<em>$，使得$Tx^</em>=x^*$.</p><h1 id="线性赋范空间"><a href="#线性赋范空间" class="headerlink" title="线性赋范空间"></a>线性赋范空间</h1><p>距离空间中只有拓扑结构，但是在分析函数空间的时候还需要考虑元素之间的代数运算。</p><h2 id="线性空间"><a href="#线性空间" class="headerlink" title="线性空间"></a>线性空间</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>设$\mathscr{X}$为一个非空集，$\mathbb{K}$为实（复）数域，如果下列条件满足，则称$\mathscr{X}$为实（复）线性空间。</p><ul><li>$\mathscr{X}$是一个加法交换群，也即：<ul><li>$x+y=y+x$ （加法交换律）</li><li>$(x+y)+z = x+(y+z)$ （加法结合律）</li><li>存在唯一的$\theta \in \mathscr{X}$，使得$\forall x \in \mathscr{X}, x+\theta=\theta+x$ （唯一零元）</li><li>对$\forall x \in \mathscr{X}$, 存在唯一的$ x^{\prime} \in \mathscr{X}$，使得$x+x^\prime=\theta$，记为$-x$（唯一负元）</li></ul></li><li>定义了数域$\mathbb{K}$中的数$a$和$x\in \mathscr{X}$的数乘运算，满足以下条件：<ul><li>$a(\beta x)=(a \beta) x $ （数乘的结合律）</li><li>$1 \cdot x=x$ （乘法零元）</li><li>$(a+\beta) x=a x+\beta x$ （分配率）</li><li>$a(x+y)=a x+a y$ （分配率）</li></ul></li></ul><h3 id="线性空间上的距离"><a href="#线性空间上的距离" class="headerlink" title="线性空间上的距离"></a>线性空间上的距离</h3><p>线性空间上的距离定义一共需要满足两个关键条件：</p><p><strong>一、平移的不变性：</strong></p><script type="math/tex; mode=display">\rho(x+z, y+z)=\rho(x, y) \quad(\forall x, y, z \in \mathscr{X})</script><p>而通过平移的不变性能够推出距离函数是对加法连续的，即：</p><script type="math/tex; mode=display">\left.\begin{array}{l}{\rho\left(x_{n}, x\right) \rightarrow 0} \\ {\rho\left(y_{n}, y\right) \rightarrow 0}\end{array}\right\} \Rightarrow \rho\left(x_{n}+y_{n}, x+y\right) \rightarrow 0</script><p>证明步骤只需要根据平移不变形的定义就可，在此省略。事实上，这两者是相互等价的。</p><p><strong>二、数乘的连续性：</strong></p><p>数乘连续性定义为以下：</p><ul><li>$\rho\left(x_{\mathfrak{n}}, x\right) \rightarrow 0 \Rightarrow \rho\left(a x_{\mathfrak{n}}, a x\right) \rightarrow 0 \quad(\forall a \in \mathbb{K})$</li><li>$a_{n} \rightarrow a(\mathbb{K}) \Rightarrow \rho\left(a_{n} x, a x\right) \rightarrow 0 \quad(\forall x \in \mathscr{X})$</li></ul><h3 id="线性空间上的准范数（模）"><a href="#线性空间上的准范数（模）" class="headerlink" title="线性空间上的准范数（模）"></a>线性空间上的准范数（模）</h3><p>在距离的基础上定义一个单值函数$p$：</p><script type="math/tex; mode=display">p: \mathscr{X} \rightarrow \mathbb{R}^{1}, \quad p(x) \triangleq \rho(x, \theta) \quad(\forall x \in \mathscr{X})</script><p>通过距离函数本身的3个性质，我们能够得出这个单值函数$p$的性质（略过）。而事实上，这个单值函数是我们刻画一个点大小的量，也就是准范数。</p><p>线性空间$\mathscr{X}$上的准范数定义为该空间上的的一个函数$|\cdot|: \mathscr{X} \rightarrow \mathbb{R}^{1}$，满足以下条件：</p><ul><li>$|x| \geqslant 0 \quad(\forall x \in \mathscr{X});|x|=0 \Leftrightarrow x=\theta$ （正定性）</li><li>$| x+y|\leqslant| x|+| y | \quad(\forall x, y \in \mathscr{X})$ （三角不等式）</li><li>$|-x|=|x| \quad(\forall x \in \mathscr{X})$ （逆元范数相等）</li><li>$\lim _{a_n \rightarrow 0}\left|a_{n} x\right|=0, \lim _{|x_n| \rightarrow 0}\left|a x_{n}\right|=0(\forall x \in \mathscr{X}, \quad \forall a \in \mathbb{R})$ （对数乘连续）</li></ul><p>如果在赋准范数空间$\mathscr{X}$上用准范数来定义收敛，即：</p><script type="math/tex; mode=display">\|x_n-x\|\rightarrow 0 \triangleq x_n \rightarrow x</script><p>那么这个空间就称作<strong>${F^*}$空间</strong>；如果这个空间同时又是完备的，那么这个空间就称作<strong>$F$空间</strong>。</p><h3 id="Banach空间与范数"><a href="#Banach空间与范数" class="headerlink" title="Banach空间与范数"></a>Banach空间与范数</h3><p>在准范数的基础上增加齐次性条件，准范数便成为了范数。其中齐次性条件指的是：</p><script type="math/tex; mode=display">\|a x\|=|a|\|x\| \quad(\forall a \in \mathbb{K}, \quad \forall x \in \mathscr{X})</script><p>当赋准范数的线性空间中的准范数是范数时，这个线性空间称为$B^<em>$空间；完备的$B^</em>$空间称作$B$空间，也就是Banach空间。我们也叫该空间为线性赋范空间。</p><h3 id="线性赋范空间上的模等价"><a href="#线性赋范空间上的模等价" class="headerlink" title="线性赋范空间上的模等价"></a>线性赋范空间上的模等价</h3><p>和开头所介绍的一样，引入距离和范数是为了研究一种收敛性。因此我们可以认为导致同一种收敛性的不同范数是等价的。更加准确地说：</p><p>假设在线性空间$\mathscr{X}$上定义了两个范数$|\cdot|_1$以及$|\cdot|_2$，如果在$n\rightarrow \infty$时，$\left|x_{n}\right|_{2} \rightarrow 0 \Rightarrow\left|x_{n}\right|_{1} \rightarrow 0$，则我们认为$|\cdot|_1$比$|\cdot|_2$强。而如果两个范数互相都比对方强，则我们认为两个范数等价。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;度量空间&quot;&gt;&lt;a href=&quot;#度量空间&quot; class=&quot;headerlink&quot; title=&quot;度量空间&quot;&gt;&lt;/a&gt;度量空间&lt;/h1&gt;&lt;p&gt;度量空间也叫做距离空间（metric space）。这是一种拓扑空间，其上的拓扑由指定的距离决定。在一个空间中引入距离是为了
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
  </entry>
  
</feed>
