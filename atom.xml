<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>In Love with CodeCode</title>
  
  <subtitle>Haizhou&#39;s Blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.shihaizhou.com/"/>
  <updated>2020-02-28T05:55:11.223Z</updated>
  <id>http://www.shihaizhou.com/</id>
  
  <author>
    <name>Haizhou Shi</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Graph Attention Networks</title>
    <link href="http://www.shihaizhou.com/2020/02/28/Graph-Attention-Networks/"/>
    <id>http://www.shihaizhou.com/2020/02/28/Graph-Attention-Networks/</id>
    <published>2020-02-28T04:57:44.000Z</published>
    <updated>2020-02-28T05:55:11.223Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/abs/1710.10903" target="_blank" rel="noopener">paper “Graph Attention Networks”</a></li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>之前的GCN方法将中心节点的信息和K-th neighborhood进行传播，但是认为每个邻接节点的权重是相同的。这篇工作enables (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to <strong>inductive as well as transductive</strong> problems. </p><p>其中的inductive learning problem 包括了 tasks where the model has to generalize to completely unseen graphs. 广义的inductive learning和deductive learning相对：前者指的是给出例子进行归纳从而找到规律；后者指的是给出规则，让学习者运用规则到特定的例子上。</p><h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><p>在GAT中，这章描述的Graph Attention Layer会从始至终被使用。首先文章采用了一个在节点空间中共享的映射矩阵$W$，两个节点之间的相关性$e_{ij}$通过这个矩阵加上一个函数$a$进行度量，其中$a$是一个参数化的函数：</p><script type="math/tex; mode=display">e_{i j}=a\left(\mathbf{W} \vec{h}_{i}, \mathbf{W} \vec{h}_{j}\right)</script><p>需要注意的是，我们可以将这个相关性计算apply到图中的任意两个节点上而不去考虑图本身的结构。但是一般来说Attention机制被应用在1st-order neighborhood（需要注意的是<strong>中心节点在本文也被定义在1st-order  neighborhood内</strong>）上，经过归一化我们有：</p><script type="math/tex; mode=display">\alpha_{i j}=\operatorname{softmax}_{j}\left(e_{i j}\right)=\frac{\exp \left(e_{i j}\right)}{\sum_{k \in \mathcal{N}_{i}} \exp \left(e_{i k}\right)}</script><p>最后输出层的节点表示$\vec{h}_{i}^{\prime}$为：</p><script type="math/tex; mode=display">\vec{h}_{i}^{\prime}=\sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j} \mathbf{W} \vec{h}_{j}\right)</script><p>为了让self-attention在训练的时候更加稳定，可以使用Multi-head的技术。最后的表示就是多个头结果的concatenation：</p><script type="math/tex; mode=display">\vec{h}_{i}^{\prime}=\|_{k=1}^{K} \sigma\left(\sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \vec{h}_{j}\right)</script><p>特别地，在整个网络的最后一层预测层再进行concatenation是不make sense的。通过average和延后的non-linear层我们有：</p><script type="math/tex; mode=display">\vec{h}_{i}^{\prime}=\sigma\left(\frac{1}{K} \sum_{k=1}^{K} \sum_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \vec{h}_{j}\right)</script><p><img src="https://i.loli.net/2020/02/28/dWKEa3iwVjNFpJh.png" alt="image.png"></p><p><strong>Discussion and Future Work</strong></p><p>The time complexity of a <strong>single GAT attention head</strong> computing $F^\prime$ features may be expressed as $O(|V|FF^\prime + |E|F^\prime)​$, where F is the number of input features. 因为这仅仅是一个head的计算复杂度，可以看到还是相对较高的。</p><p>未来可以关注的方向有：</p><ul><li>overcoming the practical problems to be able to handle larger batch sizes.</li><li>take advantage of the attention mechanism to perform a thorough analysis on the model interpretability.</li><li>extend the model to incorporate edge features (possibly indicating relationship among nodes) . </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1710.10903&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;paper “Graph Attention 
      
    
    </summary>
    
    
      <category term="GNN" scheme="http://www.shihaizhou.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Graph Convolutional Neural Networks</title>
    <link href="http://www.shihaizhou.com/2020/02/25/Graph-Convolutional-Neural-Networks/"/>
    <id>http://www.shihaizhou.com/2020/02/25/Graph-Convolutional-Neural-Networks/</id>
    <published>2020-02-25T02:14:12.000Z</published>
    <updated>2020-02-27T11:55:55.308Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/abs/1609.02907" target="_blank" rel="noopener">paper “Semi-Supervised Classification with Graph Convolutional Networks”</a></li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>本文提出了广泛流行的GCN，通过对ChebNet进行一阶近似，从而达到了以下效果：首先是一个scalable的方法框架；第二是计算复杂度和图的边集大小呈线性关系。</p><p>在本文之前的工作处理半监督节点分类问题时，大多在 loss function 上加上图的正则项$\mathcal L_{reg}$： </p><script type="math/tex; mode=display">\mathcal{L}_{\mathrm{reg}}=\sum_{i, j} A_{i j}\left\|f\left(X_{i}\right)-f\left(X_{j}\right)\right\|^{2}=f(X)^{\top} \Delta f(X)</script><p>其中$\Delta = D-A$代表的是未经过归一化的graph laplacian. The formulation of this euation relies on the assumption that connected nodes in the graph are likely to share the same label. This assumption, however, might restrict modeling capacity, as graph edges need not necessarily encode node similarity, but could containa dditional information.</p><h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><p>基于以上的动机本文提出了一个神经网络架构$f(X,A)$从而避免正则项的使用。</p><script type="math/tex; mode=display">H^{(l+1)}=\sigma\left(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)}\right)</script><p>其中 $\tilde A$ 表示加上self-loop的邻接矩阵，$\tilde D$ 为其对应的度矩阵；$\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$ 则为经过归一化的邻接矩阵。$H^{(l)}\in \mathbb R^{N\times D}$ 为第$l$ 层的的图节点表示，对应的$W^{(l)} \in \mathbb R ^{D\times D}$为第$l$层的参数矩阵，从这里我们可以看出参数矩阵和图的结构是无关的（因为和节点维数无关），从而给了GCN scalability。以上的等式优美的地方在于它是可以从Spectral-based方法推导得到的。</p><h2 id="Derivation-from-ChebNet"><a href="#Derivation-from-ChebNet" class="headerlink" title="Derivation from ChebNet"></a>Derivation from ChebNet</h2><p>ChebNet是利用矩阵上的切比雪夫多项式进行K阶近似的方法。简单来说是将图傅里叶变换中的$g_\theta$进行近似。</p><script type="math/tex; mode=display">g_{\theta^{\prime}}(\Lambda) \approx \sum_{k=0}^{K} \theta_{k}^{\prime} T_{k}(\tilde{\Lambda})</script><p>Note that this expression is now K-localized since it is a Kth-order polynomial in the Laplacian, i.e. it depends only on nodes that are at maximum K steps away from the central node (Kth-order neighborhood). </p><p>K阶近似意味着中心节点的表示仅会被周围距离为K的节点影响。本文的一个重要想法是：我们不需要在单个filter上去定义K阶近似，相反我们限制一个filter就是一阶近似，<strong>将这个一阶近似层叠K层也能够达到Kth-order neighborhood的效果</strong>。同时估计ChebNet中$\lambda_{\max}\approx 2$。一阶（线性）近似的两个自由变量分别取为相反数$\theta = \theta^ \prime_0 = -\theta^ \prime_1$。这样我们就得到最开始定义的GCN上单通道的卷积操作：</p><script type="math/tex; mode=display">\begin{align}g_{\theta^{\prime}} \star x &\approx \theta_{0}^{\prime} x+\theta_{1}^{\prime}\left(L-I_{N}\right) x \\&=\theta_{0}^{\prime} x-\theta_{1}^{\prime} D^{-\frac{1}{2}} A D^{-\frac{1}{2}} x \\&\approx \theta\left(I_{N}+D^{-\frac{1}{2}} A D^{-\frac{1}{2}}\right) x\end{align}</script><p>注意到中间的变换矩阵所有的特征值都在$[0,2]$的区间内，这会导致数值计算上的不稳定。运用renormalization trick $I_{N}+D^{-\frac{1}{2}} A D^{-\frac{1}{2}} \rightarrow \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$ 即可。</p><p>将以上的单通道卷积操作扩展到$X\in \mathbb R^{N\times C}$，即有现在的公式。该公式的计算复杂度为$O(|E|FC)$，因为矩阵$A$是一个sparse matrix。</p><script type="math/tex; mode=display">Z=\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} X \Theta</script><h2 id="Semi-Supervised-Node-Classification"><a href="#Semi-Supervised-Node-Classification" class="headerlink" title="Semi-Supervised Node Classification"></a>Semi-Supervised Node Classification</h2><p>用一个简单的两层模型来解释这个问题中GCN的forward model：</p><script type="math/tex; mode=display">Z=f(X, A)=\operatorname{softmax}\left(\hat{A} \operatorname{ReLU}\left(\hat{A} X W^{(0)}\right) W^{(1)}\right)</script><p>用以下XE loss function下进行训练：</p><script type="math/tex; mode=display">\mathcal{L}=-\sum_{l \in \mathcal{Y}_{L}} \sum_{f=1}^{F} Y_{l f} \ln Z_{l f}</script><p>示意图为以下：</p><p><img src="https://i.loli.net/2020/02/27/1mzRJDTO9cGBdZX.png" alt="image.png"></p><p><strong>Limitations and Future Work</strong></p><ul><li>For very large and densely connected graph datasets, further approximations might be necessary.</li><li>Our framework currently does not naturally support edge features and is limited to undirected graphs (weighted oo unweighted).</li><li>we implicitly assume locality (dependence on the Kth-order neighborhood for a GCN with K layers) and equal importance of self-connections vs. edges to neighboring nodes. It might be beneficial to introduce a trade-off parameter $\lambda $ in the definition of $\tilde A$: $\tilde A  = A + \lambda I_N$. </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1609.02907&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;paper “Semi-Supervised 
      
    
    </summary>
    
    
      <category term="GNN" scheme="http://www.shihaizhou.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Transformer Basics</title>
    <link href="http://www.shihaizhou.com/2020/02/24/Transformer-Basics/"/>
    <id>http://www.shihaizhou.com/2020/02/24/Transformer-Basics/</id>
    <published>2020-02-23T17:22:40.000Z</published>
    <updated>2020-02-23T17:22:40.102Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">paper: “Attention Is All You Need”</a></li><li><a href="https://zhuanlan.zhihu.com/p/44121378" target="_blank" rel="noopener">知乎文章 “Transformer详解”</a></li></ul><h1 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h1><p>Sequence Modeling问题简述为以下：</p><blockquote><p>The encoder maps an input sequence of symbol representations $x = (x_1,\cdots,x_n)$ to a sequence of continuous representations $z = (z_1,\cdots,z_n)$. Given $z$, the decoder then generates an output sequence $y=(y_1, …, y_m)$ of symbols one element at a time. </p></blockquote><p>encoder-decoder framework是auto-regressive的，也就是说当前生成的词是conditioned on the previously generated word (previously generated symbols as additional input when generating the next). 注意：虽然我们传统意义上使用encoder-decoder时，encoder往往将variable-length的input映射成fixed-length的向量（RNN中为最后一个hidden state），但是encoder在编码时已经生成了很多的中间结果构成$z = (z_1,\cdots,z_n)$ .</p><p>Transformer的模型如下图所示。</p><p><img src="https://i.loli.net/2020/02/23/ZNT194ipKYeAzv6.png" alt="image.png"></p><h2 id="Encoder-and-Decoder-Stacks"><a href="#Encoder-and-Decoder-Stacks" class="headerlink" title="Encoder and Decoder Stacks"></a>Encoder and Decoder Stacks</h2><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><ul><li><p>由6层相同的layer组成，每层layer由3层sublayer组成。</p></li><li><p>第一层sublayer是multi-head self-attention；第二层是position-wise fully connected feed-forward network。</p></li><li><p>在两层之间加入了residual network的设计：</p><script type="math/tex; mode=display">\text{layer_norm}(x+\text{sublayer}(x))</script></li><li><p>为了使residual成为可能，model的所有向量长度都是对齐的$d_\text{model} = 512$.</p></li><li><p>Encoder是可以完全并行化处理的，因为所有的input vector是全局可见的。</p></li></ul><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><ul><li>由6层相同的layer组成，每层layer由3层sublayer组成。</li><li>在Encoder结构的基础上中间插入了一层multi-head self-attention.</li></ul><blockquote><p> We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.</p></blockquote><p>这一段话是理解Decoder过程的关键。首先要明确的是Transformer是auto-regressive的，所以这样一来结合图片我们知道，Transformer在decode阶段第$i$位的input是第$i-1$位decode的output，以及中间的Multi-Head Attention阶段要用到的第$i-1$位encode的结果。</p><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>Attention机制可以被看作是given a set of key-value pairs，将query映射到一个output上。output通常是weighted sum of the values, based on the similarity of the query-key pairs.</p><p><img src="https://i.loli.net/2020/02/24/8icua43VCtygQHO.png" alt="image.png"></p><h3 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h3><p><strong>Scaled Dot-Product Attention</strong> 是本文使用的attention机制。In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$. The keys and values are also packed together into matrices $K$ and $V$. We compute the matrix of outputs as: </p><script type="math/tex; mode=display">\text { attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V</script><h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p>通过实验发现如果将$Q,K,V$这些向量分别线性映射（映射也是参数化的）到不同的空间，然后并行地计算Attention，最后concat并映射回原来的空间，将会非常有提升（见图）。这样的技术叫做<strong>Multi-Head Attention</strong>.</p><p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</p><script type="math/tex; mode=display">\begin{aligned} \text { multi_head }(Q, K, V) &=\text { concat }\left(\text { head }_{1}, \ldots, \text { head }_{\mathrm{h}}\right) W^{O} \\ \text { where head }_{\mathrm{i}} &=\text { attention }\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \end{aligned}</script><p>在本文中采用了 $h=8$ parallel attention layers，在不同head的空间中数据维数显著减小，提升了运算表现。</p><h2 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h2><p><strong>Feed-Forward Networks</strong> 定义为线性映射+RELU+线性映射：</p><script type="math/tex; mode=display">\mathrm{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}</script><h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h2><p>如何加入当前位置的位置信息，作者给出了一个sine+cosine的函数：</p><script type="math/tex; mode=display">\begin{array}{c}{P E_{(p o s, 2 i)}=\sin \left(\text {pos} / 10000^{2 i / d_{\text {model }}}\right)} \\ {P E_{(\text {pos, }, 2 i+1)}=\cos \left(\text {pos} / 10000^{2 i / d_{\text {model }}}\right)}\end{array}</script><p>其中$pos$为当前位置而$i$为当前embedding的dimension。这样的函数<strong>能够帮助编码模型的相对位置信息</strong>，因为对于一个固定的relative offset $k$，$PE_{pos+k}$能够被表示成$PE_{pos}$的线性函数（$\sin(\lambda k)$以及$\cos(\lambda k)$为常数，根据三角展开式得到）。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;paper: “Attention I
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
  </entry>
  
  <entry>
    <title>Grammar of Java</title>
    <link href="http://www.shihaizhou.com/2020/02/23/Algorithms-and-Grammar-of-java/"/>
    <id>http://www.shihaizhou.com/2020/02/23/Algorithms-and-Grammar-of-java/</id>
    <published>2020-02-23T12:37:01.000Z</published>
    <updated>2020-02-23T12:37:54.138Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Grammar"><a href="#Grammar" class="headerlink" title="Grammar"></a>Grammar</h1><h3 id="BufferedReader-比较快"><a href="#BufferedReader-比较快" class="headerlink" title="BufferedReader (比较快)"></a>BufferedReader (比较快)</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.*;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> BufferedReader reader = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(System.in));</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">String[] line = reader.nextLine().split();</span><br><span class="line">        <span class="keyword">int</span> num = Integer.parseInt(line[<span class="number">0</span>]);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Scanner-输入字符串"><a href="#Scanner-输入字符串" class="headerlink" title="Scanner 输入字符串"></a>Scanner 输入字符串</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Scanner scanner = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"><span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">        System.out.print(<span class="string">"Fuck you."</span>);</span><br><span class="line">        String input = scanner.nextLine();</span><br><span class="line">        System.out.println(<span class="string">"input = "</span> + input);</span><br><span class="line">        System.out.printf(<span class="string">"%d"</span>, <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Input-Integer"><a href="#Input-Integer" class="headerlink" title="Input Integer"></a>Input Integer</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">String input = scanner.nextLine();</span><br><span class="line"><span class="keyword">int</span> num = Integer.parseInt(input);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> num = Scanner.nextInt(); <span class="comment">// very slow</span></span><br><span class="line"><span class="keyword">int</span> num = Integer.parseInt(Scanner.next())</span><br></pre></td></tr></table></figure><h3 id="Convert-Integer-to-String"><a href="#Convert-Integer-to-String" class="headerlink" title="Convert Integer to String"></a>Convert Integer to String</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">str = Integer.toString(num);</span><br><span class="line"></span><br><span class="line">str = Integer.toBinaryString(num);</span><br><span class="line">str = Integer.toHexString(num);</span><br><span class="line">str = Integer.toOctalString(num);</span><br><span class="line"></span><br><span class="line">str = Integer.toString(num, radix=<span class="number">10</span>);</span><br></pre></td></tr></table></figure><h3 id="Convert-Double-to-String"><a href="#Convert-Double-to-String" class="headerlink" title="Convert Double to String"></a>Convert Double to String</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">str = Double.toString(num);</span><br><span class="line"></span><br><span class="line">str = Double.toBinaryString(num);</span><br><span class="line">str = Double.toHexString(num);</span><br><span class="line">str = Double.toOctalString(num);</span><br><span class="line"></span><br><span class="line">str = Double.toString(num, radix=<span class="number">10</span>);</span><br></pre></td></tr></table></figure><h3 id="String-Methods"><a href="#String-Methods" class="headerlink" title="String Methods"></a>String Methods</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">char</span>[] charArray = &#123;<span class="string">'0'</span>, <span class="string">'f'</span>, <span class="string">'u'</span>&#125;;</span><br><span class="line">String str = <span class="keyword">new</span> String(charArray);</span><br><span class="line"></span><br><span class="line"><span class="comment">// length of the string</span></span><br><span class="line">length = str.length();</span><br><span class="line"></span><br><span class="line"><span class="comment">// index</span></span><br><span class="line"><span class="keyword">char</span> a = str.charAt(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// substring</span></span><br><span class="line">str.substring(<span class="number">0</span>, str.length()-<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// reverse a string</span></span><br><span class="line">str = <span class="keyword">new</span> StringBuilder(str).reverse().toString();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 格式化输出 double, int 等</span></span><br><span class="line">String.format(<span class="string">"%d"</span>, num);</span><br><span class="line">String.format(<span class="string">"%.1f"</span>, num);</span><br><span class="line"></span><br><span class="line"><span class="comment">// string的分开，和python差不多</span></span><br><span class="line">String[] splited = str.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// string的头尾去掉空格</span></span><br><span class="line">String trimed = str.trim();</span><br><span class="line"></span><br><span class="line"><span class="comment">// replace: 非正则表达式，返回一个新的字符串，只能够替换char</span></span><br><span class="line">str.replace(<span class="keyword">char</span> oldChar, <span class="keyword">char</span> newChar);</span><br><span class="line"></span><br><span class="line"><span class="comment">// replaceAll</span></span><br><span class="line">str.repalceAll(<span class="string">"^\+"</span>, <span class="string">""</span>); <span class="comment">// replace all the starting "</span></span><br><span class="line"></span><br><span class="line">str = <span class="string">"fuck"</span>;</span><br><span class="line"><span class="comment">// compareTo, compareToIgnoreCase</span></span><br><span class="line">str.compareTo(<span class="string">"fuck"</span>);</span><br><span class="line">str.compareToIgnoreCase(<span class="string">"FUCk"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// endsWith, returns boolean</span></span><br><span class="line">str.endsWith(<span class="string">"ck"</span>);</span><br><span class="line"><span class="comment">// startsWith, returns boolean</span></span><br><span class="line">str.startsWith(<span class="string">"wh"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// equals, equalsIgnoreCase</span></span><br><span class="line">str.equalsIgnoreCase(<span class="string">"Fuck"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 找到第一个出现的位置</span></span><br><span class="line">indexOf(<span class="keyword">char</span> chr);</span><br><span class="line">indexOf(<span class="keyword">char</span> chr, <span class="keyword">int</span> fromIndex);</span><br><span class="line">indexOf(String str, <span class="keyword">int</span> fromIndex);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 找到最后一个出现的位置</span></span><br><span class="line">lastIndexOf(<span class="keyword">char</span> chr);</span><br><span class="line"></span><br><span class="line"><span class="comment">// regionMatches, 测试两个字符串区域是否相等</span></span><br><span class="line">str.regionMatches(<span class="keyword">int</span> offset, String otherString, <span class="keyword">int</span> offset_hat, <span class="keyword">int</span> length);</span><br><span class="line"></span><br><span class="line"><span class="comment">// concat, 在仅仅只有两个string的时候是最快的</span></span><br><span class="line">str.concat(<span class="string">"you"</span>)</span><br></pre></td></tr></table></figure><h3 id="Arrays-and-ArrayList"><a href="#Arrays-and-ArrayList" class="headerlink" title="Arrays and ArrayList"></a>Arrays and ArrayList</h3><p>Java中的array是在heap中创建的，每个array的元素都会被自动初始化，该值根据不同的类型而定。</p><ul><li>int - 初始化为0</li><li>boolean - 初始化为false</li><li>double - 初始化为0.0</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// array</span></span><br><span class="line"><span class="keyword">int</span>[] nums = &#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>&#125;;</span><br><span class="line"><span class="keyword">int</span>[] nums = <span class="keyword">new</span> nums[<span class="number">6</span>];</span><br><span class="line"><span class="keyword">int</span>[] nums = <span class="keyword">new</span> nums[]&#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// iterate</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.length; i++)</span><br><span class="line">    System.out.println(nums[i]);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i: nums)</span><br><span class="line">    System.out.println(i);</span><br><span class="line"></span><br><span class="line"><span class="comment">// size</span></span><br><span class="line">len = nums.length;</span><br><span class="line"></span><br><span class="line"><span class="comment">// array的填充(常作为初始化的方法)</span></span><br><span class="line"><span class="keyword">int</span>[] nums = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">5</span>];</span><br><span class="line">Arrays.fill(nums, Integer.MAX_VALUE);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 二维数组的填充</span></span><br><span class="line"><span class="keyword">int</span>[][] nums = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">5</span>][<span class="number">5</span>];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span>[] row : nums)</span><br><span class="line">    Arrays.fill(row, Integer.MAX_VALUE);</span><br><span class="line"></span><br><span class="line"><span class="comment">// ArrayList</span></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"></span><br><span class="line"><span class="comment">// creation, 不能用正常数据类型(int, double, char),只能用Integer, Double, </span></span><br><span class="line">ArrayList&lt;Integer&gt; arr = <span class="keyword">new</span> ArrayList&lt;Integer&gt;();</span><br><span class="line"><span class="comment">// Arrays.asList() to initialize a list：</span></span><br><span class="line">List&lt;String&gt; tmp = Arrays.asList(<span class="string">"1"</span>, <span class="string">"2"</span>, <span class="string">"4"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 添加对象进入list</span></span><br><span class="line"><span class="comment">// 在index i处添加对象，i之后的对象自动往后移</span></span><br><span class="line">arr.add(i, obj);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在list末尾添加对象</span></span><br><span class="line">arr.add(obj);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 移去index i处的元素</span></span><br><span class="line">arr.remove((<span class="keyword">int</span>)i);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 移去对象</span></span><br><span class="line">arr.remove(obj);</span><br><span class="line"></span><br><span class="line"><span class="comment">// List转换为array</span></span><br><span class="line">arr.toArray();</span><br></pre></td></tr></table></figure><h3 id="Map的使用"><a href="#Map的使用" class="headerlink" title="Map的使用"></a>Map的使用</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Map 创建</span></span><br><span class="line">Map&lt;Character, Integer&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">// size</span></span><br><span class="line">map.size()</span><br><span class="line"></span><br><span class="line"><span class="comment">// clear</span></span><br><span class="line">map.clear();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 含有key/value</span></span><br><span class="line">map.containsKey();</span><br><span class="line">map.containsValue();</span><br><span class="line"></span><br><span class="line"><span class="comment">// get, put, remove</span></span><br><span class="line">map.get(k);</span><br><span class="line">map.put(k, v);</span><br><span class="line">map.remove(k);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 是否空</span></span><br><span class="line">map.isEmpty();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 键值集合</span></span><br><span class="line">map.keySet();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 快速加入或者自增</span></span><br><span class="line">map.put(key, map.getOrDefault(key, <span class="number">0</span>)+<span class="number">1</span>);</span><br></pre></td></tr></table></figure><h3 id="Queue的使用"><a href="#Queue的使用" class="headerlink" title="Queue的使用"></a>Queue的使用</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//声明</span></span><br><span class="line">Queue&lt;Integer&gt; queue = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">//添加弹出元素</span></span><br><span class="line">queue.offer(<span class="number">5</span>); <span class="comment">// if full return false</span></span><br><span class="line">queue.poll();<span class="comment">// if empty return null</span></span><br><span class="line">queue.peek();<span class="comment">// if empty return null</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//vevtor中继承的方法</span></span><br><span class="line">queue.size();</span><br><span class="line">queue.isEmpty();</span><br><span class="line"></span><br><span class="line"><span class="comment">// PriorityQueue</span></span><br><span class="line">Queue&lt;Integer&gt; queue = <span class="keyword">new</span> PriorityQueue&lt;&gt;((a,b) -&gt; b-a); <span class="comment">// maxHeap</span></span><br></pre></td></tr></table></figure><h3 id="Stack"><a href="#Stack" class="headerlink" title="Stack"></a>Stack</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 初始化</span></span><br><span class="line">Stack&lt;Integer&gt; st = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line"></span><br><span class="line">st.peek();</span><br><span class="line">st.push();</span><br><span class="line">st.pop();</span><br><span class="line">st.isEmpty();</span><br></pre></td></tr></table></figure><p>array用Arrays.sort()，如果是普通的基本数据类型，则没有lambda表达式。</p><h3 id="List-数组"><a href="#List-数组" class="headerlink" title="List 数组"></a>List 数组</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 注意后面的ArrayList不带尖括号</span></span><br><span class="line">List&lt;Integer&gt;[] arr = <span class="keyword">new</span> ArrayList[<span class="number">10</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment">// 数组排序, use new Comparator statement</span></span><br><span class="line">Arrays.sort(arr, <span class="keyword">new</span> Comparator&lt;List&lt;Integer&gt;&gt;() &#123;</span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(List&lt;Integer&gt; a, List&lt;Integer&gt;)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> a.size() - b.size();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 数组每个的初始化</span></span><br><span class="line"><span class="keyword">for</span> (i=<span class="number">0</span>;i&lt;arr.lengh;i++) arr[i] = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br></pre></td></tr></table></figure><h3 id="Arrays"><a href="#Arrays" class="headerlink" title="Arrays"></a>Arrays</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span>[] arr = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">10</span>];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) arr[i] = i;</span><br><span class="line"><span class="comment">// 1. sort, 适用基本数据类型</span></span><br><span class="line">Arrays.sort(arr); <span class="comment">// ascending</span></span><br><span class="line">Collections.sort(arr, Collections.reverseOrder()); <span class="comment">// descending</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// sort, 非基本数据类型：array of lists 为例 (new Comparator&lt;class&gt;() &#123;@Override ...&#125; ) </span></span><br><span class="line">Arrays.sort(lists, <span class="keyword">new</span> Comparator&lt;List&lt;Integer&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(List&lt;Integer&gt; a1, List&lt;Integer&gt; a2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> -a1.get(<span class="number">0</span>) + a2.get(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. binarySearch, 如果不在数组中返回(-insertIndex-1)</span></span><br><span class="line">Arrays.binarySearch(arr, <span class="number">10</span>); <span class="comment">// return </span></span><br><span class="line">Arrays.binarySearch(arr, fromIndex, toIndex, key);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. copyOfRange</span></span><br><span class="line"><span class="keyword">int</span>[] copy = Arrays.copyOfRange(arr, <span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4. equals: shallow equal 用在单个元素上</span></span><br><span class="line">Arrays.equals(arr, copy);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 5. deepEquals: deep equal 用在元素也是数组上，递归进行比较</span></span><br><span class="line">Arrays.deepEquals(arr, copy);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 6. toString (deepToString)</span></span><br><span class="line">System.out.println(Arrays.toString(arr));</span><br></pre></td></tr></table></figure><h3 id="Functional-Programming-in-Java-OJ系统中不一定快"><a href="#Functional-Programming-in-Java-OJ系统中不一定快" class="headerlink" title="Functional Programming in Java (OJ系统中不一定快)"></a>Functional Programming in Java (OJ系统中不一定快)</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// WRONG!!!!!!</span></span><br><span class="line">String[] strings = Array.stream(arr).map(x-&gt;String.valueOf(x)).toArray(String[]::<span class="keyword">new</span>);</span><br></pre></td></tr></table></figure><p><strong>以上代码会报错，</strong>这是由于Java实现了IntStream, LongStream, DoubleStream三个基本类型的stream，这些基本类型的stream有更好的性能，但是需要注意的是这些基本类型的stream的map函数只支持自身类型-&gt;自身类型(int-&gt;int/long-&gt;long/double-&gt;double)的映射，如果需要进行String类型的转换那么我们需要使用mapToObj()函数。</p><p>三个基本类型的Stream相互转化的函数为mapToDouble(), mapToInt(), mapToLong().</p><p><strong>应当优先使用基本类型的stream</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// List, Array</span></span><br><span class="line">Stream&lt;Integer&gt; stream = Arrays.stream(arr);</span><br><span class="line">Stream&lt;Integer&gt; stream = list.stream();</span><br><span class="line"></span><br><span class="line"><span class="comment">// map (Integer / int)</span></span><br><span class="line">arr = Arrays.stream(arr).map(x-&gt;x*<span class="number">2</span>).toArray(); <span class="comment">// no function passed to toArray() call.</span></span><br><span class="line">String[] strings = Array.stream(arr).mapToObj(x-&gt;String.valueOf(x)).toArray(String[]::<span class="keyword">new</span>); </span><br><span class="line">String[] strings = Array.stream(arr).mapToObj(x-&gt;String.valueOf(x)).toArray(n-&gt;<span class="keyword">new</span> String[n]);</span><br><span class="line"></span><br><span class="line"><span class="comment">// filter</span></span><br><span class="line">arr = Arrays.stream(arr).filter(x-&gt;x&gt;<span class="number">10</span>).toArray();</span><br><span class="line">String[] strings = list.stream().filter(x-&gt;x&gt;<span class="number">10</span>).toArray(String[]::<span class="keyword">new</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// reduce</span></span><br><span class="line"><span class="keyword">int</span> sum = Arrays.stream(arr).sum(); <span class="comment">// 只能够用在基本类型stream上</span></span><br><span class="line"><span class="keyword">int</span> sum = Arrays.stream(arr).reduce(<span class="number">0</span>, (a,b)-&gt;a+b);</span><br><span class="line"></span><br><span class="line"><span class="comment">// forEach 可以很帅地打印</span></span><br><span class="line">Arrays.stream(arr).forEach(x-&gt;System.out.println(x));</span><br><span class="line"></span><br><span class="line"><span class="comment">// IntStream的很帅的用法</span></span><br><span class="line"><span class="comment">// 快速初始化</span></span><br><span class="line">IntStream stream = IntStream.of(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>);</span><br><span class="line">IntStream range = IntStream.range(<span class="number">1</span>, <span class="number">10</span>);</span><br><span class="line">IntStream rangeClosed = IntStream.rangeClosed(<span class="number">1</span>, <span class="number">10</span>); <span class="comment">// 包含10的range</span></span><br><span class="line">IntStream infinite = IntStream.iterate(<span class="number">1</span>, operand-&gt;oprand + <span class="number">2</span>); <span class="comment">// 生成一个无限生成奇数的generator</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span>[] arr = <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, -<span class="number">1</span>&#125;;</span><br><span class="line">String[] strs = <span class="keyword">new</span> String[]&#123;<span class="string">"what"</span>, <span class="string">"the"</span>, <span class="string">"fuck"</span>&#125;;</span><br><span class="line"><span class="comment">// 求最大、最小、求和 (如果没有 .get() / .orElse() 则返回的是optional)</span></span><br><span class="line"><span class="keyword">int</span> max = Arrays.stream(arr).max().orElse(Integer.MIN_VALUE);</span><br><span class="line"><span class="keyword">int</span> min = Arrays.stream(arr).min().orElse(Integer.MAX_VALUE);</span><br><span class="line"><span class="keyword">int</span> sum = Arrays.stream(arr).sum().orElse(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">String max = Arrays.stream(strs).max((a,b)-&gt;a.compareTo(b)).orElse(<span class="string">""</span>);</span><br><span class="line">String sum = Arrays.stream(strs).reduce(<span class="string">""</span>, (a,b)-&gt;a.concate(b));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 另一种有趣写法</span></span><br><span class="line"><span class="keyword">int</span> max = Arrays.stream(arr).reduce(Integer.MIN_VALUE, Math::max);</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Grammar&quot;&gt;&lt;a href=&quot;#Grammar&quot; class=&quot;headerlink&quot; title=&quot;Grammar&quot;&gt;&lt;/a&gt;Grammar&lt;/h1&gt;&lt;h3 id=&quot;BufferedReader-比较快&quot;&gt;&lt;a href=&quot;#BufferedReader-
      
    
    </summary>
    
    
      <category term="reality" scheme="http://www.shihaizhou.com/tags/reality/"/>
    
  </entry>
  
  <entry>
    <title>Graph Neural Networks Basics</title>
    <link href="http://www.shihaizhou.com/2020/02/22/Graph-Neural-Networks-Basics/"/>
    <id>http://www.shihaizhou.com/2020/02/22/Graph-Neural-Networks-Basics/</id>
    <published>2020-02-22T03:28:57.000Z</published>
    <updated>2020-02-27T05:46:14.315Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong>:</p><ul><li><a href="https://arxiv.org/abs/1901.00596" target="_blank" rel="noopener">paper “A Comprehensive Survey on Graph Neural Networks”</a></li><li><a href="https://csustan.csustan.edu/~tom/Clustering/GraphLaplacian-tutorial.pdf" target="_blank" rel="noopener">Graph Laplacian Tutorial</a></li><li><a href="https://www.quora.com/Whats-the-intuition-behind-a-Laplacian-matrix-Im-not-so-much-interested-in-mathematical-details-or-technical-applications-Im-trying-to-grasp-what-a-laplacian-matrix-actually-represents-and-what-aspects-of-a-graph-it-makes-accessible" target="_blank" rel="noopener">Graph Laplacian explanation on Quora</a></li><li><a href="https://www.zhihu.com/question/26822364" target="_blank" rel="noopener">Laplacian explanation on Zhihu</a></li><li><a href="http://www.norbertwiener.umd.edu/Research/lectures/2014/MBegue_Prelim.pdf" target="_blank" rel="noopener">Graph Fourier Transformation Tutorial</a></li><li><a href="https://zhuanlan.zhihu.com/p/67336297" target="_blank" rel="noopener">拉普拉斯算子和拉普拉斯矩阵</a></li><li><a href="https://arxiv.org/abs/1606.09375" target="_blank" rel="noopener">paper “Chebyshev Spectral CNN (ChebNet)”</a> </li><li><a href="https://arxiv.org/abs/0912.3848" target="_blank" rel="noopener">paper “Wavelets on Graphs via Spectral Graph Theory”</a></li></ul><h1 id="Notations"><a href="#Notations" class="headerlink" title="Notations"></a>Notations</h1><p><strong>Definition 1</strong> (Graph). A <strong>graph</strong> is represented as $G=(V,E)$, where $V$ is the set of vertices or nodes, and $E$ is the set of edges. In the graph, let $v_i \in V$ to denote a node and $e_{ij}=(v_i, v_j) \in E$ to denote an edge pointing from $v_i$ to $v_j$. （文章中写的是$v_j$ to $v_i$）</p><p><strong>Definition 2</strong> (Adjacency matrix &amp; degree matrix). The <strong>adjacency matrix</strong> $A$ of a graph is a $n\times n$ matrix with</p><script type="math/tex; mode=display">A_{ij} = \left\{    \begin{array}{lr}    1, \quad (e_{ij} \in E)\\     0. \quad (e_{ij} \notin E)    \end{array}\right.</script><p>The <strong>degree matrix</strong> $D$ is a $n\times n$ diagonal matrix with</p><script type="math/tex; mode=display">D_{ii}=\sum_{j=1}^n A_{ij} .</script><p><strong>Lemma 1</strong> (Power of adjacency matrix). Let $G$ be a weighted graph, with binary adjacency matrix $A$.  Let $\tilde A$ be the adjacency matrix with unit loops added on every vertex, e.g. $A_{m,n} =\tilde A_{m,n}$ for $m\neq n$ and $\tilde A_{m,n} =1$ for $m=n$. Then for each $s &gt; 0$, $(\tilde A^s)_{m,n}$ equals the number of paths of length s connecting $m$ and $n$, and $(\tilde A^s)_{m,n}$ equals the number of all paths of length $r \leq s$ connecting m and n. <strong>（注意：这条引理对于GNN之后推导Locality非常重要）</strong></p><p><strong>Definition 3</strong> (Neighborhood of a node). The <strong>neighborhood of a node</strong> $v$ is defined as </p><script type="math/tex; mode=display">N(v)=\{u \in V | (u,v) \in E\}</script><p><strong>Definition 4</strong> (Node attributes). The <strong>node attributes of the graph</strong> is represented as $X$, where $X \in \mathbb R ^{n\times d}$ is called as node feature matrix. $x_v \in \mathbb R^d$ represents the feature vector of node $v$. </p><p><strong>Definition 5</strong> (Edge attributes). The <strong>edge attributes of the graph</strong> is represented as $X^e$, where $X^e \in \mathbb R^{m\times c}$ is an edge feature matrix with $x_{v,u}^e ∈ \mathbb R^c$ representing the feature vector of an edge $(v, u)$.</p><p><strong>Definition 6</strong> (Spatial-Temporal Graph). A <strong>spatial-temporal graph</strong> is an attributed graph where the node attributes change dynamically over time. The spatial-temporal graph is defined as $G^{(t)} =(V,E,X^{(t)})$ with $X^{(t)} ∈ \mathbb R^{n\times d}$. </p><p>常用的Notation表示一览：</p><p><img src="https://i.loli.net/2020/02/19/iSPnINotEzhWHxA.png" alt="image.png"></p><h1 id="Graph-Laplacian"><a href="#Graph-Laplacian" class="headerlink" title="Graph Laplacian"></a>Graph Laplacian</h1><p>这里我们主要讨论的是undirected graph，对于directed graph来说有其他更多的工作。</p><h2 id="How-to-Define-Graph-Laplacian"><a href="#How-to-Define-Graph-Laplacian" class="headerlink" title="How to Define Graph Laplacian?"></a>How to Define Graph Laplacian?</h2><p><strong>Definition 7</strong> (Euclidian Laplacian). Laplacian operator in Euclidian space is defined by</p><script type="math/tex; mode=display">\Delta f = \nabla^2 f = \nabla \cdot \nabla f</script><p>Equivalently, the Laplacian of $f$ is the sum of all the <em>unmixed</em> second partial derivatives in the Cartesian coordinates $x_i$:</p><script type="math/tex; mode=display">\Delta f = \sum_{i=1}^n \frac {\partial^2 f}{\partial x^2_i}</script><p>在欧式空间的拉普拉斯算子就是一个实值函数“梯度的散度”。从物理上来理解拉普拉斯算子的重要性：<strong>标量函数的梯度往往是一种“驱动力”（或者叫“势”），而针对“驱动力”求散度就可以知道空间中“源”的分布。</strong></p><p>将欧式空间中的Laplacian operator推广到Graph上，需要回答以下几个问题：</p><ul><li>What is the meaning of a function over a graph?</li><li>How do we define the gradient of a graph function?</li><li>How do we define the divergence of the graph gradient?</li></ul><h2 id="Graph-Laplacian-Defintion-amp-Intuition"><a href="#Graph-Laplacian-Defintion-amp-Intuition" class="headerlink" title="Graph Laplacian Defintion &amp; Intuition"></a>Graph Laplacian Defintion &amp; Intuition</h2><p>首先为了定义图函数，我们将图在 $xOy$ 平面上延展开，将节点看成自变量（也就是说节点集合是 $xOy$ 平面上的一个子集），定义函数$ f:V\rightarrow \mathbb R$ 是一个从节点到实数的映射。</p><p>接着为了定义图函数上的gradient，回忆起在数值分析中我们学过的difference的概念，也就是在离散空间中梯度的近似近似。同样的我们可以定义：在图上，两个连通节点函数值的difference就是图上的gradient。Gradient of the function along the edge $e=(u,v)$ is given by $\text{grad}(f)|_e=f(u)−f(v)$. </p><p>在这个基础上我们先给出graph laplacian的定义：</p><p><strong>Definition 8</strong> (Graph Laplacian operator). <strong>Graph Laplacian</strong> operator $\Delta$ with given graph function $f$ and given node $x$ is defined as</p><script type="math/tex; mode=display">\Delta f(x) = \sum_{y \sim x} f(x)-f(y)</script><p>其中$y \sim x$表示节点$y$是$x$的邻接点，也可以表示成$y \in N(x)$. 这条式子定义了一个节点的Laplacian等于该节点所有的incident edge上gradient的和。这就导致了一些疑问：<strong>Laplacian是梯度的散度，按照道理应当是gradient的导数的和，为什么这里少了一次求导呢？</strong></p><p>为了解决这个疑问，我们回顾在离散空间中的Laplacian是怎样定义的。在一维离散空间上的Laplacian是二阶差分（second-order difference）：</p><script type="math/tex; mode=display">\Delta f(x) = \frac{f(x+1)-2f(x)+f(x-1)}{1^2}</script><p>而在二维离散空间上的Laplacian算子是矩阵：</p><script type="math/tex; mode=display">L=\left[\matrix{0 & 1 & 0 \\1 & -4 & 1 \\0 & 1 & 0 \\}\right]</script><p>该矩阵可以看作是两个正交方向（$x$方向和$y$方向）上二阶差分的叠加。通过对比以上的定义，我们需要解决一个问题：如何在graph上计算二阶差分？事实上laplacian在直观上没有办法计算二阶差分，因为<strong>在同一个维度（边）上找不到3个节点</strong>。为了直观地理解graph laplacian，我们需要重新回到向量场散度的定义。</p><p><strong>Definition 9</strong> (Divergence). The <strong>divergence of a vector field</strong> $F(x)$ at a point $x_0$ is defined as the limit of the ratio of the surface integral of $F$ out of the surface of a closed volume $V$ enclosing $x_0$ to the volume of $V$, as $V$ shrinks to zero. </p><script type="math/tex; mode=display">\left.\operatorname{div} \mathbf{F}\right|_{\mathbf{x}_{0}}=\lim _{V \rightarrow 0} \frac{1}{|V|} \int_{S(V)} \mathbf{F} \cdot \hat{\mathbf{n}} d S</script><p>直观上来讲，散度是计算一点附近邻域内通量的极限。为了将这个概念推广到图结构的离散空间中，我们可以将图结构重新映射到离散的三维空间，我们做以下几个假设：</p><ul><li>空间中体积的单位元（最小元）是$1\times 1\times 1$的小立方块；</li><li>图节点是一个单位元；</li><li>相邻节点有一个面（边）紧贴在一起，在这个面上根据边的梯度定义梯度向量。</li></ul><p>在这个空间中，节点、边、梯度向量场都是离散的。考察具体例子：图$G=(V,E)$以及在图上定义的实值函数 $f:V \rightarrow \mathbb R$，其中$V=\{v_1, v_2, v_3\}$，$E=\{(v_1,v_2), (v_1,v_3)\}$。将这个图映射到我们以上定义的离散空间中，有以下：</p><p><img src="https://i.loli.net/2020/02/21/tSsVPIxkZH6EfnN.png" alt="image.png"></p><p>该图中的两个箭头表示的梯度向量场分别为 $f(v_1)-f(v_2)$ 以及 $f(v_1)-f(v_3)$. 除此之外的空间中的向量均为$0$向量。那么根据定义，关于节点$v_1$的Laplacian为：</p><script type="math/tex; mode=display">\begin{align}\left.\operatorname{div} \mathbf{F}\right|_{\mathbf{v}_{1}} &=\lim _{V \rightarrow 0} \frac{1}{|V|} \int_{S(V)} \mathbf{F} \cdot \hat{\mathbf{n}} d S \\&= \frac{1}{|1^3|} [1^2(f(v_1)-f(v_2)) + 1^2(f(v_1)-f(v_2))] \\&= \sum_{v_j \sim v_1}f(v_1)-f(v_j)\end{align}</script><p>对于更高维度的空间（节点度数$&gt;$8），因为我们认为不同的边是相互正交的，故而也是适用的。从而我们给出了graph laplacian一个更加直观的、符合原定义的解释。</p><h2 id="Incidence-Matrix"><a href="#Incidence-Matrix" class="headerlink" title="Incidence Matrix"></a>Incidence Matrix</h2><p>更进一步，我们可以引入关联矩阵（Incidence Matrix）定义一个图函数对于每一条边的gradient：</p><script type="math/tex; mode=display">\text{grad}(f) = K^Tf</script><p>where $K$ is an $V\times E$ matrix called the <strong>incidence matrix</strong>. $K$ is constructed as follows: For every edge $e=(u,v)$ in the graph, we assign $K_{u,e}=+1$ and $K_{v,e}=-1$. 我们可以将graph gradient看成是一个新的定义在edge上的函数：$g=\text{grad}(f)$.</p><p>关于incidence matrix，有两点需要注意。首先在图论中有一个非常重要的性质：我们认为<strong>图上的边相互正交</strong>。接着我们可以把一条边看作是空间中的一个维度，所有的边就构成了一个$|E|$维的单位正交基，随意指定维度坐标轴的正向（也就是指定incidence matrix上+1和-1的位置），这样用边集作为基表示的节点集就是incidence matrix.</p><p><img src="https://i.loli.net/2020/02/20/6cSpKACveBZmXYJ.png" alt="image.png"></p><p>（注意：graph laplacian可以作为母定义来推导出网格状的二阶差分，但是和网格状的二阶差分相差一个负号）</p><p>在graph gradient的基础上定义散度：$\text{div}(g)=Kg$. 而根据定义梯度的散度就是Laplacian，由此我们得到以下定义</p><p><strong>Definition 8*</strong> (Graph Laplacian operator). <strong>Graph Laplacian</strong> operator $\Delta$ with given graph function $f$ is defined as</p><script type="math/tex; mode=display">\Delta f = \text{div}(\text{grad}(f)) = KK^Tf</script><p><strong>Definition 10</strong> (Standard Graph Laplacian Matrix). <strong>Graph Laplacian Matrix</strong> $L$ is a $|V|\times |V|$ matrix defined by</p><script type="math/tex; mode=display">L = KK^T=D-A</script><p>where $K$, $A$ and $D$ is the incidence matrix, adjacency matrix and degree matrix respectively of the given graph. </p><p>Graph Laplacian和smoothness of graph有一些关联。<strong>Dirichlet Energy</strong> 定义了图的某种势能，它和variance的定义有些类似：相邻节点的function value应当和他们的edge weight成负相关。Dirichlet Energy可以由Graph Laplacian表示：</p><script type="math/tex; mode=display">E(f)=\frac{1}{2}\sum_{u∼v}w_{uv}(f(u)-f(v))^2=\|K^Tf\|^2=f^TLf</script><p>在单位球面（$f^Tf=1​$）上能够minimize Dirichlet energy的 $f​$ 就是 $L​$ 的smallest nonzero eigenvector。关于这部分的内容在<a href="https://www.quora.com/Whats-the-intuition-behind-a-Laplacian-matrix-Im-not-so-much-interested-in-mathematical-details-or-technical-applications-Im-trying-to-grasp-what-a-laplacian-matrix-actually-represents-and-what-aspects-of-a-graph-it-makes-accessible" target="_blank" rel="noopener">Graph Laplacian explanation on Quora</a>有更多的解释。</p><h1 id="Graph-Convolution"><a href="#Graph-Convolution" class="headerlink" title="Graph Convolution"></a>Graph Convolution</h1><p><strong>Definition 11</strong> (Normalized Graph Laplacian Matrix). The <strong>normalized graph laplacian matrix</strong> $L_n$ is defined as</p><script type="math/tex; mode=display">L_n =I-D^{- \frac{1}{2} } A D^{- \frac{1}{2} }</script><p><strong>Lemma 1*</strong> (Locality of High Power of Laplacian matrix). Let $G$ be a weighted graph, $L$ the graph Laplacian (normalized or non-normalized) and $s &gt; 0$ an integer. For any two vertices $m$ and $n$, if $d_G(m, n) &gt; s$ then $(L^s)_{m,n} = 0​$. 这条引理告诉我们高次的graph laplacian具有高次的局部连通性，具体的证明参考<a href="https://arxiv.org/abs/0912.3848" target="_blank" rel="noopener">paper “Wavelets on Graphs via Spectral Graph Theory”</a>。</p><p><strong>Lemma 2</strong> (Spectral Decomposition). If $A$ is a real symetric matrix, then it can be decomposed as</p><script type="math/tex; mode=display">A =Q\Lambda Q^T</script><p>where $Q$ is an orthogonal matrix whose columns are the eigenvectors of $A$, and $\Lambda$ is a diagonal matrix whose entries are the eigenvalues of $A$.</p><p>利用<strong>Lemma 2</strong>，我们可以将normalized graph Laplacian $L_n$分解为</p><script type="math/tex; mode=display">L_n=U\Lambda U^T</script><p>其中 $U$ 是由$L_n$的特征向量作为列向量构成的$n\times n$方阵，$U$构成了一个orhonormal space，即$U^TU=I$。$\Lambda$是对角元素为$U$对应的特征值的对角阵，$\Lambda$也被称为谱（spectrum）。</p><p><strong>Definition 12</strong> (Graph Fourier Transform). The <strong>graph Fourier transform</strong> is defined by</p><script type="math/tex; mode=display">\mathscr F(x)=U^Tx</script><p>where $x\in \mathbb R^{|V|}$ is a feature vector (graph signal) of the graph, which means the i-th value of $x$ is the value of the i-th node. </p><p><strong>Definition 12*</strong> (Inverse Graph Fourier Transform). The <strong>inverse graph Fourier transform</strong> is defined by</p><script type="math/tex; mode=display">\mathscr F^{-1}(\hat x)=U\hat x</script><p>where $\hat x$ is a the result of the graph Fourier transform. </p><blockquote><p>The graph Fourier transform projects the input graph signal to the orthonormal space where the basis is formed by eigenvectors of the nor-malized graph Laplacian. </p></blockquote><p><strong>Definition 13</strong> (Graph Convolution). The <strong>graph convolution</strong> of the input signal $x$ with a filter $g\in \mathbb R^{|V|}$ is defined as</p><script type="math/tex; mode=display">\begin{aligned} \mathbf{x} *_{G} \mathbf{g} &=\mathscr{F}^{-1}(\mathscr{F}(\mathbf{x}) \odot \mathscr{F}(\mathbf{g})) \\ &=\mathbf{U}\left(\mathbf{U}^{T} \mathbf{x} \odot \mathbf{U}^{T} \mathbf{g}\right) \end{aligned}</script><p>If we denote a filter as $g_\theta = \text{diag}(U^Tg)$, then the spectral graph convolution is simplified as</p><script type="math/tex; mode=display">\begin{align}\mathbf{x} *_{G} \mathbf{g} _{\theta}&= \mathbf{U}\left(\mathbf{U}^{T} \mathbf{x} \odot \mathbf{U}^{T} \mathbf{g}\right) \\&= \mathbf{U}\left(\mathbf{U}^{T} \mathbf{g} \odot \mathbf{U}^{T} \mathbf{x}\right) \\&= \mathbf{U}\left(\text{diag}(\mathbf{U}^{T} \mathbf{g}) (\mathbf{U}^{T} \mathbf{x}\right)) \\&= \mathbf{U} \mathbf{g}_{\theta} \mathbf{U}^{T} \mathbf{x}\end{align}</script><p>所有的Spectral-based ConvGNNs都遵循这个convolution的定义，主要的区别就在于选取不同的filter $g$. 这里我们可以看到，因为$U$是orthonormal basis，所以$U^T$是一个可逆矩阵，也是一个可逆映射。因此当我们参数化$g$的时候可以将这一步矩阵乘法省略，用一个简单的对角矩阵来代替即可。</p><p>如果我们将spectrum $\Lambda$作为$g_\theta$的参数，那么我们称满足以下条件的filter称为non-parametric filter，也就是这个filter的自由度就是自身的维度。</p><script type="math/tex; mode=display">g_{\theta}(\Lambda)=\operatorname{diag}(\theta)</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1901.00596&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;paper “A Comprehensive
      
    
    </summary>
    
    
      <category term="GNN" scheme="http://www.shihaizhou.com/tags/GNN/"/>
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
  </entry>
  
  <entry>
    <title>Information Theory</title>
    <link href="http://www.shihaizhou.com/2020/02/17/Information-Theory-Simple-Concepts/"/>
    <id>http://www.shihaizhou.com/2020/02/17/Information-Theory-Simple-Concepts/</id>
    <published>2020-02-17T07:59:39.000Z</published>
    <updated>2020-02-20T08:16:03.748Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><p><a href="https://arxiv.org/pdf/1708.07459.pdf" target="_blank" rel="noopener">arxiv文章 “Divergence, Entropy, Information”</a></p></li><li><p><a href="http://colah.github.io/posts/2015-09-Visual-Information/" target="_blank" rel="noopener">博客文章 “Visual Information Theory”</a></p></li><li><p><a href="http://www.cs.cmu.edu/~aarti/Class/10704/lecs.html" target="_blank" rel="noopener">CMU课程</a></p></li></ul><h1 id="Divergence"><a href="#Divergence" class="headerlink" title="Divergence"></a>Divergence</h1><p>散度divergence $d(p,q)$通常被用来表征”surprise”的程度：你相信一个分布为$q$，经过试验却发现它实际上是$p$的惊讶程度。为了引出KL-Divergence，我们首先定义概率单纯型(Probability Simplex).</p><p><strong>Definition</strong>  (Probability Simplex). For any finite alphabet $\mathcal{X}$ with $|\mathcal{X}|=m$, the probability simplex $\mathcal P^{\mathcal X}$ is the set</p><script type="math/tex; mode=display">\mathcal{P}^{\mathcal X} \triangleq\left\{q \in \mathbb{R}^{m} | \sum_{i} q_{i}=1, q_{i} \geq 0 \quad \forall i\right\}</script><p>注意：Probability Simplex是一个维度为$m-1$的空间，因为这个空间上的向量需要满足$\sum_{i} q_{i}=1$，从而导致其少了一个自由度。</p><p><strong>Definition</strong> (Kullback-Leibler Divergence). For $p, q \in \mathcal{P}^{\mathcal X}$, the <strong>Kullback-Leibler (KL) divergence</strong> of $q$ from $p$ is</p><script type="math/tex; mode=display">d(p, q) \triangleq \sum_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)}</script><p>where we are using the conventions that $\log \infty=\infty$, $\log 0=-\infty$, $0/0=0$, and $0\times \infty=0$.</p><p>注意：同entropy不同，KL divergence不仅仅局限于离散型变量，对于连续型变量也是适用的：</p><script type="math/tex; mode=display">D_{K L}(p(x) \| q(x))=\int_{-\infty}^{\infty} p(x) \ln \frac{p(x)}{q(x)} d x</script><p>以上也是为什么作者会从KL divergence开始介绍Information Theory，因为KL divergence是一个更加广泛的定义。在实际计算两个分布的的KL divergence时，因为采样会不完全，所以需要用到以上数值的习惯性定义，或者使用function approximation来拟合观察到的分布；另一种做法是在计算的时候使用平滑策略：为两个变量missing $x$值赋予一个小的频率常数$\epsilon$，这部分多出来的频率由其他采样过的$x$支付，这样就保证了所有的频率都是非零的。</p><p><strong>Theorem</strong> (Gibbs’ Inequality). For all $p, q \in \mathcal{P}^{\mathcal{X}}$ , it holds that $d(p, q) \geq 0$, with equality iff $p = q$.</p><p>证明这个定理的思路：</p><ul><li>等价于对于任意的真实分布$p$，$\min d(p,q) = 0$，且等号成立条件为$q=p$.</li><li>使用拉格朗日乘数法计算梯度为0的点即可.</li></ul><p>注意：这条定理告诉我们为什么KL divergence和距离的定义非常相似，因为它满足了距离中的正则性。而同时我们需要注意的是它并不满足距离的对称性以及距离的三角不等式性质。</p><p>在机器学习中，我们经常需要做极大似然估计：</p><script type="math/tex; mode=display">\theta^{*}=\underset{\theta}{\operatorname{argmax}} \prod_{i=1}^{n} p_{X}^{\theta}\left(x_{i}\right)</script><p>其中，$p_{X}^{\theta}$表示的是由参数$\theta$决定的在随机变量$X$上的概率分布。而其实我们可以证明，极大似然估计的等价就是极小KL divergence函数的优化：</p><script type="math/tex; mode=display">\theta^{*} = \arg\min_\theta d(\hat{p}_X,p^\theta_X)</script><h1 id="Entropy"><a href="#Entropy" class="headerlink" title="Entropy"></a>Entropy</h1><p><strong>Definition</strong> (Shannon Entropy). The <strong>Shannon entropy</strong> of $p \in \mathcal{P}^{\mathcal X}$ is</p><script type="math/tex; mode=display">H(p) \triangleq-\sum_{x \in \mathcal{X}} p(x) \log p(x)</script><p>我们也使用 $H(X)$ 来表示随机变量$X$的熵。</p><p><strong>Theorem</strong> The Shannon entropy is related to the divergence according to the formula</p><script type="math/tex; mode=display">H(p) = \log |X| - d(p,u)</script><p>其中$u$表示在集合$X$上的均匀分布。通过这条定理我们可以知道一个分布的熵和你相信这个分布是均匀分布时得到的surprise是负相关的，同时因为Gibbs’ Inequality，我们知道当一个分布是均匀分布的时候熵取到最大。另外，这条定理也告诉我们熵是不能够在连续型随机变量上定义的，因为在$\mathbb R$上是没有“均匀分布的”.</p><p>而对于更加常规的信息论学习，通常是先介绍Entropy的定义，然后转而介绍KL divergence等。Entropy翻译作信息熵，也就是一个离散型随机变量的不确定性。不确定性的定义可以被表示成一个编码问题：一个alphabet中的每个事件已知其发生的概率分布，该如何对每个事件进行编码从而能够复出最小的代价？以下为结论：如果事件$x$发生的概率为$p(x)$，最优编码位数为$-\log_2{p(x)}$，则该事件的总代价为$-p(x)\log_2{p(x)}$。从而有了以上的Entropy定义。均匀分布的时候拥有最大的不确定性，也就是对其编码的代价最大。</p><h2 id="Conditional-Entropy"><a href="#Conditional-Entropy" class="headerlink" title="Conditional Entropy"></a>Conditional Entropy</h2><p>概率论中最美妙的就是引入了Conditional Probability $P(A|B)$，也就是$B$事件发生的前提下$A$事件发生的概率。同样的在信息论中我们也会关注到多变量分布的情况。为了表述清晰，以下是我们会用到的符号及其意义：</p><ul><li>$p_{X} \in \mathcal{P}^{\mathcal X}$: 一个在alphabet $\mathcal X$上的随机变量$X$的分布$p$</li><li>$p_{Y} \in \mathcal{P}^{\mathcal Y}$: 一个在alphabet $\mathcal Y$上的随机变量$Y$的分布$p$</li><li>$p_{X,Y} \in \mathcal{P}^{\mathcal X \times \mathcal Y}$: 一个在他们上面的联合分布$p$</li><li>$p_{X} \otimes p_{Y} \in \mathcal{P}^{\mathcal{X} \times \mathcal Y}$: 定义为边缘分布的乘积，也即$\left(p_{X} \otimes p_{Y}\right)(x, y)= p_{X}(x) p_{Y}(y)$.</li></ul><p><strong>Definition</strong> (Conditional Entropy). The <strong>conditional entropy</strong> of $X$ given $Y$ is</p><script type="math/tex; mode=display">H(X | Y) \triangleq \sum_{x, y \in \mathcal{X} \times \mathcal{Y}} p_{X, Y}(x, y) \log p_{X | Y}(x | y)</script><p>注意：有人可能会觉得为了维持对称性，需要将条件熵定义为：</p><script type="math/tex; mode=display">\tilde{H}(X | Y)=\sum_{x, y \in \mathcal{X} \times \mathcal{Y}} p_{X | Y}(x | y) \log p_{X | Y}(x | y)</script><blockquote><p>However, a quick think makes clear that this definition isn’t appropriate, because it doesn’t include any information about the distribution of $Y$. If $Y$ is concentrated around some very informative (or uninformative) values, then $\tilde H$ won’t notice that some values are more valuable than others. </p></blockquote><p><strong>Theorem</strong> The conditional entropy is related to the unconditional entropy as</p><script type="math/tex; mode=display">H(X|Y)= H(X,Y)-H(Y)</script><p>where $H(X,Y)$ is the entropy of the joint distribution $p_{X,Y}$. </p><p>这一条定理非常容易记住，只需要和条件概率公式进行比对：</p><script type="math/tex; mode=display">p_{X|Y}(x|y)={p_{X,Y}(x,y)\over p_Y(y)}</script><p>或者用更加通俗易懂的语言来解释它：</p><blockquote><p>The uncertainty you have about $X$ given that you’ve been told $Y$ is equal to the uncertainty you had about both $X$ and $Y$ , less the uncertainty that was resolved when you learned $Y$.</p></blockquote><p><strong>Theorem</strong> (Side Information reduces uncertainty). </p><script type="math/tex; mode=display">H(X|Y) \leq H(X)</script><p>该定理：额外的信息不会增加一个随机变量的不确定性。提供额外的信息$Y$造成的增益$H(X)-H(X|Y) \geq 0$，增益为0时就是$Y$和$X$互不相关(Y carries no information about X)。</p><h1 id="Information"><a href="#Information" class="headerlink" title="Information"></a>Information</h1><p><strong>Definition</strong> (Mutual Information). The <strong>mutual information</strong> $I(X,Y)$ in $Y$ about $X$ is </p><script type="math/tex; mode=display">I(X,Y) \triangleq H(X) - H(X|Y).</script><p><strong>Theorem</strong> The mutual information may also be written as</p><script type="math/tex; mode=display">\begin{align}I(X,Y) &= d(p_{X,Y}, p_{X} \otimes p_{Y}) \\&= \mathbb E _Y[d(p_{X|Y}, p_X)]\end{align}</script><p>以上的两个表示方法能够通过简单的代数推导得出，而根据第一条等式我们互换$XY$变量可以得到以下推论</p><p><strong>Corrolary</strong> The mutual information is symmetric:</p><script type="math/tex; mode=display">I(X,Y) = I(Y,X)</script><p>而第二条等式的含义则是如果你选择无视$Y$给出的side information并还是认为分布是按照$p_X$的，那么mutual information或者就等于你选择忽视和接受Y之间的surprise。</p><p><strong>Theorem</strong> (Data Processing Inequality). Let $X$ and $Y$ be random variables, and let $Z = g(Y)$, where $g$ is any function $g : Y \rightarrow Z$. Then,</p><script type="math/tex; mode=display">I(X,Z) \leq I(X,Y)</script><p>这条定理告诉我们，对于一个作为side information的随机变量，对这个变量（或者该变量的估计）进行任意的函数映射操作，所得到的结果作为新的side information，其提供的mutual information一定不会超过原来的随机变量。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1708.07459.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;arxiv文章 “Diverge
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
  </entry>
  
  <entry>
    <title>Metric Space</title>
    <link href="http://www.shihaizhou.com/2020/02/11/Some-Maths/"/>
    <id>http://www.shihaizhou.com/2020/02/11/Some-Maths/</id>
    <published>2020-02-11T07:58:43.000Z</published>
    <updated>2020-02-13T11:52:20.483Z</updated>
    
    <content type="html"><![CDATA[<h1 id="度量空间"><a href="#度量空间" class="headerlink" title="度量空间"></a>度量空间</h1><p>度量空间也叫做距离空间（metric space）。这是一种拓扑空间，其上的拓扑由指定的距离决定。在一个空间中引入距离是为了刻画“收敛”这个概念，同时距离也能够定义在向量空间中的（准）范数。</p><h3 id="距离空间"><a href="#距离空间" class="headerlink" title="距离空间"></a>距离空间</h3><p>距离空间$\mathscr{X}$是指非空集$\mathscr{X}$上定义了一个双变量的实值函数$\rho(x,y)$，该函数满足以下三个条件：</p><ul><li>$\rho(x,y) \geq 0$，等号成立条件当且仅当$x=y$ （非负）</li><li>$\rho(x,y)=\rho(y,x)$ （可交换）</li><li>$\rho(x,y)\leq \rho(x,z) + \rho(z,y)$ （三角不等式）</li></ul><p>以$\rho$为距离的度量空间记作$(\mathscr{X}, \rho)$。“距离”的概念是对欧式空间距离的抽象，不难发现欧式空间两点间的距离满足以上的条件。</p><h3 id="空间-C-a-b"><a href="#空间-C-a-b" class="headerlink" title="空间$C[a,b]$"></a>空间$C[a,b]$</h3><p>空间$C[a,b]$定义为在区间$[a.b]$上的连续函数全体（即这个空间内的点就是函数）。在其上定义距离</p><script type="math/tex; mode=display">\rho(x,y) \triangleq \max_{a<t<b}|x(t)-y(t)|</script><p>距离空间$(C[a,b], \rho)$在以后简记为$C[a,b]$.</p><h3 id="收敛"><a href="#收敛" class="headerlink" title="收敛"></a>收敛</h3><p>距离空间$(\mathscr{X}, \rho)$上的点列$\{x_n\}$叫做收敛到$x_0$指的是：</p><script type="math/tex; mode=display">\rho\left(x_{n}, x_{0}\right) \rightarrow 0(n \rightarrow \infty)</script><p>这时也记作$\lim _{n \rightarrow \infty} x_{n}=x_{0}$，或者简单记作$x_n \rightarrow x_0$.</p><p>而在函数空间$C[a,b]$中的点列$\{x_n\}$收敛到$x_0$指的是$\{x_n(t)\}$一致收敛到$x_0(t)$.</p><h3 id="完备"><a href="#完备" class="headerlink" title="完备"></a>完备</h3><p>基本列（柯西列）：点列$\{x_n\}$是基本列，如果 $\rho\left(x_{n}, x_{m}\right) \rightarrow 0(n, m \rightarrow \infty)$。</p><p>如果空间中的所有基本列都是收敛列，那么这个空间就是完备的。</p><h3 id="连续映射"><a href="#连续映射" class="headerlink" title="连续映射"></a>连续映射</h3><p>设$T:(\mathscr{X}, \rho) \rightarrow(\mathscr{Y}, r)$ 是一个映射。如果对于$\mathscr{X}$中的任意点列$\{x_n\}$和点$x_0$，都有：</p><script type="math/tex; mode=display">\rho\left(x_{n}, x\right) \rightarrow 0 \Rightarrow r\left(T x_{n}, T x_{0}\right) \rightarrow 0 \quad(n \rightarrow \infty)</script><p>则称映射$T$是连续的。</p><h3 id="压缩映射"><a href="#压缩映射" class="headerlink" title="压缩映射"></a>压缩映射</h3><p>设$T:(\mathscr{X}, \rho) \rightarrow(\mathscr{X}, \rho)$ 是一个到自身的映射。如果存在$0&lt;\alpha&lt;1$，使得对于任意的$x,y \in \mathscr{X}$，都有：</p><script type="math/tex; mode=display">\rho(T x, T y) \leqslant \alpha \rho(x, y)</script><p>则称映射$T$为一个压缩映射。</p><h3 id="Banach不动点定理（压缩映射原理）"><a href="#Banach不动点定理（压缩映射原理）" class="headerlink" title="Banach不动点定理（压缩映射原理）"></a>Banach不动点定理（压缩映射原理）</h3><p>假设$(\mathscr{X}, \rho)$是一个完备的距离空间，$T$是这个距离空间上到自身的一个压缩映射，则在该空间上存在唯一的不动点$x^<em>$，使得$Tx^</em>=x^*$.</p><h1 id="线性赋范空间"><a href="#线性赋范空间" class="headerlink" title="线性赋范空间"></a>线性赋范空间</h1><p>距离空间中只有拓扑结构，但是在分析函数空间的时候还需要考虑元素之间的代数运算。</p><h2 id="线性空间"><a href="#线性空间" class="headerlink" title="线性空间"></a>线性空间</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>设$\mathscr{X}$为一个非空集，$\mathbb{K}$为实（复）数域，如果下列条件满足，则称$\mathscr{X}$为实（复）线性空间。</p><ul><li>$\mathscr{X}$是一个加法交换群，也即：<ul><li>$x+y=y+x$ （加法交换律）</li><li>$(x+y)+z = x+(y+z)$ （加法结合律）</li><li>存在唯一的$\theta \in \mathscr{X}$，使得$\forall x \in \mathscr{X}, x+\theta=\theta+x$ （唯一零元）</li><li>对$\forall x \in \mathscr{X}$, 存在唯一的$ x^{\prime} \in \mathscr{X}$，使得$x+x^\prime=\theta$，记为$-x$（唯一负元）</li></ul></li><li>定义了数域$\mathbb{K}$中的数$a$和$x\in \mathscr{X}$的数乘运算，满足以下条件：<ul><li>$a(\beta x)=(a \beta) x $ （数乘的结合律）</li><li>$1 \cdot x=x$ （乘法零元）</li><li>$(a+\beta) x=a x+\beta x$ （分配率）</li><li>$a(x+y)=a x+a y$ （分配率）</li></ul></li></ul><h3 id="线性空间上的距离"><a href="#线性空间上的距离" class="headerlink" title="线性空间上的距离"></a>线性空间上的距离</h3><p>线性空间上的距离定义一共需要满足两个关键条件：</p><p><strong>一、平移的不变性：</strong></p><script type="math/tex; mode=display">\rho(x+z, y+z)=\rho(x, y) \quad(\forall x, y, z \in \mathscr{X})</script><p>而通过平移的不变性能够推出距离函数是对加法连续的，即：</p><script type="math/tex; mode=display">\left.\begin{array}{l}{\rho\left(x_{n}, x\right) \rightarrow 0} \\ {\rho\left(y_{n}, y\right) \rightarrow 0}\end{array}\right\} \Rightarrow \rho\left(x_{n}+y_{n}, x+y\right) \rightarrow 0</script><p>证明步骤只需要根据平移不变形的定义就可，在此省略。事实上，这两者是相互等价的。</p><p><strong>二、数乘的连续性：</strong></p><p>数乘连续性定义为以下：</p><ul><li>$\rho\left(x_{\mathfrak{n}}, x\right) \rightarrow 0 \Rightarrow \rho\left(a x_{\mathfrak{n}}, a x\right) \rightarrow 0 \quad(\forall a \in \mathbb{K})$</li><li>$a_{n} \rightarrow a(\mathbb{K}) \Rightarrow \rho\left(a_{n} x, a x\right) \rightarrow 0 \quad(\forall x \in \mathscr{X})$</li></ul><h3 id="线性空间上的准范数（模）"><a href="#线性空间上的准范数（模）" class="headerlink" title="线性空间上的准范数（模）"></a>线性空间上的准范数（模）</h3><p>在距离的基础上定义一个单值函数$p$：</p><script type="math/tex; mode=display">p: \mathscr{X} \rightarrow \mathbb{R}^{1}, \quad p(x) \triangleq \rho(x, \theta) \quad(\forall x \in \mathscr{X})</script><p>通过距离函数本身的3个性质，我们能够得出这个单值函数$p$的性质（略过）。而事实上，这个单值函数是我们刻画一个点大小的量，也就是准范数。</p><p>线性空间$\mathscr{X}$上的准范数定义为该空间上的的一个函数$|\cdot|: \mathscr{X} \rightarrow \mathbb{R}^{1}$，满足以下条件：</p><ul><li>$|x| \geqslant 0 \quad(\forall x \in \mathscr{X});|x|=0 \Leftrightarrow x=\theta$ （正定性）</li><li>$| x+y|\leqslant| x|+| y | \quad(\forall x, y \in \mathscr{X})$ （三角不等式）</li><li>$|-x|=|x| \quad(\forall x \in \mathscr{X})$ （逆元范数相等）</li><li>$\lim _{a_n \rightarrow 0}\left|a_{n} x\right|=0, \lim _{|x_n| \rightarrow 0}\left|a x_{n}\right|=0(\forall x \in \mathscr{X}, \quad \forall a \in \mathbb{R})$ （对数乘连续）</li></ul><p>如果在赋准范数空间$\mathscr{X}$上用准范数来定义收敛，即：</p><script type="math/tex; mode=display">\|x_n-x\|\rightarrow 0 \triangleq x_n \rightarrow x</script><p>那么这个空间就称作<strong>${F^*}$空间</strong>；如果这个空间同时又是完备的，那么这个空间就称作<strong>$F$空间</strong>。</p><h3 id="Banach空间与范数"><a href="#Banach空间与范数" class="headerlink" title="Banach空间与范数"></a>Banach空间与范数</h3><p>在准范数的基础上增加齐次性条件，准范数便成为了范数。其中齐次性条件指的是：</p><script type="math/tex; mode=display">\|a x\|=|a|\|x\| \quad(\forall a \in \mathbb{K}, \quad \forall x \in \mathscr{X})</script><p>当赋准范数的线性空间中的准范数是范数时，这个线性空间称为$B^<em>$空间；完备的$B^</em>$空间称作$B$空间，也就是Banach空间。我们也叫该空间为线性赋范空间。</p><h3 id="线性赋范空间上的模等价"><a href="#线性赋范空间上的模等价" class="headerlink" title="线性赋范空间上的模等价"></a>线性赋范空间上的模等价</h3><p>和开头所介绍的一样，引入距离和范数是为了研究一种收敛性。因此我们可以认为导致同一种收敛性的不同范数是等价的。更加准确地说：</p><p>假设在线性空间$\mathscr{X}$上定义了两个范数$|\cdot|_1$以及$|\cdot|_2$，如果在$n\rightarrow \infty$时，$\left|x_{n}\right|_{2} \rightarrow 0 \Rightarrow\left|x_{n}\right|_{1} \rightarrow 0$，则我们认为$|\cdot|_1$比$|\cdot|_2$强。而如果两个范数互相都比对方强，则我们认为两个范数等价。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;度量空间&quot;&gt;&lt;a href=&quot;#度量空间&quot; class=&quot;headerlink&quot; title=&quot;度量空间&quot;&gt;&lt;/a&gt;度量空间&lt;/h1&gt;&lt;p&gt;度量空间也叫做距离空间（metric space）。这是一种拓扑空间，其上的拓扑由指定的距离决定。在一个空间中引入距离是为了
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
  </entry>
  
  <entry>
    <title>RL(4) - Value Function Approximation</title>
    <link href="http://www.shihaizhou.com/2019/09/04/RL-Course-Lesson-4-Value-Function-Approximation/"/>
    <id>http://www.shihaizhou.com/2019/09/04/RL-Course-Lesson-4-Value-Function-Approximation/</id>
    <published>2019-09-04T08:21:23.000Z</published>
    <updated>2020-02-18T12:22:20.624Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>之前使用的State Value或者Action-state Value均采用一一对应的方式，也就是每个状态动作对都会对应一个价值。但是随着问题的复杂化，例如当动作空间变得更多，状态集合变得更大，维持着一张Q table就不再是能够接受的事情。所以在这样的背景下，我们提出了(state-aciton/state) value function，也就是维护一组approximation weight来对当前agent所处状态和所采取动作的价值进行估计。用数学语言进行表示分别为$\widehat{V}(s ; w)$ 和 $\hat{Q}(s, a ; w)$。 使用Approximation Function的好处主要为：减少了存储空间，减少了计算量，减少了需要sample的数据（因为Approximation意味着Generalization）。</p><p>常见的function approximator有以以下：</p><ul><li>Linear combinations of features</li><li>Neural networks</li><li>Decision treees</li><li>Nearest neighbors</li><li>Fourier / wavelet bases</li></ul><h1 id="Value-Function-Approximation"><a href="#Value-Function-Approximation" class="headerlink" title="Value Function Approximation"></a>Value Function Approximation</h1><h2 id="With-an-Oracle"><a href="#With-an-Oracle" class="headerlink" title="With an Oracle"></a>With an Oracle</h2><p>假设我们现在能够通过一个oracle访问到state value的真实值$V^{\pi}(s)$ ，那么我们对这个真实值的估计为 $\hat{V}(s ; \boldsymbol{w})$ ，对应的loss function为：</p><script type="math/tex; mode=display">J(\boldsymbol{w})=\mathbb{E}_{\pi}\left[\left(V^{\pi}(s)-\hat{V}(s ; \boldsymbol{w})\right)^{2}\right]</script><p>那么对应的为梯度下降值为：</p><script type="math/tex; mode=display">\Delta w=-\frac{1}{2} \alpha \nabla_{w} J(w)</script><p>利用该公式不断地去寻找local optimal即可。</p><h2 id="Model-free-VFA-Policy-Evaluation"><a href="#Model-free-VFA-Policy-Evaluation" class="headerlink" title="Model-free VFA Policy Evaluation"></a>Model-free VFA Policy Evaluation</h2><p>当我们没有对于oracle的知识时，与之前的Model-free policy evaluation类似，为了评估当前的policy，我们需要固定住这个policy然后不断地取出sample值来估计每个state value或者state-action value。过程中我们采用的是维护一张表，但现在整个VFA的核心观点就是将这张表用一个拟合函数来代替，所以和之前有Oracle的情况类似，我们将每个sample出来的点都看作是正常值，sample的点的EV就是这个Oracle，不过会给我们的训练带来一些扰动。</p><p>我们使用一个feature vector来代表一个state $s$ 所具有的特征，那么这就是我们VFA的输入：</p><script type="math/tex; mode=display">x(s)=\left(\begin{array}{c}{x_{1}(s)} \\ {x_{2}(s)} \\ {\ldots} \\ {x_{n}(s)}\end{array}\right)</script><p>为了方便起见，我们使用一个线性的拟合函数来继续下面的讨论，拟合函数表示为：</p><script type="math/tex; mode=display">\hat{V}(s ; w)=\sum_{j=1}^{n} x_{j}(s) w_{j}=x(s)^{T} w</script><p>同样的，损失函数的表示和上一节相同：</p><script type="math/tex; mode=display">J(w)=\mathbb{E}_{\pi}\left[\left(V^{\pi}(s)-\hat{V}(s ; w)\right)^{2}\right]</script><p>在MC方法中，return value是Expected Return $V^{\pi}\left(s_{t}\right)$ 无偏但是有噪声的sample，因此我们可以将MC VFA降级到一个(state, return) pair上的监督学习：</p><script type="math/tex; mode=display">\begin{aligned} \Delta \boldsymbol{w} &=\alpha\left({G}_{t}-\hat{V}\left(s_{t} ; \boldsymbol{w}\right)\right) \nabla_{w} \hat{V}\left(s_{t} ; \boldsymbol{w}\right) \\ &=\alpha\left(G_{t}-\hat{V}\left(s_{t} ; \boldsymbol{w}\right)\right) {x}\left(s_{t}\right) \\ &=\alpha\left(G_{t}-{x}\left(s_{t}\right)^{T} \boldsymbol{w}\right) {x}\left(s_{t}\right) \end{aligned}</script><h2 id="Convergence-Guarantees-for-Linear-VFA-for-Policy-Evaluation"><a href="#Convergence-Guarantees-for-Linear-VFA-for-Policy-Evaluation" class="headerlink" title="Convergence Guarantees for Linear VFA for Policy Evaluation"></a>Convergence Guarantees for Linear VFA for Policy Evaluation</h2><p>一个MDP当其中的Policy固定之后就是一个对应的Markov Chain，而最终会converge到一个状态分布 $d(s)$，这分布 $d(s)$ 被称作stationary distribution over states of $\pi$. 关于这个分布有以下两个性质，第一个表示所有状态的分部之和为1；第二个为一个balance equation：</p><script type="math/tex; mode=display">\sum_{s} d(s)=1 \\d\left(s^{\prime}\right)=\sum_{s} \sum_{a} \pi(a | s) p\left(s^{\prime} | s, a\right) d(s)</script><p>定义一个关于某个特定策略下的Linear VFA的MSE为以下（relative to the true value）：</p><script type="math/tex; mode=display">\operatorname{MSVE}(\boldsymbol{w})=\sum_{s \in S} d(s)\left(V^{\pi}(s)-\hat{V}^{\pi}(s ; \boldsymbol{w})\right)^{2}</script><p>Monte Carlo policy evaluation with VFA保证拟合到最小的MSE的状态：</p><script type="math/tex; mode=display">\operatorname{MSVE}\left(\boldsymbol{w}_{M C}\right)=\min _{\boldsymbol{w}} \sum_{s \in S} d(s)\left(V^{\pi}(s)-\hat{V}^{\pi}(s ; \boldsymbol{w})\right)^{2}</script><p>如果是Linear的拟合函数，并且有a batch of samples，则我们能够算出weight的解析解，这里我们具体不做展开，放一张课程截图，方法就是求目标函数的导数并让其为0。</p><p><img src="https://i.loli.net/2019/09/04/St1JrYHn4eBFyli.png" alt="image.png"></p><h3 id="TD-Learning-with-VFA"><a href="#TD-Learning-with-VFA" class="headerlink" title="TD Learning with VFA"></a>TD Learning with VFA</h3><p>在TD方法中，同MC方法一样，我们将一系列的sample和特征向量降级成监督学习，这里的采用immediate reward + discounted value，是一个利用了马尔科夫性质的return的近似，从而是一个有偏估计：</p><script type="math/tex; mode=display">\left\langle s_{1}, r_{1}+\gamma \hat{V}^{\pi}\left(s_{2} ; w\right)\right\rangle,\left\langle s_{2}, r_{2}+\gamma \hat{V}^{\pi}\left(s_{3} ; w\right)\right\rangle, \dots</script><p>在Linear TD(0)中，weights的增量为：</p><script type="math/tex; mode=display">\begin{aligned} \Delta \boldsymbol{w} &=\alpha\left(r+\gamma \hat{V}^{\pi}\left(s^{\prime} ; \boldsymbol{w}\right)-\hat{V}^{\pi}(s ; \boldsymbol{w})\right) \nabla_{w} \hat{V}^{\pi}(s ; \boldsymbol{w}) \\ &=\alpha\left(r+\gamma \hat{V}^{\pi}\left(s^{\prime} ; \boldsymbol{w}\right)-\hat{V}^{\pi}(s ; \boldsymbol{w})\right) \boldsymbol{x}(s) \\ &=\alpha\left(r+\gamma \boldsymbol{x}\left(s^{\prime}\right)^{T} \boldsymbol{w}-\boldsymbol{x}(s)^{T} \boldsymbol{w}\right) \boldsymbol{x}(s) \end{aligned}</script><p>TD(0)方法保证拟合到一个constant factor下的最小MSE状态：</p><script type="math/tex; mode=display">\operatorname{MSVE}\left(\boldsymbol{w}_{T D}\right) \leq \frac{1}{1-\gamma} \min _{\boldsymbol{w}} \sum_{s \in S} d(s)\left(V^{\pi}(s)-\hat{V}^{\pi}(s ; \boldsymbol{w})\right)^{2}</script><p>通常实践情况下TD方法的拟合会更加快。</p><h1 id="Control-using-VFA"><a href="#Control-using-VFA" class="headerlink" title="Control using VFA"></a>Control using VFA</h1><p>现在采用value function approximation来表示state-action values $\hat{Q}^{\pi}(s, a ; \boldsymbol{w}) \approx Q^{\pi}$，同样的我们的目标是用这个方程去尝试拟合Oracle的Q值。使用features来表示state和action的组合：</p><script type="math/tex; mode=display">x(s, a)=\left(\begin{array}{c}{x_{1}(s, a)} \\ {x_{2}(s, a)} \\ {\dots} \\ {x_{n}(s, a)}\end{array}\right)</script><p>同样简化情况，使用Linear combination of features来表示state-action value function：</p><script type="math/tex; mode=display">\hat{Q}(s, a ; \boldsymbol{w})=\boldsymbol{x}(s, a)^{T} \boldsymbol{w}=\sum_{j=1}^{n} x_{j}(s, a) w_{j}</script><p>使用SGD来做weights的更新：</p><script type="math/tex; mode=display">\nabla_{\boldsymbol{w}} J(\boldsymbol{w})=\nabla_{\boldsymbol{w}} \mathbb{E}_{\pi}\left[\left(Q^{\pi}(s, a)-\hat{Q}^{\pi}(s, a ; \boldsymbol{w})\right)^{2}\right]</script><p>对于Model-free的情况来讲，因为没有办法直接访问到某个state的true value function，所以我们需要采用一个target value作为替代品。</p><p>在MC方法中，我们采用return $G_t$ 作为target value：</p><script type="math/tex; mode=display">\Delta \boldsymbol{w}=\alpha\left(G_{t}-\hat{Q}\left(s_{t}, a_{t} ; \boldsymbol{w}\right)\right) \nabla_{w} \hat{Q}\left(s_{t}, a_{t} ; \boldsymbol{w}\right)</script><p>在SRASA方法中，我们采用TD target $r+\gamma \hat{Q}\left(s^{\prime}, a^{\prime} ; w\right)$，这条式子利用了当前的已经经过近似的值：</p><script type="math/tex; mode=display">\Delta \boldsymbol{w}=\alpha\left(r+\gamma \hat{Q}\left(s^{\prime}, a^{\prime} ; \boldsymbol{w}\right)-\hat{Q}(s, a ; \boldsymbol{w})\right) \nabla_{\boldsymbol{w}} \hat{Q}(s, a ; \boldsymbol{w})</script><p>在Q-Learning方法中，我们采用TD target $r+\gamma \max _{a^{\prime}} \hat{Q}\left(s^{\prime}, a^{\prime} ; w\right)$ ，这条式子利用了max of 当前经过近似的值：</p><script type="math/tex; mode=display">\Delta \boldsymbol{w}=\alpha\left(r+\gamma \max _{a^{\prime}} \hat{Q}\left(s^{\prime}, a^{\prime} ; \boldsymbol{w}\right)-\hat{Q}(s, a ; \boldsymbol{w})\right) \nabla_{\boldsymbol{w}} \hat{Q}(s, a ; \boldsymbol{w})</script><p>需要注意的是，TD方法中我们没有采用gradient of an objective function。关于之前VFA的Convergence的总结如下图所示，相比于算法是否拟合，更为重要的是算法最终达到的点表现如何。</p><p><img src="https://i.loli.net/2019/09/04/htpZ1JS6vzo5ueK.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;p&gt;之前使用的State Value或者Action-state
      
    
    </summary>
    
    
      <category term="RL" scheme="http://www.shihaizhou.com/tags/RL/"/>
    
  </entry>
  
  <entry>
    <title>RL(3) - Model-Free Control</title>
    <link href="http://www.shihaizhou.com/2019/08/15/RL-Course-Lesson-3-Model-Free-Control/"/>
    <id>http://www.shihaizhou.com/2019/08/15/RL-Course-Lesson-3-Model-Free-Control/</id>
    <published>2019-08-15T07:50:58.000Z</published>
    <updated>2020-02-18T12:22:13.988Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h1><p>上一节主要内容是在没有Model的情况下如何去做Policy Evaluation，本章的重点是要没有Model的情况下去学会做决策，下一章将会介绍Value Function Approximation。学会优秀的策略需要应对以下两个问题，首先是Delayed Consequences，也就是说当前的决策可能需要在未来好几步才能够反映出好坏和价。其次就是要做到适当的Exploration，因为需要进行适当量的探索才能够发现较为优秀的策略。</p><p>对于许多的MDP模型来说，以下两种问题需要我们使用Model-free Control去解决：</p><ul><li>MDP model is unknown but can be sampled.</li><li>MDP model is known but it is computationally infeasible to use directly, except through sampling.</li></ul><p>On-policy learning 和 Off-policy learning是两种学习的方法，前者在和环境的交互中直接进行学习，也就是当前的policy和当前的sample是对应的；而后者能够从另一个不同的policy中去进行Evaluation和参数的更新。</p><h1 id="Generalized-Policy-Iteration"><a href="#Generalized-Policy-Iteration" class="headerlink" title="Generalized Policy Iteration"></a>Generalized Policy Iteration</h1><p>首先回顾一下PI的流程（此时的PI是有真正Model的）：初始化一个policy$\pi$；重复以下两步操作，首先进行Policy Evaluation，然后利用$\arg \max_{a} Q(s,a)$对当前policy进行更新（$Q$值计算用到了$P$）；直到拟合。在Model-free的情况下PI的流程也是一样的：重复Policy Evaluation + Policy Improvement。只是Policy Evaluation现在需要进行estimate Q来得到。</p><p><strong>MC for On-policy Q Evalution</strong></p><ul><li>Initialize $N(s,a)=0, G(s,a)=0, Q^\pi(s,a)=0, \forall s \in S, \forall a \in A$</li><li>Loop<ul><li>Using policy $\pi$ sample episode $i=s_{i,1}, a_{i,1},r_{i,1},s_{i,2}, a_{i,2},r_{i,2}, \dots, s_{i,T_i}$</li><li>$G_{i,t}=r_{i,t}+\gamma r_{i,t+1}+ \gamma^2 r_{i,t+2}+ \dots + \gamma^{T_i-1}r_{i,T_i}$</li><li>Foreach (state,action) $(s,a)$ visited in episode $i$<ul><li>For <strong>first or every</strong> time $t$ that $(s,a)$ is visited in episode $i$<ul><li>$N(s,a)=N(s,a)+1, G(s,a)=G(s,a)+G_{i,t}$</li><li>Update estimate $Q^\pi(s,a)=G(s,a)/N(s,a)$ </li></ul></li></ul></li></ul></li></ul><p>在此estimated Q value的基础上，我们只需要更新policy为：</p><script type="math/tex; mode=display">\pi_{i+1}(s)=\arg\max_a Q^{\pi_i}(s，a)</script><p>这样的estimation可能会有一些问题，例如如果$\pi$是deterministic的或者对某些action的probability为0，那么我们就没有办法计算$Q(s,a)$ for any $a \neq \pi(s)$，这是由于我们在sample时不会采样到其余的action。并且我们希望对于Q值的estimation和policy improvement能够相互穿插，而不是现在的大量的estimation的sample和更新得到一次policy的参数更新。这些内容会在下文中cover。为了平衡exploration和exploitation，一个简单的想法就是使用$\epsilon$-greedy方法。</p><script type="math/tex; mode=display">\pi(a|s) = \begin{cases} \arg\max_a Q^\pi(s,a), \text{with prob} (1-\epsilon) \\a, \text{with prob} ({ {\epsilon}\over{|A|} })\end{cases}</script><p>这里可以证明：For any $\epsilon$-greedy policy $\pi_i$, the $\epsilon$-greedy policy w.r.t $Q^{\pi_i}$, $\pi_{i+1}$ is a monotonic improvement $V^{\pi_{i+1}} \geq V^{\pi_i}$ .证明过程略去，给出笔记中的截图。</p><p><img src="https://i.loli.net/2020/02/12/78XCIdeNPOgvo1Z.png" alt="image.png"></p><p><strong>Greedy in the Limit of Infinite Exploration (GLIE)</strong></p><p>$\epsilon$-greedy策略是能够让每次策略更新依旧保持单调增长的一种方式。而我们可以对这个策略进行适当的refine从而让这种更新策略保证收敛。GLIE的定义由以下两个条件组成：</p><p>首先，所有的state-action对被访问的次数在sample次数的数量趋向于无穷时也应当趋向于无穷：</p><script type="math/tex; mode=display">\lim_{i \to \infty} N_i(s,a) \to \infty</script><p>第二，当sample数趋于无穷的时候，policy选择Q表中最优值的概率应当趋于1.</p><script type="math/tex; mode=display">\lim_{i \to \infty} \pi(a|s) \to \arg\max_a Q(s,a)</script><p>一个满足GLIE条件的例子就是让$\epsilon$-greedy探索的随机性系数$\epsilon$随着sample的样本改变：$\epsilon_i = 1/i$. 同时GLIE Monte-Carlo Control能够确保收敛到Optimal的state-action value function：$Q(s,a)\to Q^*(s,a)$.</p><h1 id="Monte-Carlo-Online-Control"><a href="#Monte-Carlo-Online-Control" class="headerlink" title="Monte Carlo Online Control"></a>Monte Carlo Online Control</h1><p>结合上面的内容，在线的MC Control算法可以用以下的伪代码表示：</p><p><img src="https://i.loli.net/2020/02/12/YsSFuAb3Q5cOMjx.png" alt="image.png"></p><h1 id="Temporal-Difference-Methods-for-Control"><a href="#Temporal-Difference-Methods-for-Control" class="headerlink" title="Temporal Difference Methods for Control"></a>Temporal Difference Methods for Control</h1><p>通常情况下使用TD方法进行Model-free PI的框架可以被归纳为：随机初始化策略$\pi$；重复更新策略（和使用MC方法几乎相同）Policy Evaluation + Policy Improvement。 </p><h2 id="SARSA"><a href="#SARSA" class="headerlink" title="SARSA"></a>SARSA</h2><p>SARSA算法的伪代码如下图所示，和MC方法的核心变化在于TD方法不需要等一个episode完结才能够进行更新，而是根据下一个step的观察可以直接进行更新，其依据就是利用Bellman方程的estimation：</p><script type="math/tex; mode=display">\begin{align}V^{\pi}(s)&=V^{\pi}(s)+\alpha\left[G_{i, t}-V^{\pi}(s)\right] \\V^{\pi}(s)&=V^{\pi}(s)+\alpha\left(\left[r_{t}+\gamma V^{\pi}\left(s_{t+1}\right)\right]-V^{\pi}(s)\right)\end{align}</script><p><img src="https://i.loli.net/2020/02/12/Icz5XLaUYQxBP8O.png" alt="image.png"></p><p>SARSA for finite-state &amp; finite-action MDPs 在以下两个条件成立时能够保证收敛到最优的action-value：</p><ul><li>策略序列$\pi_{t}(a | s)$满足上面所提及的GLIE条件</li><li>步长$\alpha_t$满足Robbins-Munro序列要求：<ul><li>$\sum_{t=1}^{\infty} \alpha_{t}=\infty$ </li><li>$\sum_{t=1}^{\infty} \alpha_{t}^{2}&lt;\infty$</li></ul></li></ul><p>在实践中我们的步长一般不遵从这里的限制，通常使用的是逐级递减的步长就可以。这里提到了SARSA采用的更新策略是实际sample出的下一个step的情况，这种方式对比Q-Learning来说是“相对客观”的。Q-Learning会过分乐观（因为更新策略中拿到的是max value），但是SARSA算法在Sutton and Barto的cliff walk example中表现会更好。关于这个问题，我们会在这章最后简单用代码实现一下。但是很多的问题都不会具有这样的性质。</p><h2 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h2><p>Q-Learning也是一个Temporal Difference的方法，和SARSA相比，Q-Leaning不需要从policy当中sample出确切的action，而是更新时采用当前时间步最大期望的action-state值进行更新，因此Q-Learning也被认为是一种off-policy的算法。虽然理论上Q-Learning能够确保收敛到和SARSA一样的最优值，但初始化的状态也依旧会产生一些影响。总体来说，optimistic initialization会比较有帮助。以下是两种算法更新函数的比较。</p><ul><li>SARSA：$Q\left(s_{t}, a_{t}\right) \leftarrow Q\left(s_{t}, a_{t}\right)+\alpha\left(\left(r_{t}+\gamma Q\left(s_{t+1}, a_{t+1}\right)\right)-Q\left(s_{t}, a_{t}\right)\right)$</li><li>Q-Learning：$Q\left(s_{t}, a_{t}\right) \leftarrow Q\left(s_{t}, a_{t}\right)+\alpha\left(\left(r_{t}+\gamma \max _{a^{\prime}} Q\left(s_{t+1}, a^{\prime}\right)\right)-Q\left(s_{t}, a_{t}\right)\right)$</li></ul><p><img src="https://i.loli.net/2020/02/12/RXLWOgA1ivYUrTx.png" alt="image.png"></p><h1 id="Maximization-Bias"><a href="#Maximization-Bias" class="headerlink" title="Maximization Bias"></a>Maximization Bias</h1><p>课程笔记中举了一个扔硬币的例子：</p><p><img src="https://i.loli.net/2020/02/12/JSmkK34ByURY2MZ.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/02/12/MuXIcNPvUzsmO2g.png" alt="image.png"></p><p>为什么会导致这样的偏差？原因在于我们将estimate的过程和control的过程放在了一起，导致了对于结果过于乐观的估计。在扔硬币的例子中为了回答第二个问题我们其实完全可以重新estimate选择出来的硬币，这样就可以避免了这样的bias。那么通过这个经验，我们就可以将estimation和control的过程进行分离从而保证估计的无偏性。</p><p>关于这个问题的更加数学化的例子：</p><p><img src="https://i.loli.net/2020/02/12/quaWcGDZIA53rpO.png" alt="image.png"></p><p>为了解决这个问题，研究者找到了Double Q-Learning算法。简单来说就是采用一个Q-function来进行estimation，另一个Q-function进行action sampling。以下为Double Q-Learning的伪代码：</p><p><img src="https://i.loli.net/2020/02/12/soWFqnbldNgHCfa.png" alt="image.png"></p><p>这种方法使用了independent samples to estimate the value。而在现实中使用该算法时，可以以0.5的概率对这两个Q function进行交替的estimate和sample。Double Q-Learning只增加了内存开销，不会产生更大的计算开销，而且能够达到非常好的效果。以下的实验结果告诉我们Q-Learning在前期非常可能suffer在不是最优的策略中，但Double Q-Learning却能够很好地解决这个问题。</p><p><img src="https://i.loli.net/2019/09/03/GrDQdLpaAq4KFTR.png" alt="image.png"></p><p><a href="https://medium.com/@ameetsd97/deep-double-q-learning-why-you-should-use-it-bedf660d5295" target="_blank" rel="noopener">这篇博客</a>对这个问题进行了一些阐述，最主要的观点是Q-Learning中的max函数导致了过于乐观的估计。如果Reward带上了随机性（例如遵从一个EV为负的分布），那么sample前期的正向结果都会导致模型得到错误的估计，估计导致策略向这个错误的方向偏移，会使得模型在未来的一段时间内都得出不好的结果。</p><blockquote><p>What is happening here is that, if an action’s value was overestimated, it will be chosen as the best action, and it’s overestimated value is used as the target. Instead, we can have two different action-value estimates as mentioned in the above equation, and the “best” action can be chosen based on the values of the first action-value estimate and the target can be decided by the second action-value estimate. How do we train though?</p><p>Divide the samples into two halves. For the first half, we can use the above equation to train, and for the second half swap the “1” with the “2” in the equation. In practice, data keeps coming into the agent, and thus a coin is flipped to decide if the above equation should be used or the swapped one. How does this help?</p><p>Since the samples are stochastic, it is less likely that both the halves of the samples are overestimating the <em>same</em> action. The above method separates two crucial parts of the algorithm, choosing the best action and using the estimate of that action, and hence works well.</p></blockquote><p>虽然我们仅仅是在非常简单的场景当中阐述了可能产生maximization bias的可能情况，也有研究者指出在Atari这样复杂的游戏中也会产生这样的问题。而同时，如果所有的action-values都受到了overestimation的影响，那么对于最终学习到的策略不会有任何影响；相反，如果不同的state所受到的影响不同（例如有些state的reward是恒定的），那么对最终的optimal policy会产生影响。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Preface&quot;&gt;&lt;a href=&quot;#Preface&quot; class=&quot;headerlink&quot; title=&quot;Preface&quot;&gt;&lt;/a&gt;Preface&lt;/h1&gt;&lt;p&gt;上一节主要内容是在没有Model的情况下如何去做Policy Evaluation，本章的重点是要没
      
    
    </summary>
    
    
      <category term="RL" scheme="http://www.shihaizhou.com/tags/RL/"/>
    
  </entry>
  
  <entry>
    <title>Emergent Communication Through Negotiation</title>
    <link href="http://www.shihaizhou.com/2019/08/13/Notes-on-Emergent-Communication-Through-Negotiation/"/>
    <id>http://www.shihaizhou.com/2019/08/13/Notes-on-Emergent-Communication-Through-Negotiation/</id>
    <published>2019-08-13T07:06:11.580Z</published>
    <updated>2020-02-18T12:33:49.790Z</updated>
    
    <content type="html"><![CDATA[<p>原文地址：<a href="https://arxiv.org/abs/1804.03980" target="_blank" rel="noopener">Emergent Communication Through Negotiation</a></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>这篇工作利用MARL (Multi-Agent Reinforcement Learning)来研究在谈判协商中的语言生成问题。文章设置了两种交流的protocol，第一种是和游戏本身内容高度相关的proposal，第二种是用于交换潜在信息的cheap talk。文章实验发现对于self-interested类型的agent来说，谈判过程不会导致语言的生成；而对于prosocial类型的agent来说，谈判过程生成了语言。文章的结论验证了“Cooperation is necessary for language to emerge”，即合作是语言生成的必须条件。</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>文章的动机在于探索交流 (Communication)是如何产生的。“Given these basic requirements, an interesting question to ask is what task structures aid the emergence of communication and how different communication protocols affect task success.” 在这篇文章之前有很多工作通过referential games的设置研究指代问题，本文指出人类语言并不简单是一个指代工具，而且人类很多情况下的沟通是非完全合作性质的，因此本文研究的是在这些非完全合作情境中语言的出现。文章中关于指代问题的文章链接贴在下面，未来可以研究一下。</p><ul><li><a href="http://rbr.cs.umass.edu/shlomo/papers/GAZjaamas07.pdf" target="_blank" rel="noopener">Learning to communicate in a decentralized environment</a></li><li><a href="https://arxiv.org/abs/1612.07182" target="_blank" rel="noopener">Multi-agent cooperation and the emergence of (natural) language</a></li><li><a href="https://arxiv.org/abs/1705.10369" target="_blank" rel="noopener">Emergent language in a multi-modal, multi-step referential game</a></li></ul><p>文章采用了一个经典的谈判问题The bargaining problem来作为研究的环境。文章认为，有效的交流在该问题中是非常关键的，因为agent需要通过交流传达出自己的desire，以及通过交流推断对方的意图。原先的工作主要用offer/counter-offer这样的bargaining game形式来做，但是没有研究emergent communication这样的问题。而MARL在最近的发展为研究这类问题提供了非常好的方法。“By repeatedly interacting with other agents learning at the same time, agents can gradually bootstrap complex behaviour, including motor skills and linguistic communication.”关于motor skills和linguistic communication的引用已经列在下面。</p><ul><li><a href="https://arxiv.org/abs/1710.03748" target="_blank" rel="noopener">Emergent complexity via multi-agent competition</a></li><li><a href="https://arxiv.org/abs/1612.07182" target="_blank" rel="noopener">Multi-agent cooperation and the emergence of (natural) language</a></li><li><a href="https://arxiv.org/abs/1705.11192" target="_blank" rel="noopener">Learning to communicate with sequences of symbols.</a></li></ul><p>文章在bargaining问题设置了两个交流通道，第一种通道是proposal，第二种通道是cheap talks。在一对一的实验中，文章发现selfish agent可以通过proposal通道和对方agent达成相对合理的分割，但是selfish agent却没有办法利用cheap talks的通道；相反prosocial agent却通过cheap talks的通道生成了语言，为了更好地和对方沟通达成相同目标。第二组实验是针对一个社群的agent进行的，先前关于社群的工作有以下几个观察：</p><ul><li>cheap talk can have a significant effect on the evolutionary dynamics of the population. (<a href="http://www.dklevine.com/archive/refs4540.pdf" target="_blank" rel="noopener">Efficiency in evolutionary games: Darwin, nash and the secret handshake</a>)</li><li>cheap talk also influences the equilibria, stability, and basins of attractions. (<a href="http://www.socsci.uci.edu/~bskyrms/bio/papers/CheapTalk.pdf" target="_blank" rel="noopener">Signals, evolution and the explanatory power of transient information.</a>)</li><li>unless trainned in a diverse environment, agents overfit to their specific opponent or teammate. (<a href="https://pdfs.semanticscholar.org/fbe9/950202a7fcc756369a38cb1ef4b9b994ae88.pdf" target="_blank" rel="noopener">A unified game-theoretic approach to multiagent reinforcement learning.</a>)</li></ul><p>文章在这些观察上给出了自己的设置：一个agent去和不同等级的prosociality的agent进行交互的同时去identify and model other agents’ beliefs帮助谈判的成功。这一结果和基于Theory of Mind这篇文章的模型是相似的：boundedly rational agents can collectively benefit by making inferences about the sophistication levels and beliefs of their opponents (<a href="https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1000254&amp;type=printable" target="_blank" rel="noopener">Game theory of mind</a>).</p><p><img src="https://i.loli.net/2019/08/15/XFW1ktdYVN6BKgI.png" alt="Screen Shot 2019-08-15 at 4.58.26 PM.png"></p><h1 id="Game-Setting"><a href="#Game-Setting" class="headerlink" title="Game Setting"></a>Game Setting</h1><h2 id="Negotiation-Environment"><a href="#Negotiation-Environment" class="headerlink" title="Negotiation Environment"></a>Negotiation Environment</h2><p>基本设置有以下：</p><ul><li>三种item：[peppers, cherries, strawberries]</li><li>每种item在[0,5]的整数区间内随机初始化数量</li><li>相互隐藏的utility function对应不同的agent对不同item的喜好程度，分布在[0,10]的整数区间内</li><li>每一次通信包括了proposal和cheap talk</li><li>special action用于接受最近一次对方提出的proposal。如果接受的proposal为非法的（例如某个超出当前的最大数量）则双方同时获得0 reward</li><li>如果在最后一轮依旧没有协议达成，双方同时获得0 reward</li><li>最大协商轮数$N$不再是一个固定的自然数，而是一个在[4,10]整数范围内进行truncated Poisson distribution with mean 7的值。这么做是为了消除先手优势（先手方可以在最后一轮提出极大对自己有利的proposal）。这是参考了ultimatum game的问题进行的改进（<a href="http://www.dklevine.com/archive/refs4291.pdf" target="_blank" rel="noopener">An experimental analysis of ultimatum bargaining</a>）。</li></ul><p><em>但这里的问题就是：其实我们也可以给出一个根据概率分布的策略来实现先手对后手的优势，例如在第4轮前保持proposal不变，随着谈判向后进行再逐步严格proposal，因为在之后轮terminate的概率会变高，因此后手尽快接受proposal的意图也会更加明显。所以这里我个人认为只需要<strong>先后手依次进行</strong>就能够消除所谓的问题ultimatum bargaining的问题。</em></p><h2 id="Communication-Channels"><a href="#Communication-Channels" class="headerlink" title="Communication Channels"></a>Communication Channels</h2><p>Proposal channel就是一个提出的分割方案；Linguistic channel是本文对cheap talk这个沟通形式的一个具体方案，它在两个方面和Proposal channel不同：</p><ul><li>Non-bindingness: Messages sent via this channel do not commit the sender to any course of action, unlike directly transmitting a proposal which binds the sender to the proposal.</li><li>Unverifiability: There is no inherent link between the linguistic utterance and the private proposal made, meaning that the agents could potentially lie.</li></ul><p>文章关注的点在于：whether and under what circumstances the agents can make use of this channel to facilitate negotiation and establish a common ground for the symbols.</p><h2 id="Agent-Sociality-amp-Reward-Schemes"><a href="#Agent-Sociality-amp-Reward-Schemes" class="headerlink" title="Agent Sociality &amp; Reward Schemes"></a>Agent Sociality &amp; Reward Schemes</h2><p>agent的社会性本质上是由奖励方案反应的，在本文中，prosocial agent就是为了最大化双方的共同价值，而selfish agent就是单纯地最大化自己的价值。在预期里，prosocial agent是需要communication的通道的因为他们需要交流各自的utility function才能达到总体的最大化。</p><h2 id="Agent-Architecture-amp-Learning"><a href="#Agent-Architecture-amp-Learning" class="headerlink" title="Agent Architecture &amp; Learning"></a>Agent Architecture &amp; Learning</h2><p>每个时间步$t$, agent接受3个形式的input：</p><ul><li>item context $c^j=[i;u_j]$, 是item pool $i$ 和该agent的utility函数（也就是向量）</li><li>utterance $m_{t-1}$, 是对方上一个时间步给的cheap talk</li><li>proposal $p_{t-1}$, 是对方上一个时间步给的proposal</li></ul><p>关于agent参数的modeling并不是很重要，无非就是一些LSTM罢了。而每个agent需要独立地优化以下objective function：</p><script type="math/tex; mode=display">\pi^*_i=\arg\max_{\pi_i}E_{\tau\sim(\pi_A,\pi_B)}[R_i(\tau)]+\lambda H(\pi_i)</script><p>其中的$H(\pi_i)$是一个叫做entropy regularization term的东西，是为了鼓励agent在训练时去更多地explore，<em>这一块的内容需要进一步学习。</em></p><h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><p>前两个实验的内容（<strong>Can self-interested agents learn to negotiate</strong> &amp; <strong>Can prosocial agents learn to coordinate?</strong> ）并没有特别有趣的，重点放在后面两个实验。</p><h2 id="Analysis-of-Linguistic-Communication"><a href="#Analysis-of-Linguistic-Communication" class="headerlink" title="Analysis of Linguistic Communication"></a>Analysis of Linguistic Communication</h2><p>在self-interested agent和prosocial agent的协商实验中，通过分析symbol usage得出了以下结论：</p><ul><li>self-interested agent没有生成类似于自然语言一样的Zipfian distribution；而对于prosocial agent来说确生成了这样长尾效应的元组分布。</li><li>在这两方的协商中文章认为形成了一种类似于speaker-listener的交流方式，也就是说self-interested agent会倾听对方的给出的reward的信息，而不会泄露自己的utility。而prosocial会通过linguistic channel编码自己的信息告诉对方自己的utility从而达成合作。<em>我对这个分析的怀疑在于：理性来说泄露自己的utility给一个完全self-interested agent来说是不会有反馈的，因为你只会帮助对方榨取自己的价值。只有一种情况有帮助：你utility高的item在对方的utility为0，对方才有让出利益的动机。这一点也需要验证。</em></li></ul><p><img src="https://i.loli.net/2019/08/15/gxPS9k2wB1OEqfu.png" alt="Screen Shot 2019-08-15 at 4.57.19 PM.png"></p><p>接着文章验证了在linguistic channel中有存在关于自身utility的关键信息，通过一个LSTM去编码解码linguistic channel的内容来预测utility的信息。结果是self-interested无法用LSTM去做合理的预测，而prosocial确实是含有语义信息的。<em>但是疑问在于下图中的第二行为什么prosocial-prosocial的agent A也没有办法解码，文章没有给出解释。</em></p><p><em>同时另外一个值得研究的问题在于，self-interested agent如果在channel中学会了欺骗行为，那么是否意味着拿LSTM去预测正确的标签是不合理的？这是未来需要讨论的一点。</em></p><p><img src="https://i.loli.net/2019/08/15/M8WPVRaDTtrqkQG.png" alt="Screen Shot 2019-08-15 at 4.06.49 PM.png"></p><h2 id="A-Society-of-Agents"><a href="#A-Society-of-Agents" class="headerlink" title="A Society of Agents"></a>A Society of Agents</h2><p>更加符合现实的情况是有一个社群的agents，并且每一个agent的亲社会性是不同等级的，因此一个agent为了最大化自己的目标需要学会去identify不同agent的亲社会性，判断其是否擅于合作。文章设置了10个不同亲社会等级的agent，通过固定某个agent，维护一个opponent embedding来实现对对手亲社会性的利用。发现opponent的行为属性能够完全反映在embedding向量中，以下的PCA降维可视化能够反映出这一点。</p><p><img src="https://i.loli.net/2019/08/15/QpuwicqsP6ZY2FK.png" alt="Screen Shot 2019-08-15 at 4.26.37 PM.png"></p><p>关于在community中的语言学现象，有以下的总结：</p><ul><li>In one of our experimental settings, a community of prosocial agents developed a language and were able to use this to achieve better negotiation success.</li><li>Interestingly, the only situation when agents developed a language was when ID information was not provided.</li><li>When prosocial agents make use of the linguistic channel, the communication protocol differs within the community. 这意味着尽管agent A都是固定的，不同的agent B和agent A会使用不同的language。（<em>疑问在于有没有做 the same level of prosociality但不同agent的实验？随机性如何消除的？</em>）文章使用的方法是计算不同agent的Spearman correlation。</li></ul><h1 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h1><p>未来的有趣研究方向是Language能否在self-interested agent interacting中产生。Recent encouraging results by <a href="https://arxiv.org/abs/1703.06207" target="_blank" rel="noopener">Crandall et al. (2018)</a> show that communication can help agents cooperate. However, their signalling mechanism is heavily engineered: the speech acts are predefined and the consequences of the speech acts on the observed behaviour are deterministically hard-coded. It would be interesting to see whether a learning algorithm, such as <a href="https://arxiv.org/abs/1709.04326" target="_blank" rel="noopener">Foerster et al. (2017)</a>, can discover the same result.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;原文地址：&lt;a href=&quot;https://arxiv.org/abs/1804.03980&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Emergent Communication Through Negotiation&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=
      
    
    </summary>
    
    
      <category term="RL" scheme="http://www.shihaizhou.com/tags/RL/"/>
    
      <category term="NLP" scheme="http://www.shihaizhou.com/tags/NLP/"/>
    
      <category term="Negotiation" scheme="http://www.shihaizhou.com/tags/Negotiation/"/>
    
  </entry>
  
  <entry>
    <title>Google Football Warm Start</title>
    <link href="http://www.shihaizhou.com/2019/07/30/Google-Football-Warm-Start/"/>
    <id>http://www.shihaizhou.com/2019/07/30/Google-Football-Warm-Start/</id>
    <published>2019-07-30T03:42:26.732Z</published>
    <updated>2019-07-31T16:12:25.935Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h1><p>The analytical work on the <a href="https://github.com/google-research/football/" target="_blank" rel="noopener">google research football project</a> involved in this article is completed during the internship at Intel. Thanks again to <em>Qiyuan Gong &amp; Shengsheng Huang</em> for their kind guidance and mentoring, and also many thanks to those who helped me during this period of time. </p><p>About the project description, please refer to the previous post <a href="https://shihaizhou.com/2019/06/12/Google-Research-Football-RL-Environment/" target="_blank" rel="noopener">Google Research Football - RL Environment</a> on my personal website, which is the summary of the <a href="https://github.com/google-research/football/blob/master/paper.pdf" target="_blank" rel="noopener">Google Football paper</a>.</p><h1 id="Working-Environment-Setting"><a href="#Working-Environment-Setting" class="headerlink" title="Working Environment Setting"></a>Working Environment Setting</h1><p>Assuming that we have two local workstations <strong>W</strong> and <strong>L</strong> with Windows and linux installed respectively, one gateway server <strong>G</strong> and one remote server <strong>S</strong>, we now introduce some user-friendly developping tools before getting started. Note that the tools/softwares in this section are not necessary once you have alternatives. </p><p>For <strong>W</strong>: </p><ul><li><strong>Pycharm (professional):</strong> available once you have an edu email, which allows you to deploy, develop, debug remotely. <a href="https://www.jetbrains.com/pycharm/" target="_blank" rel="noopener">https://www.jetbrains.com/pycharm/</a></li><li><strong>MobaXTerm:</strong> powerful remote shell software. It can save your remote ssh session, without typing the password repeatedly. <a href="https://mobaxterm.mobatek.net/" target="_blank" rel="noopener">https://mobaxterm.mobatek.net/</a></li><li><strong>VNC viewer:</strong> will be used when there is need to run graphical programs on the remote server. <a href="https://www.realvnc.com/en/connect/download/viewer/" target="_blank" rel="noopener">https://www.realvnc.com/en/connect/download/viewer/</a></li></ul><p>For <strong>L</strong>:</p><ul><li><strong>oh my zsh:</strong> fancier version of the linux bash, multiple useful plugins for customizing your own bash experience. <a href="https://ohmyz.sh/" target="_blank" rel="noopener">https://ohmyz.sh/</a></li></ul><h2 id="SSH-Tunneling"><a href="#SSH-Tunneling" class="headerlink" title="SSH Tunneling"></a>SSH Tunneling</h2><p>If you are working in a big corporation like Intel, you will need to interact with the remote server through the gateway node, thus understanding SSH tunneling is a must. </p><p>SSH tunneling process is used for the packet forwarding. For example, when <strong>W</strong> cannot directly access the IP address of the remote server <strong>S</strong>, the SSH tunneling mechanisfm can create a process running on the gateway node <strong>G</strong> that listens to one port (on <strong>W</strong>) and forward any request to another port (on <strong>S</strong>). By doing so, we make <strong>W</strong> look like directly connected to the remote server <strong>S</strong>. </p><p>To start a SSH tunneling process, type in the following command on your local workstation:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -N -f -L &lt;port of L&gt;:&lt;IP of G&gt;:&lt;port of S&gt; &lt;username of S&gt;@&lt;IP of S&gt;</span><br></pre></td></tr></table></figure><p>in which the arguments represent different meanings:</p><ul><li><strong>-N:</strong> Do not execute a remote command.  This is useful for just forwarding ports.</li><li><strong>-f:</strong> Requests ssh to go to background just before command execution.</li><li><strong>-L:</strong> Specifies that connections to the given TCP port or Unix socket on the local (client) host are to be forwarded to the given host and port, or Unix socket, on the remote side.</li></ul><p>If you are working on <strong>W</strong>, then the MobaXTerm has a pretty good UI for you to set the tunneling configuration and you don’t need to use this command. The SSH tunneling exists everywhere if there is a requirement for connection through the gateway node. </p><p>For more information about SSH tunneling, please refer to the following websites/blogs. </p><ul><li><a href="http://www.ruanyifeng.com/blog/2011/12/ssh_port_forwarding.html" target="_blank" rel="noopener">SSH tunneling: how it works</a></li><li><a href="https://www.jianshu.com/p/8f262bc444f0" target="_blank" rel="noopener">PyCharm SSH tunneling</a> </li></ul><h1 id="Running-GFootball-Locally"><a href="#Running-GFootball-Locally" class="headerlink" title="Running GFootball Locally"></a>Running GFootball Locally</h1><p>Running Google Football locally doesn’t require any more techniques than mentioned in their <a href="https://github.com/google-research/football" target="_blank" rel="noopener">github repo</a>. First you need to prepare a Linux Environment (on your local workstation <strong>L</strong>). Then get the root authority and download the following packages by runninng ‘apt-get install’. </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install git cmake build-essential libgl1-mesa-dev libsdl2-dev libsdl2-image-dev libsdl2-ttf-dev libsdl2-gfx-dev libboost-all-dev libdirectfb-dev libst-dev mesa-utils xvfb x11vnc libsqlite3-dev glee-dev libsdl-sge-dev python3-pip</span><br></pre></td></tr></table></figure><p>The packages above are mainly for OpenGL rendering (including libgl1-mesa-dev and so on) and VNC remote rendering (including xvfb and x11vnc). There may be sometimes an error showing up indicating that some of the packages couldn’t be installed because the dependency tree can’t be resolved. When encounterd with that, I suggest that you choose another machine with clean environment and start the previous process all over again. </p><p>After the installation of the packages, we then install the google football by pip, to note here that the pip version matters (remember to downgrade it to 18.1), since the ‘parse-dependency-links’ option is not available for the higher version of pip. This installation issue lately has some update, for more information please refer to  <a href="https://github.com/google-research/football/issues/1" target="_blank" rel="noopener">this installation issue</a>. </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install pip==18.1</span><br><span class="line">pip install gfootball[tf_cpu]</span><br></pre></td></tr></table></figure><p>Finally run the simple example of the google football by playing the game yourself, in case that the OpenGL version is not compatible, I suggest adding some ENV parameter as follows. You can check <a href="https://github.com/google-research/football/issues/7" target="_blank" rel="noopener">this issue</a> for more information.  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MESA_GL_VERSION_OVERRIDE=3.2 MESA_GLSL_VERSION_OVERRIDE=150 python3 -m gfootball.play_game</span><br></pre></td></tr></table></figure><p>For the full doc of running google football, please visit the README file in the <a href="https://github.com/google-research/football" target="_blank" rel="noopener">official repo</a>. </p><h1 id="Running-GFootball-Remotely"><a href="#Running-GFootball-Remotely" class="headerlink" title="Running GFootball Remotely"></a>Running GFootball Remotely</h1><h2 id="X-Window-System"><a href="#X-Window-System" class="headerlink" title="X Window System"></a>X Window System</h2><p>When training the google football agent remotely on a server without rendering, there is nothing to worry about. But what if one wants to use the rendered pixels as the environment representation (observation), or wants to supervise the training process in a more intuitive way? Before diving into the solution/actual code to this problem, we need to understand how a Linux graphical system works, since it involves the rendering system. Please check <a href="http://en.wikipedia.org/wiki/X_Window_System" target="_blank" rel="noopener">X Window System</a> on wikipedia.</p><p>The X Window System (X11) is a windowing system for bitmap displays, commonly on Unix-like operating systems. X is of a typical <strong>C/S</strong> architecture, where there is a <strong>X server</strong> residing between the input/output hardware drives (keyboard, mouse, screen) and the user programs, which act as <strong>X clients</strong>. When a user program wants to display something on the screen, it has to pull up a request to the X server, and then the X server needs to find the frame buffer of the screen and execute the rendering commands sent from the client program, i.e., writing bits into the frame buffer. That’s all we need to know in this section. </p><p>When we are running google football on the workstation, the X server and the X client are both on the same machine. While when we try to run it remotely on the server <strong>S</strong> and want to see the rendering results on the local workstation <strong>W/L</strong>, the X server and the X client are sperate: X server on the local <strong>W/L</strong>, while the X client (google football) on the server <strong>S</strong>. </p><p>The all-important fact to know is that those packages needed by rendering should be installed where rendering happens, namely, where the X server resides. </p><h2 id="Local-Linux-System"><a href="#Local-Linux-System" class="headerlink" title="Local Linux System"></a>Local Linux System</h2><p>If the local workstation is installed with the Linux system, then we could use the <a href="https://www.jianshu.com/p/24663f3491fa" target="_blank" rel="noopener"><strong>x11 forwarding mechanism</strong></a>. Which means if you install all the packages on your local workstation, you will make it. </p><p>The x11 forwarding techniques are not tested in this article. Hopefully you will succeed. </p><h2 id="Local-Windows-System"><a href="#Local-Windows-System" class="headerlink" title="Local Windows System"></a>Local Windows System</h2><p><strong><em>Forget it! the MobaXTerm x11 forwarding mechanism doesn’t work!</em></strong> If you try to do so, you will receive a confusing error message which can’t be traced:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;Couldn&apos;t load GL function glBegin: Video subsystem has not been initialized.&quot;</span><br></pre></td></tr></table></figure><p>On Windows system, we could use some simulated X server tools (such as <a href="https://sourceforge.net/projects/xming/" target="_blank" rel="noopener">Xming</a>) to receive the rendering requests from the remote server, and the x11 feature provided by MobaXTerm follows this idea. But the fact that the google football requires so many packages that could only be installed on Linux system makes it impossible to use the x11 forwarding mechanism. </p><p>The basic idea to solve this problem is to put the X server back to the server <strong>S</strong>. Instead of letting the X server write the screen bits into the frame buffer, we use a portion of the hard drive space, pretending that it’s the frame buffer of the screen, and then forward the image to the local workstation <strong>W</strong> via network. </p><h3 id="1-Set-up-the-virtual-frame-buffer-amp-message-forwarding"><a href="#1-Set-up-the-virtual-frame-buffer-amp-message-forwarding" class="headerlink" title="1. Set up the virtual frame buffer &amp; message forwarding"></a>1. Set up the virtual frame buffer &amp; message forwarding</h3><p>First, login to the remote server and install the google football package (as shown in the previous section). </p><p>Then run the following two commands in the shell:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Xvfb :1 -screen 0 800x600x24 &amp;</span><br><span class="line">x11vnc -display :1 -allow &lt;IP of W&gt; -autoport 5900 &amp;</span><br></pre></td></tr></table></figure><p>The first command <code>Xvfb</code> stands for <a href="https://www.x.org/releases/X11R7.6/doc/man/man1/Xvfb.1.xhtml" target="_blank" rel="noopener">X Virtual FrameBuffer</a>. The <code>:1</code> option denotes that this process starts a No.1 server, and the <code>-screen</code> option takes in 2 arguments: screen number and WxHxD, where D represents the color depth (measured by bits) of the screen. </p><p>The second command <code>x11vnc</code> (<a href="https://linux.die.net/man/1/x11vnc" target="_blank" rel="noopener">x11vnc man page</a>) starts a process forwarding the dumped screen output to a certain port, where <code>-display</code> option should be binded with the previously executed <code>Xvfb</code> command; and <code>-allow</code> specifies the IP address allowed; The command will automatically search a port available from a certain port which is specified by  <code>-autoport</code> option. </p><p>You need to remember the listening port number for future use, here we have 5900. </p><p><img src="https://i.loli.net/2019/07/31/5d413b31caa7e82895.png" alt></p><h3 id="2-Set-up-the-SSH-tunneling"><a href="#2-Set-up-the-SSH-tunneling" class="headerlink" title="2. Set up the SSH tunneling"></a>2. Set up the SSH tunneling</h3><p>With the port 5900 set up, we can now forward the message to our local Windows machine. Run the following command: </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -N -f -L 6666:&lt;IP of G&gt;:5900 &lt;username of S&gt;@&lt;IP of S&gt;</span><br></pre></td></tr></table></figure><p>To note here port 6666 is just a random local access point that you can designate at will for later VNC viewer. </p><h3 id="3-VNC-viewer-connection"><a href="#3-VNC-viewer-connection" class="headerlink" title="3. VNC viewer connection"></a>3. VNC viewer connection</h3><p>Open the VNC viewer. Type in <code>localhost:6666</code> and set up the VNC connection. If the connection is built, there will be a warning console popping up as shown in the following figure. Click ‘continue’ and you will be all set. </p><p><img src="https://i.loli.net/2019/07/31/5d413f7ba712732902.png" alt></p><h3 id="4-Google-Football-Training"><a href="#4-Google-Football-Training" class="headerlink" title="4. Google Football Training"></a>4. Google Football Training</h3><p>The training code is provided by google team in the file <code>run_ppo.py</code>. We should designate the exact screen it should be rendered by setting the <code>DISPLAY</code> environment paramter: </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env DISPLAY=:1.0 MESA_GL_VERSION_OVERRIDE=3.2 MESA_GLSL_VERSION_OVERRIDE=150 python3 -m gfootball.examples.run_ppo2 --dump_full_episodes=True --level=academy_run_to_score_with_keeper --render=True</span><br></pre></td></tr></table></figure><p>And the final result is shown. One thing to note here is that the refresh rate of the rendered picture is pretty low, therefore don’t get your expectation too high.</p><p><img src="https://i.loli.net/2019/07/31/5d413eda6288d55623.png" alt></p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>Up to now we have conducted one experiment to reimplement the Shooting Bronze mini-game in FIFA18. The performance of the <a href="https://arxiv.org/abs/1707.06347" target="_blank" rel="noopener">PPO algorithm</a> is actually pretty good while it’s still sometimes influenced by the large training variance. </p><h2 id="Setting"><a href="#Setting" class="headerlink" title="Setting"></a>Setting</h2><p>The football academy provided by the google research team has a very similar setting called <code>academy_run_to_score_with_keeper.py</code> in the <code>scenarios</code> folder, with the starting point of the player set in the middle of the field. By setting the ball nearer to the goal, and with some minor modification of the players’ position, we can reimplement the FIFA Shooting Bronze scenario in the google football. </p><h3 id="Original-Setting-of-academy-run-to-score-with-keeper-py"><a href="#Original-Setting-of-academy-run-to-score-with-keeper-py" class="headerlink" title="Original Setting of academy_run_to_score_with_keeper.py"></a>Original Setting of <code>academy_run_to_score_with_keeper.py</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">builder.SetBallPosition(<span class="number">0.02</span>, <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set current team to home</span></span><br><span class="line">builder.SetTeam(Team.e_Left)</span><br><span class="line">builder.AddPlayer(<span class="number">-1.0</span>, <span class="number">0.0</span>, e_PlayerRole_GK)</span><br><span class="line">builder.AddPlayer(<span class="number">0.0</span>, <span class="number">0.0</span>, e_PlayerRole_CB)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set current team to away</span></span><br><span class="line">builder.SetTeam(Team.e_Right)</span><br><span class="line">builder.AddPlayer(<span class="number">-1.0</span>, <span class="number">0.0</span>, e_PlayerRole_GK)</span><br><span class="line">builder.AddPlayer(<span class="number">0.12</span>, <span class="number">0.2</span>, e_PlayerRole_LB)</span><br><span class="line">builder.AddPlayer(<span class="number">0.12</span>, <span class="number">0.1</span>, e_PlayerRole_CB)</span><br><span class="line">builder.AddPlayer(<span class="number">0.12</span>, <span class="number">0.0</span>, e_PlayerRole_CM)</span><br><span class="line">builder.AddPlayer(<span class="number">0.12</span>, <span class="number">-0.1</span>, e_PlayerRole_CB)</span><br><span class="line">builder.AddPlayer(<span class="number">0.12</span>, <span class="number">-0.2</span>, e_PlayerRole_RB)</span><br></pre></td></tr></table></figure><h3 id="Shooting-Bronze-Reimplementation"><a href="#Shooting-Bronze-Reimplementation" class="headerlink" title="Shooting Bronze Reimplementation"></a>Shooting Bronze Reimplementation</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">builder.SetBallPosition(<span class="number">0.52</span>, <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set current team to home</span></span><br><span class="line">builder.SetTeam(Team.e_Home)</span><br><span class="line">builder.AddPlayer(<span class="number">-1.0</span>, <span class="number">0.0</span>, e_PlayerRole_GK)</span><br><span class="line">builder.AddPlayer(<span class="number">0.5</span>, <span class="number">0.0</span>, e_PlayerRole_CF)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set current team to away</span></span><br><span class="line">builder.SetTeam(Team.e_Away)</span><br><span class="line">builder.AddPlayer(<span class="number">-1.0</span>, <span class="number">0.0</span>, e_PlayerRole_GK)</span><br><span class="line">builder.AddPlayer(<span class="number">-0.3</span>, <span class="number">0.2</span>, e_PlayerRole_LB)</span><br><span class="line">builder.AddPlayer(<span class="number">-0.3</span>, <span class="number">0.1</span>, e_PlayerRole_CB)</span><br><span class="line">builder.AddPlayer(<span class="number">-0.3</span>, <span class="number">0.0</span>, e_PlayerRole_DM)</span><br><span class="line">builder.AddPlayer(<span class="number">-0.3</span>, <span class="number">-0.1</span>, e_PlayerRole_CB)</span><br><span class="line">builder.AddPlayer(<span class="number">-0.3</span>, <span class="number">-0.2</span>, e_PlayerRole_RB)</span><br></pre></td></tr></table></figure><p>One crucial thing to understand here is that the setting procedure is imperative (like pyplot and matlab). The coordinate axis of the ball is absolute, while the coordinate axis of the team is relative, which is shown in the following figure. </p><p><img src="https://i.loli.net/2019/07/31/5d414fba1ec3261321.png" alt></p><h2 id="Parallelism-Performance"><a href="#Parallelism-Performance" class="headerlink" title="Parallelism Performance"></a>Parallelism Performance</h2><p>We tested the performance (accuracy, speed) of the single-machine PPO algorithm. The PPO algorithm runs the episodes parallelly, and then collect all the traces to form a pool and then run the SGD update for several times. The parallelism of the PPO on google football is tested on a workstation with an Intel i9-7900 CPU, the result of which is shown in the following figure, where FPS stands for the total steps per second and the yellow line stands for the standard deviation of the FPS. </p><p><img src="https://i.loli.net/2019/07/31/5d4151f05527d87911.png" alt></p><p>The reason why the number of the environments could be larger than the number of the physical cores and still gain performance improvement is that the PPO implemented by <a href="https://github.com/openai/baselines" target="_blank" rel="noopener">OpenAI baselines</a> package is totally synchronous, which means that the environment stepping and the parameter updating is performed completely sequentially. The future work of the google football is highly likely to be deployed on distributed systems, making PPO algorithm and performance analysis less important. </p><h2 id="Accuray-Performance"><a href="#Accuray-Performance" class="headerlink" title="Accuray Performance"></a>Accuray Performance</h2><p>The pure PPO algorithm achieves a remarkablely high accuracy after 2M steps, sometimes with over 90% (or even 100%). While it may crash and get stuck at some local optimal point. The detailed record of the accuracy is lost, but you could check out the inference demo by clicking <a href="https://drive.google.com/file/d/1bNO5rpUhCeCZY9zPGgVCzgUlqH9QF39n/view?usp=sharing" target="_blank" rel="noopener">this link</a> on google drive. </p><h1 id="GFootball-Code-Analysis"><a href="#GFootball-Code-Analysis" class="headerlink" title="GFootball Code Analysis"></a>GFootball Code Analysis</h1><p>Personally I don’t think diving too deep into the google football environment code is a fantastic idea, for the detailed implementation should always be hidden from us users, unless there are some bugs you can’t wait to be fixed by the developers and you want to DIY (then I guess you must be super super gooood).</p><p>In this section, we will start with the overall structure of the google football project. Then we will list and explain all the exposed interfaces that you might need when developing your own algorithms. Finally we will cover some valuable issues. </p><h2 id="Overall-Structure"><a href="#Overall-Structure" class="headerlink" title="Overall Structure"></a>Overall Structure</h2><p>First of all, the whole project is composed of 2 major parts: <strong>football engine</strong> and <strong>football environment</strong>. The football engine is written in C++ because it involves the OpenGL rendering and should be highly optimized to improve the responsiveness. The football environment is written in Python and is wrapped gym-likely (<a href="https://gym.openai.com/" target="_blank" rel="noopener">OpenAI gym</a>) for better compatibility with the machine learning community. When installing the google football package, it will first compile the football engine and generate a dynamic library called <code>gfootball_engine.so</code>, which could be invoked by Python file using <code>import</code> statement (for more information about how to write the C++/Python Interface, please check the <a href="https://www.boost.org/doc/libs/1_70_0/libs/python/doc/html/index.html" target="_blank" rel="noopener">boost.Python</a> C++ library).</p><p><img src="https://i.loli.net/2019/07/31/5d41ae603455946761.png" alt></p><p>Our main focus is the gfootball environment construction, which is implemented in the directory <code>/gfootball/env</code>. The stream of creating the final environment object <code>FootballEnv</code> (in <code>/gfootball/env/football_env.py</code>) goes through at least three wrappers: </p><ul><li>First,<code>FootballEnvCore</code> (in <code>/gfootball/env/football_env_core.py</code>) interacts directly with the football engine, creates in total 11+11 <code>controllers</code> for every player, and most importantly, realizes the gym-like environment API including <code>reset</code> and <code>step</code>. </li><li>Secondly, <code>FootballEnvWrapper</code> (in <code>/gfootball/env/football_env_wrapper.py</code>) follows the same API standard and implements additional ‘writing_dumps’ function.</li><li>Finally, <code>FootballEnv</code> inherits the <code>FootballEnvWrapper</code>‘s API and loads the player algorithm by <code>_constructin_players</code>. </li></ul><p>After that, multiple wrappers could be used for a much more customized environment, as shown in the function <code>create_single_environment</code> in the file <code>/gfootball/env/__init__.py</code>. For more informationn about the wrappers, please check out the classes in <code>/gfootball/env/wrappers.py</code>.</p><p>The overall structure of the code is demonstrated in the following figure. To note that there might be some changes in the latest release, it’s better for you to check the newest version of the code and not get trapped in this article. </p><p><img src="https://i.loli.net/2019/07/31/5d41ae5fa113687245.png" alt></p><h2 id="Interfaces"><a href="#Interfaces" class="headerlink" title="Interfaces"></a>Interfaces</h2><h3 id="1-General-Configuration"><a href="#1-General-Configuration" class="headerlink" title="1. General Configuration"></a>1. General Configuration</h3><p>The configuration of the environment could be found in file <code>/gfootbal/env/config.py</code> and``. The important configuration paramters that might be used in the future include (you should double check it yourself):</p><ul><li><strong>action_set</strong>: a string deciding allowed actions. The dictionary defining the string-action_set mapping resides in file <code>/gfootball/env/football_action_set.py</code>.</li><li><strong>dump_full_episodes</strong>: a bool deciding whether dumps the full episodes to the disk when training. </li><li><strong>game_difficulty</strong>: a real number from 0 to 1 indicating the responsiveness of the bot, {easy: 0.05, medium: 0.6, hard: 0.95}.</li><li><strong>level</strong>: a string of the scenario name deciding which <code>scenarios/</code> file should imported. </li><li><strong>real_time</strong>: a bool ddeciding whether it’s real time for human player. </li></ul><h3 id="2-Scenario-Configuration"><a href="#2-Scenario-Configuration" class="headerlink" title="2. Scenario Configuration"></a>2. Scenario Configuration</h3><p>It’s implemented in file <code>/gfootball/env/scenario_builder.py</code>, but should be used with the <code>scenario_builder</code> class as shown in any file in the <code>/gfootball/env/scenarios/</code>.  There mainly interfaces that you will use when creating your own customized scenarios:</p><ul><li><strong>SetFlag(name, value)</strong>: directly invoke <code>config.set_scenario_value()</code> , which is defined in the <code>Config</code> class. The related parameters are listed as follows (all of them):<ul><li><em>deterministic</em>: whether there is some stochasticity.</li><li><em>end_episode_on_score</em>: if score then end the episode. </li><li><em>end_episode_on_possession_change</em>: if possession of the ball is changed, then end the episode.</li><li><em>end_episode_on_out_of_play</em>: if out of play, then end the episode.</li><li><em>game_duration</em>: how many steps of the environment before ending it forcely.</li><li><em>offsides</em>: whether offsides judegement is introduced to the games. </li></ul></li><li><strong>SetTeam(team)</strong>: set current team (left/right).</li><li><strong>SetBallPosition(ball_x, ball_y)</strong>: set the position of the ball. </li><li><strong>AddPlayer(x, y, role)</strong>: add a player to the field. </li></ul><p>One typical way of using this interface is to rewrite a scenario builder as <code>/gfootball/scenarios/11_vs_11_easy_stochastic.py</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> . <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_scenario</span><span class="params">(builder)</span>:</span></span><br><span class="line">  builder.SetFlag(<span class="string">'game_duration'</span>, <span class="number">3000</span>)</span><br><span class="line">  builder.SetFlag(<span class="string">'game_difficulty'</span>, <span class="number">0.05</span>)</span><br><span class="line">  builder.SetFlag(<span class="string">'deterministic'</span>, <span class="literal">False</span>)</span><br><span class="line">  <span class="keyword">if</span> builder.EpisodeNumber() % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">    first_team = Team.e_Left</span><br><span class="line">    second_team = Team.e_Right</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    first_team = Team.e_Right</span><br><span class="line">    second_team = Team.e_Left</span><br><span class="line">  builder.SetTeam(first_team)</span><br><span class="line">  builder.AddPlayer(<span class="number">-1.000000</span>, <span class="number">0.000000</span>, e_PlayerRole_GK)</span><br><span class="line">  builder.AddPlayer(<span class="number">0.000000</span>,  <span class="number">0.020000</span>, e_PlayerRole_RM)</span><br><span class="line">  builder.AddPlayer(<span class="number">0.000000</span>, <span class="number">-0.020000</span>, e_PlayerRole_CF)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.422000</span>, <span class="number">-0.19576</span>, e_PlayerRole_LB)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.500000</span>, <span class="number">-0.06356</span>, e_PlayerRole_CB)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.500000</span>, <span class="number">0.063559</span>, e_PlayerRole_CB)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.422000</span>, <span class="number">0.195760</span>, e_PlayerRole_RB)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.184212</span>, <span class="number">-0.10568</span>, e_PlayerRole_CM)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.267574</span>, <span class="number">0.000000</span>, e_PlayerRole_CM)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.184212</span>, <span class="number">0.105680</span>, e_PlayerRole_CM)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.010000</span>, <span class="number">-0.21610</span>, e_PlayerRole_LM)</span><br><span class="line">  builder.SetTeam(second_team)</span><br><span class="line">  builder.AddPlayer(<span class="number">-1.000000</span>, <span class="number">0.000000</span>, e_PlayerRole_GK)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.050000</span>, <span class="number">0.000000</span>, e_PlayerRole_RM)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.010000</span>, <span class="number">0.216102</span>, e_PlayerRole_CF)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.422000</span>, <span class="number">-0.19576</span>, e_PlayerRole_LB)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.500000</span>, <span class="number">-0.06356</span>, e_PlayerRole_CB)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.500000</span>, <span class="number">0.063559</span>, e_PlayerRole_CB)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.422000</span>, <span class="number">0.195760</span>, e_PlayerRole_RB)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.184212</span>, <span class="number">-0.10568</span>, e_PlayerRole_CM)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.267574</span>, <span class="number">0.000000</span>, e_PlayerRole_CM)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.184212</span>, <span class="number">0.105680</span>, e_PlayerRole_CM)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.010000</span>, <span class="number">-0.21610</span>, e_PlayerRole_LM)</span><br></pre></td></tr></table></figure><h3 id="3-Player-Interface"><a href="#3-Player-Interface" class="headerlink" title="3. Player Interface"></a>3. Player Interface</h3><p>The player interface is well defined and easy to implement. Just all you need to do is to load your own trained model and reimplement the <code>take_action</code> method as the rest of the player files residing in the folder <code>/gfootball/env/players</code>.</p><h3 id="4-Reward-Interface"><a href="#4-Reward-Interface" class="headerlink" title="4. Reward Interface"></a>4. Reward Interface</h3><p>The paper proposes two ways of giving reward to an agent, one of which is called ‘checkpoint’. The checkpoint reward wrapper <code>CheckpointRewardWrapper</code> is implemented in the file <code>/gfootball/wrappers.py</code> and it inherits from the <code>gym.RewardWrapper</code> class. The only thing that we need to do to customize our own reward mechanism is to implement <code>reset()</code> and <code>reward()</code> method. </p><h2 id="Related-Issues"><a href="#Related-Issues" class="headerlink" title="Related Issues"></a>Related Issues</h2><p>Several related issues were raised and (nicely) solved by the google football team. Firstly, <a href="https://github.com/google-research/football/tree/v1.1" target="_blank" rel="noopener">the latest release</a> supports multi-agent training and self-play. Secondly, about the FPS evaluation, the whole environment interacts with the agent in a synchronous way while it could be confusing when compared with playing with human, <a href="https://github.com/google-research/football/issues/41" target="_blank" rel="noopener">this issue</a> was solved just lately. Last, about <a href="https://github.com/google-research/football/issues/36" target="_blank" rel="noopener">not able to change the active player</a>, the only answer we got is that it’s not exposed to us users thus modifying it needs to dig down to the C++ engine. </p><h1 id="Last-Words"><a href="#Last-Words" class="headerlink" title="Last Words"></a>Last Words</h1><p>Thank you for reading this article. I would be super happy even if it only helps you a little bit! Wish you good luck! If you have any question, please do not hesitate to contact me via my email at the bottom! </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Preface&quot;&gt;&lt;a href=&quot;#Preface&quot; class=&quot;headerlink&quot; title=&quot;Preface&quot;&gt;&lt;/a&gt;Preface&lt;/h1&gt;&lt;p&gt;The analytical work on the &lt;a href=&quot;https://github
      
    
    </summary>
    
    
      <category term="notes" scheme="http://www.shihaizhou.com/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>RL(2) - Model-Free Policy Evaluation</title>
    <link href="http://www.shihaizhou.com/2019/07/15/RL-Course-Lesson-2-Model-Free-Policy-Evaluation/"/>
    <id>http://www.shihaizhou.com/2019/07/15/RL-Course-Lesson-2-Model-Free-Policy-Evaluation/</id>
    <published>2019-07-15T07:49:57.000Z</published>
    <updated>2020-02-18T12:22:08.794Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Policy-Evaluation"><a href="#Policy-Evaluation" class="headerlink" title="Policy Evaluation"></a>Policy Evaluation</h1><p>本章主要解决如何在缺少MDP模型内部的转移函数的情况下进行Policy Evaluation。主要的方法为动态规划、蒙特卡洛（Monte Carlo Policy Evaluation）方法以及Temporal Difference（TD）。</p><p>首先回顾一下价值迭代用到的方法：</p><script type="math/tex; mode=display">V_{k}^{\pi}(s)=r(s,\pi(s)) + \gamma \sum_{s'\in S}{P(s'|s,\pi(s))V_{k-1}^{\pi}(s')}</script><p>其中$V_k^{\pi}(s)$是在策略$\pi$下的$k$-horizon value的精确值；而当$k$足够大的时候又是Infinite horizon value的估计值。Policy Evaluation的核心在于以下的数学期望表达式：策略$\pi$下的状态-价值=当前状态下Return值的数学期望。</p><script type="math/tex; mode=display">V^{\pi}(s)=E_\pi[G_t|s_t=s]</script><p>使用动态规划的Evaluation方法利用了公式：</p><script type="math/tex; mode=display">V^\pi(s)\approx E_\pi[r_t+\gamma V_{k-1}|s_t=s]</script><p>使用这个递推表达式有以下的约束和drawback：首先是公式中的$P(s’|s, \pi(s))$需要MDP Model $M$的具体条件转化概率矩阵；第二使用Bootstrapping方法要求整个过程是马尔科夫的（也就是和状态历史无关）。事实上在绝大部分情况下我们的agent都是不知道自己所处环境的状态转移关系$P$以及奖励模型$R$的，也就是这章要讲的Model-free Policy Evaluation。</p><h1 id="Monte-Carlo-Policy-Evaluation"><a href="#Monte-Carlo-Policy-Evaluation" class="headerlink" title="Monte Carlo Policy Evaluation"></a>Monte Carlo Policy Evaluation</h1><p>Policy Evaluation的目标是：对任意状态$s$，计算$V^{\pi}(s)=E_\pi[G_t|s_t=s]$。最简单的想法就是在当前的环境中能够进行采样，对每个采样的轨迹（trajectory）进行奖励的求和并对总体平均，这样就是一个对于$G_t$的estimate。该方法是一个通用方法，它不需要马尔科夫假设作为支撑，但它也只能被用于episodic MDP上，也就是说每个trajectory必须要有terminate的时候。通常MC方法都采用增量式的更新模式。</p><h2 id="First-Visit-MC-amp-Every-Time-MC"><a href="#First-Visit-MC-amp-Every-Time-MC" class="headerlink" title="First-Visit MC &amp; Every-Time MC"></a>First-Visit MC &amp; Every-Time MC</h2><p>First-Visit MC方法仅仅更新一个新采样Episode中第一次看到的状态的价值函数，这种方法是$V^\pi$的无偏估计。Every-Time MC方法每一次在sample中看到状态都会进行更新，虽然它是$V^\pi$的有偏估计，但是它依旧是一个consistent estimator并且比First-Visit MC具有更好的均方误差（MSE）。</p><h2 id="Variant-of-Every-Time-MC"><a href="#Variant-of-Every-Time-MC" class="headerlink" title="Variant of Every-Time MC"></a>Variant of Every-Time MC</h2><p>在Incremental MC方法当中需要维护一个数组$N(s)$来记录每个状态被访问更新的次数。定义$G_{i,t}$是在第$i$个episode的第$t$个time step的return值，我们将更新状态价值函数的表达式进行一些改写，将后面的项数看作是增量部分，那么$N(s)$就是每一次sample时更新的步幅，可以用$\alpha$表示：</p><script type="math/tex; mode=display">\begin{align}V^{\pi}(s) &= V^{\pi}(s){N(s)-1\over N(s)}+{G_{i,t}\over N(s)} \\&= V^\pi(s) + {1\over N(s)}[G_{i,t}-V^\pi(s)] \\&= V^\pi(s) + \alpha [G_{i,t}-V^\pi(s)]\end{align}</script><ul><li><p>当$\alpha={1\over N(s)}$时，更新的方法和Every-Visit MC相同。</p></li><li><p>当$\alpha\gt {1\over N(s)}$时，倾向于忘记之前的data sample，而对最近的sample较为敏感。这种更新策略对于Non-stationary domain的问题比较有效。</p></li></ul><h2 id="Key-Limitations-of-MC"><a href="#Key-Limitations-of-MC" class="headerlink" title="Key Limitations of MC"></a>Key Limitations of MC</h2><ul><li>首先MC是一个high-variance的方法，通常需要大量的sampled data才能够解决这个问题。</li><li>需要Episodic Setting，轨迹必须终结。</li></ul><h1 id="Temporal-Difference-Learning"><a href="#Temporal-Difference-Learning" class="headerlink" title="Temporal Difference Learning"></a>Temporal Difference Learning</h1><p>TD-learning是一个结合了MC方法和DP方法的Policy Evaluation策略，它采用了bootstrapping的思想，能够支持Infinite-Horizon的问题以及能够做到在每个step取得tuple $(s,a,r,s’)$后都直接进行更新。</p><p>MC方法中，我们用来更新价值的公式为：</p><script type="math/tex; mode=display">V^\pi(s) = V^\pi(s) + \alpha [G_{i,t}-V^\pi(s)]</script><p>我们知道$G$是状态价值函数的estimate，要拿到这个值必须要在episode结束之后。观察Bellman Equation的后半部分，这里的形式是利用MDP model的状态转移概率来写出精确的数学期望，但是在sampling的时候后面的部分会以sample的形式作为下一个状态和reward返回给agent，因此我们可以这个estimate来对Return值$G$进行bootsrappinng</p><script type="math/tex; mode=display">\begin{align}B^\pi V(s) &= r(s,\pi(s)) + \gamma \sum_{s'\in S}P(s'|s,\pi(s))V(s') \\V^\pi(s) &= V^\pi(s) + \alpha([r_t+\gamma V^\pi(s_{t+1})] - V^\pi(s))\end{align}</script><p>我们将TD error定义为更新的数值：</p><script type="math/tex; mode=display">\delta_t = r_t + \gamma V^\pi(s_{t+1}) - V^\pi(s_t)</script><h1 id="Evaluating-Algorithms"><a href="#Evaluating-Algorithms" class="headerlink" title="Evaluating Algorithms"></a>Evaluating Algorithms</h1><p>比较DP/MC/TD三个方法的性质：</p><div class="table-container"><table><thead><tr><th></th><th>DP</th><th>MC</th><th>TD</th></tr></thead><tbody><tr><td>Support Model-Free?</td><td></td><td>T</td><td>T</td></tr><tr><td>Support Non-episodic?</td><td>T</td><td></td><td>T</td></tr><tr><td>Support Non-Markov?</td><td></td><td>T</td><td></td></tr><tr><td>Unbiasd Estimate?</td><td></td><td>T (First-Visit MC)</td></tr></tbody></table></div><p><strong>评价RL算法的重要指标：是否是Biased Estimate？更新的方差大小？数据效率和计算效率？</strong></p><p>MC可以是无偏估计（取决于具体状态函数的更新形式），但该方法具有较高的variance；优点是即便在Function Approximation也能够收敛。</p><p>TD虽然是一个有偏估计，但是却有更小的variance，TD(0)只保证在Tabular Representation下收敛，而不保证在Function Approximation能够收敛。</p><h2 id="Batch-MC-amp-Batch-TD"><a href="#Batch-MC-amp-Batch-TD" class="headerlink" title="Batch MC &amp; Batch TD"></a>Batch MC &amp; Batch TD</h2><p>Batch solution for finite dataset是一种离线算法，能够提高data efficiency。对于已经sample得到的K个episodes，使用MC或TD算法对这K个episodes进行resample并更新参数即可构成该离线算法。</p><p>课程例举了一个实例来说明Batch MC和Batch TD的最终优化结果是不同的。</p><p>课程中认为Batch MC是收敛于min MSE，分析如下：给定一个确定的episode set，我们可以算出什么时候Batch MC的更新为0，也就是说最终每个状态的价值值会收敛到sample出的batch的该状态的平均Return值。这和优化min MSE的结果是相同的（优化函数的导数为0）。</p><script type="math/tex; mode=display">\begin{align}\alpha [\overline{G_{i,t}}-V^\pi(s)] &= 0 \\\Leftrightarrow V^\pi(s) &= \overline{G_{i,t}}\end{align}</script><p>而对于Batch TD(0)方法来说，它最终converge到的是“DP policy $V^\pi$ for the MDP with the maximum likelihood model estimates”.极大似概率的MDP model为以下：</p><script type="math/tex; mode=display">\begin{align}\hat{P}(s'|s,a) &= {1\over N(s,a)}\sum_{k=1}^{K}\sum_{t=1}^{L_k-1}1(s_{k,t}=s,a_{k,t}=a, s_{k,t+1}=s') \\\hat{r}(s,a) &= {1\over N(s,a)}\sum_{k=1}^{K}\sum_{t=1}^{L_k-1}1(s_{k,t}=s,a_{k,t}=a)r_{t,k}\end{align}</script><p>也就是说在给定的sample集上我们能够根据频率构建出关于MDP model的一个估计（利用如上公式），Batch TD(0)收敛到的点就是这个估计的DP policy。同样的分析最终的收敛点为：</p><script type="math/tex; mode=display">\begin{align}\alpha [\overline{r_t+\gamma V^\pi(s_{t+1})}-V^\pi(s)] &= 0 \\\Leftrightarrow V^\pi(s) &= \overline{r_t+\gamma V^\pi(s_{t+1})}\end{align}</script><p>而我们知道等式右边的平均项其实就是reward和带状态转移概率的价值函数的估计。</p><h2 id="Some-Important-Properties"><a href="#Some-Important-Properties" class="headerlink" title="Some Important Properties"></a>Some Important Properties</h2><p>对于TD和MC方法，更新的时间复杂度关于episode长度$L$来说都是$O(L)$。MC比较TD来说它的data efficiency更高一些，但是TD利用了马尔科夫性质。当我们解决具有马尔科夫性质的问题时，利用这个性质往往会比较有帮助。关于为什么TD利用了马尔科夫性质，我的理解是下面的近似替换表达式认为在当前time step的return value = 当前的immediate reward + 下一状态的state value是历史无关的，只和当前状态有关。</p><script type="math/tex; mode=display">G_{i,t}\approx r_t + \gamma V^\pi(s_{t+1})</script><h1 id="MC-Off-Policy-Evaluation"><a href="#MC-Off-Policy-Evaluation" class="headerlink" title="*MC Off-Policy Evaluation"></a>*MC Off-Policy Evaluation</h1><p>当前的policy evaluation都是基于sampling的，是on-policy的方法，on-policy简单来说就是在线跑算法并且在线evaluate。所以本章研究的是如何利用现有的data进行policy evaluation。现在formulate MC Off-Policy Evaluation的问题。</p><ul><li>Aim - estimate value of policy $\pi_1$, $V^{\pi_1}(s)$, givein episodes generated under behavior policy $\pi_2$.<ul><li>$s_1, a_1, r_1, s_2, a_2, r_2, \ldots$ where the actions are sampled from $\pi_2$</li></ul></li><li>If $\pi_2$ is stochastic, can often use it to estimate the value of an alternative policy (formal conditions to follow)</li><li>No requirement that have a model nor that state is Markov. </li></ul><p>Off-policy evaluation存在的主要问题就是两个不同的policy产生出episode和reward的distribution不同。重要性采样（Importance Sampling）的目标是估计一个函数$f(x)$在某一个概率分布$p(x)$下的数学期望，但是现在只有从另一个分布$q(s)$采样出的数据点$x_1,x_2,\ldots,x_n$。首先我们可以很容易地求出这个函数在$q(s)$上的数学期望。</p><script type="math/tex; mode=display">E_{x\sim q}[f(x)] = \int_xq(x)f(x)</script><p>假设$h_j$为所有sample点中的第$j$条轨迹：</p><script type="math/tex; mode=display">h_j = (s_{j,1}, a_{j,1}, r_{j,1}, s_{j,2}, a_{j,2}, r_{j,2}, \ldots,s_{j, L_j})</script><p>那么根据马尔科夫假设以及乘法原理，得出这条轨迹出现的概率为每个状态转移的乘积：</p><script type="math/tex; mode=display">p(h_j|\pi, s=s_{j,1}) = \prod_{t=1}^{L_j-1} \pi(a_{j,t}|s_{j,t})p(r_{j,t}|s_{j,t}, a_{j,t})p(s_{j,t+1}|s_{j,t},a_{j,t})</script><p>虽然整体问题是Model-Free的环境，但从上式得知，导致两个不同的policy分布的地方仅仅在于这策略本身决定action的部分。所以我们容易求出两个策略下轨迹的概率比值，从而得出需要的近似，其中$G(h_j)$为轨迹$h_j$的return value。</p><script type="math/tex; mode=display">V^{\pi_1}(s) \approx \sum_{j=1}^{n} {p(h_j|\pi_1,s)\over p(h_j|\pi_2,s)} G(h_j)</script><p>问题：课程中说这个推导过程不需要马尔科夫假设。但是上面将概率写成了只和当前状态相关的概率连乘积不是已经用到了马尔科夫假设了吗？需要进一步思考和验证。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Policy-Evaluation&quot;&gt;&lt;a href=&quot;#Policy-Evaluation&quot; class=&quot;headerlink&quot; title=&quot;Policy Evaluation&quot;&gt;&lt;/a&gt;Policy Evaluation&lt;/h1&gt;&lt;p&gt;本章主要解决如何在缺
      
    
    </summary>
    
    
      <category term="RL" scheme="http://www.shihaizhou.com/tags/RL/"/>
    
  </entry>
  
  <entry>
    <title>RL(1) - MP, MRP, MDP</title>
    <link href="http://www.shihaizhou.com/2019/07/10/RL-Course-Lesson-1-MP-MRP-MDP/"/>
    <id>http://www.shihaizhou.com/2019/07/10/RL-Course-Lesson-1-MP-MRP-MDP/</id>
    <published>2019-07-10T10:16:03.000Z</published>
    <updated>2020-02-18T12:22:01.515Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Markov-Property"><a href="#Markov-Property" class="headerlink" title="Markov Property"></a>Markov Property</h1><p>马尔科夫性质指的是状态转移的方程与历史状态无关而仅仅和当前状态相关。用数学表达马尔科夫状态$s_t$：</p><script type="math/tex; mode=display">p(s_{t+1}|s_t,a_t)=p(s_{t+1}|h_t,a_t)</script><h1 id="Markov-Process"><a href="#Markov-Process" class="headerlink" title="Markov Process"></a>Markov Process</h1><p>马尔科夫过程也可以被称作马尔科夫链，是一种具有马尔科夫性质的无记忆随机过程。马尔科夫过程由一个有限状态集$S$以及一个状态转移模型$P$构成，其中$P$指定了从某个特定状态转移到另一个状态的概率:</p><script type="math/tex; mode=display">P(s_{t+1}=s'|s_t=s)</script><p>需要注意的是在马尔科夫过程的设定中还没有reward或者action，这两项元素会逐步被加进来。如果状态集$S$是有限的，那么我们可以将$P$用一个矩阵表示：</p><script type="math/tex; mode=display">P = \left[\matrix{P(s_1|s_1) & P(s_2|s_1) & \ldots & P(s_N|s_1)\\                P(s_1|s_2) & P(s_2|s_2) & \ldots & P(s_N|s_2)\\                \vdots & \vdots & \ddots & \vdots \\                P(s_1|s_N) & P(s_2|s_N) & \ldots & P(s_N|s_N)\\}\right]</script><h1 id="Markov-Reward-Process"><a href="#Markov-Reward-Process" class="headerlink" title="Markov Reward Process"></a>Markov Reward Process</h1><p>马尔科夫奖励过程其实就是马尔科夫链（Markov Chain）加上rewards。马尔科夫奖励过程由以下四项元素组成：</p><ul><li>$S$：代表环境或者世界的一个有限状态集</li><li>$P$：一个状态转移模型，声明了$P(s_{t+1}=s’|s_t=s)$</li><li>$R$：一个奖励函数，声明了在某一状态下agent收到的奖励，$R(s_t=s)=E[r_t|s_t=s]$ </li><li>$\gamma$：对于未来奖励进行加权的折扣因子，$\gamma \in [0,1]$ </li></ul><h2 id="Parameter-Explanation"><a href="#Parameter-Explanation" class="headerlink" title="Parameter Explanation"></a>Parameter Explanation</h2><h3 id="Horizon"><a href="#Horizon" class="headerlink" title="Horizon"></a>Horizon</h3><p>注意这个名词很少在项目中被提到，简单来说它就是一个episode的time step数目。horizon可以是无限的，如果任意一个horizon都是有限的，那么我们可以将这样的MRP称作Finite MRP。</p><h3 id="Return"><a href="#Return" class="headerlink" title="Return"></a>Return</h3><p>Return $G_t$ 的中文译名我没有关注，在数学上的含义就是从某个时间步$t$开始沿着某个状态$s$的序列获得的奖励之和：</p><script type="math/tex; mode=display">G_t=r_t+\gamma r_{t+1}+\gamma^{2}r_{t+2}+\gamma^{3}r_{t+3}+\ldots</script><h3 id="State-Value-Function"><a href="#State-Value-Function" class="headerlink" title="State Value Function"></a>State Value Function</h3><p>状态$s$的状态价值函数指的是从状态$s$出发取得的return的期望值：</p><script type="math/tex; mode=display">V(s)=E[G_t|s_t=s]=E[r_t+\gamma r_{t+1}+\gamma^{2}r_{t+2}+\gamma^{3}r_{t+3}+\ldots|s_t=s]</script><h3 id="Discount-Factor"><a href="#Discount-Factor" class="headerlink" title="Discount Factor"></a>Discount Factor</h3><p>折扣因子是小于等于1的正数，这个参数的提出基于两个考虑。第一是能够带来数学上的便利：在其小于1时保证每个状态的状态价值函数的值保证收敛；第二就是符合人在行动时的常见模式：比起远处的reward人们更在意immediate reward。当$\gamma=0$时，意味着模型只考虑immediate reward，成为完全的贪婪模型；当$\gamma=1$时则适用于有限马尔科夫过程，认为未来和当前的reward有相同的重要性。</p><h2 id="Computing-State-Value-Function-for-MRP"><a href="#Computing-State-Value-Function-for-MRP" class="headerlink" title="Computing State Value Function for MRP"></a>Computing State Value Function for MRP</h2><h3 id="Via-simulation"><a href="#Via-simulation" class="headerlink" title="Via simulation"></a>Via simulation</h3><p>首先最直观的方法是通过模拟（simulation）的方法求得每个状态的state value，得到大量的episodes，将所有的return值进行加和求平均即可，<strong>这样做不需要整个环境是符合马尔科夫假设的</strong>。</p><h3 id="Via-matrix-equation"><a href="#Via-matrix-equation" class="headerlink" title="Via matrix equation"></a>Via matrix equation</h3><p>如果马尔科夫假设在应用场景中是合理的，那么我们可以写出一条state value之间的关系式，其含义是 状态$s$的value = 当前状态的immediate reward + 经过折扣的各个状态的价值期望。</p><script type="math/tex; mode=display">V(s)=R(s)+\gamma \sum_{s'\in S}{P(s'|s)V(s')}</script><p>因为对于所有的状态都成立，所以我们可以将上面针对某个状态的等式写成矩阵形式（依然只适用于有限马尔科夫过程）：</p><script type="math/tex; mode=display">\left[\matrix{V(s_1)\\ \vdots \\ V(s_N)}\right] = \left[\matrix{R(s_1)\\ \vdots \\ R(s_N)}\right] + \gamma \left[\matrix{P(s_1|s_1) & \ldots & P(s_N|s_1)\\                P(s_1|s_2) & \ldots & P(s_N|s_2)\\                \vdots & \ddots & \vdots \\                P(s_1|s_N) & \ldots & P(s_N|s_N)\\}\right] \left[\matrix{V(s_1)\\ \vdots \\ V(s_N)}\right]</script><p>更进一步地简化矩阵内部的元素，得到：</p><script type="math/tex; mode=display">V=R+\gamma PV</script><p>最终目标是算出$V$向量，通过矩阵解方程方法，得到：</p><script type="math/tex; mode=display">V=(I-\gamma P)^{-1}R</script><p>根据数值分析中的知识，我们知道矩阵求逆是$O(N^3)$的复杂度。</p><h3 id="Via-dynamic-programming"><a href="#Via-dynamic-programming" class="headerlink" title="Via dynamic programming"></a>Via dynamic programming</h3><p>其实我们可以通过递推的方式来解得最终的表示，不过我们需要证明整个递推表达式是收敛的。</p><script type="math/tex; mode=display">V_{k}(s)=R(s)+ \gamma \sum_{s'\in S}{P(s'|s)V_{k-1}(s')}</script><p>Dynamic Programming with tolerence $\epsilon$：</p><p><img src="https://i.loli.net/2020/02/05/wy6EUtMYGgoKW7m.png" alt="image.png"></p><p>每一次iteration的时间复杂度为$O(|S|^2)$，也就是$O(|N|^2)$。事实上我们可以证明这个算法最终收敛到真实的$V$上，并且我们还能够给出误差估计值为<script type="math/tex">\left\|V^{\prime}-V\right\|_{\infty} \leq \frac{\epsilon \gamma}{1-\gamma}</script>。证明分为以下步骤：</p><ul><li>定义Bellman Operator，证明这个算子是strict contraction的。</li><li>根据引理得知，strict contraction的算子只有一个不动点，而$BV=R+\gamma PV=V$，则这个不动点就是真实的$V$。</li><li>利用三角不等式和Bellman Operator 的性质得到，严格证明式为以下：</li></ul><script type="math/tex; mode=display">\begin{aligned}\left\|V_{k}-V\right\|_{\infty} & \leq\left\|V_{k}-V_{k+1}\right\|_{\infty}+\left\|V_{k+1}-V\right\|_{\infty}=\left\|V_{k}-V_{k+1}\right\|_{\infty}+\left\|B V_{k}-B V\right\|_{\infty} \\ & \leq\left\|V_{k}-V_{k+1}\right\|_{\infty}+\gamma\left\|V_{k}-V\right\|_{\infty}=\epsilon+\gamma\left\|V_{k}-V\right\|_{\infty} \end{aligned}</script><h1 id="Markov-Decision-Process"><a href="#Markov-Decision-Process" class="headerlink" title="Markov Decision Process"></a>Markov Decision Process</h1><p>马尔科夫决策过程（MDP）是马尔科夫奖励过程（MRP）+actions。类似的，MDP由以下5组元素构成：</p><ul><li>$S$：代表环境或者世界的一个有限状态集，$s\in S$</li><li>$A$：代表agent能够执行的有限动作集，$a\in A$</li><li>$P$：一个状态转移模型。和MRP不同的是，MDP中的状态转移模型是一个同时关于当前状态$s$和当前行动$a$的函数，声明了$P(s_{t+1}=s’|s_t=s,a_t=a)$</li><li>$R$：一个奖励函数。同样的奖励函数也接受了动作$a$作为其中一个变量，声明了在某一状态下采取某一动作agent收到的奖励，$R(s_t=s,a_t=a)=E[r_t|s_t=s,a_t=a]$ 。需要注意的是这仅仅是这门课程中的表示方法，仔细想一下其实$R$是一个仅关于状态的函数也是正确的。</li><li>$\gamma$：对于未来奖励进行加权的折扣因子，$\gamma \in [0,1]$ </li></ul><p>综上我们可以将MDP简化写为一个五元组$(S,A,P,R,\gamma)$。</p><h2 id="MDP-Policy"><a href="#MDP-Policy" class="headerlink" title="MDP + Policy"></a>MDP + Policy</h2><p>对于给定的Policy（这里的给定指的是policy为一个<strong>stationary policy</strong>，即不随着time step的变化而变化的），MDP会退化成一个MRP，因为给定的Policy导致了给定的状态转移概率分布，同时也导致了给定的奖励函数分布。</p><script type="math/tex; mode=display">\begin{align}R^\pi(s) &= \sum_{a\in A}\pi(a|s)R(s,a) \\P^\pi(s'|s) &= \sum_{a\in A}\pi(a|s)P(s'|s,a)\end{align}</script><p>以上的退化形式告诉我们能够利用MRP中的评价方法对于某个特定的policy $\pi$ 进行评价，即计算每个状态的价值函数。Iterative的方法递推公式如下所示，这被称作某个特定policy的Bellman Backup，关于Bellman Operator，我们现在只需要知道这是一个用于辅助证明迭代方法最终能够收敛的手段，在未来如果有机会可以进行更详细的了解。</p><script type="math/tex; mode=display">V_{k}^{\pi}(s)=r(s,\pi(s)) + \gamma \sum_{s'\in S}{P(s'|s,\pi(s))V_{k-1}^{\pi}(s')}</script><h1 id="MDP-Control"><a href="#MDP-Control" class="headerlink" title="MDP Control"></a>MDP Control</h1><p>MDP Control目的在于找到一个最优的策略$\pi^*$使得在该策略下的状态价值函数最大化。</p><script type="math/tex; mode=display">\pi^*(s)=\arg\max_{\pi}V^{\pi}(s)</script><p>课程中提到这种情况下一定存在一个唯一的最优价值函数，但这里还需要再进一步研究。对于Infinite Horizon problem，最优策略一定是deterministic &amp; stationary的，也就是最终会坍缩到不依赖于time step。</p><p>寻找最优的Policy即Policy Search有三种思想，除去最简单的Enumeration的暴力方法（如果是Deterministic Policy的话可以将所有的Policy都罗列出来，复杂度为$|A|^{|S|}$），最为重要的两种方法为策略迭代（Policy Iteration，PI）和价值迭代（Value Iteration，VI）。</p><h2 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h2><p>策略迭代的想法是找到最优的value &amp; policy。</p><p>定义State-Action Value $Q$，其含义为在当前状态$s$下采取动作$a$之后再follow策略$\pi$收获的价值：</p><script type="math/tex; mode=display">Q^\pi(s,a) = R(s,a) + \gamma \sum_{s'\in S}P(s'|s,a)V^\pi(s')</script><p>上述公式实际上是采取了未来一步的探索。将这条式子应用于迭代模型，对于当前迭代轮次$i$：每个$(s,a)$ pair都可以算出相应的$Q$值并且可以更新到下一个迭代轮次：</p><script type="math/tex; mode=display">\begin{align}Q^{\pi_i}(s,a) &= R(s,a) + \gamma \sum_{s'\in S}P(s'|s,a)V^{\pi_i}(s') \\\pi_{i+1}(s) &= \arg \max_{a} Q^{\pi_i}(s,a), \forall s \in S\end{align}</script><p>需要证明这样的更新方法最终是一定能够收敛到全局最优解的，也就是说需要证明这是一个Monotonic Improvement。定义策略价值函数的序关系之后再进行一些简单的放缩即可得到整个更新的过程是单调的。</p><script type="math/tex; mode=display">V^{\pi_1} \geq V^{\pi_2}:V^{\pi_1}(s) \geq V^{\pi_2}(s), \forall s \in S</script><p>因为策略更新是单调的，所以整个策略迭代的过程的停止终点就是策略的更新为零的时候，即：</p><script type="math/tex; mode=display">||\pi_i - \pi_{i-1}||_1=0</script><p>同时策略的更新次数是有界的，也就是在之前提到的Enumeration方法的复杂度$|A|^{|S|}$之内一定能够达到最优点。</p><h2 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h2><p>价值迭代的核心思想在于“Maintain optimal value of starting in a state $s$ if have a finite number of steps k left in the episode.”也就是说首先假设未来只有k步，通过迭代不断地将k往后推，从而达到拟合。</p><p>首先介绍Bellman Equation，一个策略的价值函数必须满足以下等式：</p><script type="math/tex; mode=display">V^{\pi}(s)=R^{\pi}(s)+\gamma\sum_{s'\in S}P^{\pi}(s'|s)V^{\pi}(s')</script><p>Bellman Operator也被称作Bellman Backup Operator。算子（Operator）就是作用在一个函数上将函数再进行一次映射。Bellman Operator作用在这里的value function上同时返回一个新的映射到所有状态的函数。</p><script type="math/tex; mode=display">BV(s)=\max_{a}R(s,a)+\gamma \sum_{s'\in S}P(s'|s,a)V(s')</script><p>Value Iteration的算法中的迭代部分公式为：</p><script type="math/tex; mode=display">V_{k+1}(s)=\max_{a}R(s,a)+\gamma \sum_{s'\in S}P(s'|s,a)V_{k}(s')</script><p>从整个迭代从Bellman Backup的角度来看，每一次迭代的过程就是做了以下的操作：</p><script type="math/tex; mode=display">V_{k+1}=BV_k\\\pi_{k+1}(s)=\arg \max_aR(s,a)+\gamma \sum_{s'\in S}P(s'|s,a)V_k(s')</script><p><strong>这里，每个迭代轮次$k$对应的价值$V_k$其实代表了从当前状态向后行动$k$个时间步的最大价值。</strong>对于FInite Horizon问题来说迭代不需要等到Converge，只需要运行最大Horizon步长即可。</p><p>回到策略迭代，同样可以用Bellman Operator来表达策略迭代的过程，$B^\pi$是针对于某个特定的策略$\pi$的Bellman算子：</p><script type="math/tex; mode=display">B^{\pi}V(s)=R^{\pi}(s)+\gamma \sum_{s'\in S}P^{\pi}(s'|s)V(s)</script><p>Policy Evaluation相当于计算$B^\pi$的不动点，只需要不断地进行Bellman Operator的变换直到$V$不再变化即可。</p><script type="math/tex; mode=display">\begin{align}V^\pi &= B^\pi B^\pi \ldots B^\pi V\\\pi_{k+1}(s) &= \arg\max_{a}R(s,a)+\gamma \sum_{s'\in S} P(s'|s,a)V^{\pi_k}(s')\end{align}</script><p>下面介绍Bellman Operator是一个压缩算子（Contraction）。压缩算子$O$指的是对于任何形式的norm，都有以下不等式成立。如果$V’$是上次迭代的结果，那么该不等式是迭代过程收敛的一个充分条件，表明最终的结果会收敛到一个不动点。</p><script type="math/tex; mode=display">|OV-OV'|\leq|V-V'|</script><p>可以证明当折扣因子$\gamma &lt; 1$，Bellman Operator是一个压缩算子，因此说明我们的迭代过程是收敛的。证明过程如下：</p><script type="math/tex; mode=display">\begin{align}||BV_k - BV_j||&=||\max_a{(R(s,a)+\gamma \sum_{s'\in S}P(s'|s,a)V_k(s'))} - \max_{a'}{(R(s,a')+\gamma \sum_{s'\in S}P(s'|s,a)V_j(s'))}||\\&\leq ||\max_a(R(s,a)+\gamma \sum_{s'\in S}P(s'|s,a)V_k(s') - R(s,a)-\gamma \sum_{s'\in S}P(s'|s,a)V_j(s'))|| \\&= ||\max_a \gamma \sum_{s' \in S}P(s'|s,a)(V_k(s')-V_j(s'))||\\&\leq ||\max_a (\gamma \sum_{s' \in S}P(s'|s,a)||V_k(s')-V_j(s')||)||\\&=\gamma||V_k - V_j||\end{align}</script><h2 id="VI-v-s-PI"><a href="#VI-v-s-PI" class="headerlink" title="VI v.s. PI"></a>VI v.s. PI</h2><p>VI算法依次计算Horizon为$k$的最优值，PI则被用于计算一个策略的Infinite Horizon值，并用于选择一个更好的策略，和未来即将介绍的RL重要算法Policy Gradient有非常紧密的联系。<strong>简单来说，PI算法首先随机初始化一个策略，通过evaluate这个策略得到关于这个策略的价值函数，然后再不断优化这个策略（取得更大的价值函数值）达到拟合；而VI则是将价值函数初始化为0向量，每一次迭代计算当前价值函数的Bellman Optimality Operator，由于该算子是一个压缩算子，因此具有唯一的不动点，并且该不动点是最优点，最后通过价值函数来取得最优的策略。</strong></p><h2 id="Policy-Improvement"><a href="#Policy-Improvement" class="headerlink" title="Policy Improvement"></a>Policy Improvement</h2><p>最后再formulate一下利用Policy Iteration进行Policy Improvement的具体流程：</p><p>首先计算当前策略$\pi_i$的$Q$值：</p><script type="math/tex; mode=display">Q^{\pi_i}(s,a) = R(s,a) + \gamma \sum_{s'\in S}P(s'|s,a)V^{\pi_i}(s')</script><p>因为有以下不等式成立：</p><script type="math/tex; mode=display">\begin{align}\max_aQ^{\pi_i}(s,a)&=\max_aR(s,a)+\gamma \sum_{s'\in S}P(s'|s,a)V^{\pi_i}(s')\\&\geq R(s,\pi_i(s))+\gamma \sum_{s'\in S}P(s'|s,\pi_i(s))V^{\pi_i}(s')\\&= V^{\pi_i}(s)\end{align}</script><p>意味着state-action value是严格非减的，通过迭代就能够不断优化策略。定义下一个迭代轮次$i+1$的策略：</p><script type="math/tex; mode=display">\pi_{i+1}(s) = \arg\max_aQ^{\pi_i}(s,a),\forall s\in S</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Markov-Property&quot;&gt;&lt;a href=&quot;#Markov-Property&quot; class=&quot;headerlink&quot; title=&quot;Markov Property&quot;&gt;&lt;/a&gt;Markov Property&lt;/h1&gt;&lt;p&gt;马尔科夫性质指的是状态转移的方程与历
      
    
    </summary>
    
    
      <category term="RL" scheme="http://www.shihaizhou.com/tags/RL/"/>
    
  </entry>
  
  <entry>
    <title>Google Research Football - RL Environment</title>
    <link href="http://www.shihaizhou.com/2019/06/12/Google-Research-Football-RL-Environment/"/>
    <id>http://www.shihaizhou.com/2019/06/12/Google-Research-Football-RL-Environment/</id>
    <published>2019-06-12T07:08:18.000Z</published>
    <updated>2019-06-13T02:53:35.142Z</updated>
    
    <content type="html"><![CDATA[<p>源github地址：<a href="https://github.com/google-research/football" target="_blank" rel="noopener">google football research github page</a></p><p>源paper地址：<a href="https://github.com/google-research/football/blob/master/paper.pdf" target="_blank" rel="noopener">google football research paper</a></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>文章提出了一个新的Google Research Football Environment，该环境是一个基于物理引擎的足球环境，非常容易迁移，并且是基于开源licence的。文章同时提出了三个不同难度的full-game sceario，提出了Football Benchmarks来标定模型的表现。同时文章测试了3个常用的强化学习模型（IMPALA, PPO, Ape-X DQN）。最后文章还提出了稍微简单一些的scenario，Football Academy。</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>文章的主要贡献为：</p><blockquote><ul><li>provide the Football Engine, a highly-optimized game engine that simulates the game of football,</li><li>propose the Football Benchmarks, a versatile set of benchmark tasks of varying difficulties that can be used to compare different algorithms,</li><li>propose the Football Academy, a set of progressively<br>harder and diverse reinforcement learning scenarios,</li><li>evaluate state-of-the-art algorithms on both the Football<br>Benchmarks and the Football Academy, providing an extensive set of reference results for future comparison, and</li><li>sprovide a simple API to completely customize and define<br>new football reinforcement learning scenarios.</li></ul></blockquote><h1 id="Football-Engine"><a href="#Football-Engine" class="headerlink" title="Football Engine"></a>Football Engine</h1><p>本文的Football Environment是基于另一个工作<em>GameplayFootball</em> simulator.引擎模拟了整个足球游戏，它接收来自两方球队的input action。该引擎实现了足球比赛的众多方面，包括开球，进球，犯规，角球，点球以及边线球。</p><h2 id="支持的足球规则"><a href="#支持的足球规则" class="headerlink" title="支持的足球规则"></a>支持的足球规则</h2><p>几乎所有的足球规则，甚至包括换人。游戏长度是按照frame来进行计算的，默认的整场游戏是3000 frames，但是这一点可以进行customize，初始player数量以及他们的位置也可以被调整。每方的球员会随着时间变长而疲倦，而每一方只能最多进行3次换人。</p><h2 id="内置AI对手"><a href="#内置AI对手" class="headerlink" title="内置AI对手"></a>内置AI对手</h2><p>内置的AI对手是rule based AI，由<em>GameplayFootball</em> simulator开发。困难指数$\theta$是通过调节对手的决策反应时间来影响对手的难度的。推荐的三个难度等级easy, medium, hard的$\theta$值分别为0.05, 0.6, 0.95，我们还能够将内置的AI对手换为我们自己的算法。然后文章介绍了我们最关心的问题：</p><blockquote><p>Moreover, by default, our non-active players are also con- trolled by another rule-based bot. In this case, the behav- ior is simple and corresponds to reasonable football actions and strategies, such as running towards the ball when we are not in possession, or move forward together with our active player. In particular, this type of behavior can be turned off for future research on cooperative multi-agents if desired.</p></blockquote><p>也就是说除了当前的player是自己控制的，其余的player现阶段是通过rule-based的方式被控制的，但是未来我们可以将这个特性关掉从而借助该环境进行multi-agents的研究。</p><h2 id="State-amp-Observation"><a href="#State-amp-Observation" class="headerlink" title="State &amp; Observation"></a>State &amp; Observation</h2><p>文章定义state为游戏当前的所有状态信息的集合（complete set of data returned by the environment after actions are performed），包括ball position/possession, coordinates of all players, the active player, game state(球员疲惫程度，黄牌，比分等等) and current pixel frame. </p><p>同时文章定义observataion指的是state进行任意一种转换后的结果，该结果是作为input传递给control algorithm的。文章提出了三种representation：</p><ul><li>Pixels： 1280$\times$720 RGB 图像</li><li>Super Mini Map：SMM由四个96$\times$72的矩阵组成，编码了包括了主队、客队、足球以及active player的信息。矩阵是binary的形式，简单来说就是bitmap，表征该位置上是否有上述的物体。</li><li>Floats：一个更加紧凑的representation，115维向量用于表征所有的比赛信息，包括players coordinates, ball possession and direction, active player, or game mode.</li></ul><h2 id="Actions-amp-Accessibility"><a href="#Actions-amp-Accessibility" class="headerlink" title="Actions &amp; Accessibility"></a>Actions &amp; Accessibility</h2><p>动作空间为16个离散化动作，包括八种移动动作对应八个方向、三种传球方向（Short, High, Long）、一种射门动作（Shot）、冲刺动作（Sprint，会影响球员体力值）、停止移动动作（Stop-Moving）、停止冲刺动作（Stop-Sprint）以及不进行动作（Do-Nothing）。</p><p>环境可以用于直接进行玩家和玩家之间的对抗，也可以dueling algorithms。同时游戏可以使用键盘或者gamepad进行。另外replays of several rendering qualities在训练时会被自动保存，便于研究者进行观察。</p><h2 id="随机性"><a href="#随机性" class="headerlink" title="随机性"></a>随机性</h2><p>游戏具有两种模式，可以是随机的或者是确定的。随机性在于同样的状态同样的action可能的导致不同的后果，而确定的模式在同样的策略和同样的状态下总是得到相同的结果。</p><h2 id="API-amp-Performance"><a href="#API-amp-Performance" class="headerlink" title="API &amp; Performance"></a>API &amp; Performance</h2><p>这套Engine是和OpenAI Gym的API兼容的，也就是RL中常见的<code>reset()</code>以及<code>obs, reward, done, info = step(action)</code> 那一套接口，以后有空或许可以对其做一个简单的记录。</p><p>整个Engine是写在经过大量优化的C++代码上的，可以使用GPU进行渲染。实验中在单机16核的机器（Intel Xeon E5-1650 v2 CPU3.5GHz）上每天能够跑25M个step。</p><h1 id="Football-Benchmarks"><a href="#Football-Benchmarks" class="headerlink" title="Football Benchmarks"></a>Football Benchmarks</h1><blockquote><p>Similar to the Atari games in the Arcade Learning Environment, in these tasks, the agent has to interact with a fixed environment and maximize its episodic reward by sequentially choosing suitable actions based on observations of the environment.</p></blockquote><h2 id="Algorithms"><a href="#Algorithms" class="headerlink" title="Algorithms"></a>Algorithms</h2><p>Football Benchmarks的游戏目标是对抗Engine提供的opponent bot取得全场比赛的胜利。同样的，这些benchmarks被分为easy medium 以及 hard三个level。文章采取了三个现阶段比较常用的算法来cover不同的研究场景。PPO用来模拟单机多进程的训练；IMPALA则采用了集群，500个actor的setting；以及Ape-X DQN。这几个算法未来几天有时间可以研究一下。</p><h3 id="IMPALA"><a href="#IMPALA" class="headerlink" title="IMPALA"></a>IMPALA</h3><p>原文地址：<a href="https://arxiv.org/abs/1802.01561" target="_blank" rel="noopener">IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architecture</a></p><p>该算法将learning和acting进行了解耦：单机worker不是将当前policy的gradient传回，而是将experience trajectories传输给center learner，而为了解决off-policy的问题，IMPALA提出了一种actor-critic的更新方法V-trace。本文采用了500个actor，Adam optimizer，进行500M step的训练。</p><h3 id="PPO"><a href="#PPO" class="headerlink" title="PPO"></a>PPO</h3><p>原文地址：<a href="https://arxiv.org/abs/1707.06347" target="_blank" rel="noopener">Proximal Policy Optimization Algorithms</a></p><p>该算法是一个online policy gradient算法，它优化一个clipped surrogate objective。本文实验采用了OpenAI的baseline，在16个并行worker上进行实验。同时采用了CNN。</p><h3 id="Ape-X-DQN"><a href="#Ape-X-DQN" class="headerlink" title="Ape-X DQN"></a>Ape-X DQN</h3><p>原文地址：<a href="https://arxiv.org/abs/1803.00933" target="_blank" rel="noopener">Distributed Prioritized Experience Replay</a></p><p>Ape-X DQN是一个高度scalable的DQN版本，和IMPALA相同的是，该算法也将learning和acting解耦，但是它采用了distributed replay buffer和Q-learning variant consisting of dueling network architectures &amp; double Q-learning。很多超参和IMPALA设置得相同（为了更好比较）。</p><h2 id="Reward"><a href="#Reward" class="headerlink" title="Reward"></a>Reward</h2><p>文章提出了两种设置reward的方法，分别为SCORING和CHECKPOINT。SCORING方法就是全场胜负进行+1/-1的奖励反馈。CHECKPOINT是为了解决sparsity问题而提出的。首先将对手的场地划分为10个区域，越接近对手的球门就说明越有利，当一名球员带球穿越region时就会获得+0.1的reward。</p><blockquote><p>First time our player steps into one region with the ball, the reward coming from that region and all previously unvisited further ones will be collected. In to- tal, the extra reward can be up to +1, the same as scoring a goal. To avoid penalizing an agent that would not go through all the checkpoints before scoring, any non-collected checkpoint reward is added to the scoring reward. Checkpoint rewards are only given once per episode.</p></blockquote><p>文章指出，在绝大部分的representation下这种reward的奖励方式是非马尔科夫的，这种CHECKPOINT的奖励设置方法基于我们自己的domain knowledge：越靠近球门越容易进球。</p><h1 id="Football-Academy"><a href="#Football-Academy" class="headerlink" title="Football Academy"></a>Football Academy</h1><p>文章总共提出了11个mini-games：</p><ul><li>Empty Goal Close. 玩家在box中起始，面对空门将球打进。</li><li>Empty Goal. 玩家在场地中间起始，面对空门将球打进。</li><li>Run to Score. 玩家在场地中间起始带球，身后有5名对手追，面对空门将球打进.</li><li>Run to Score with Keeper. 在Run to Score基础上加上Keeper.</li><li>Pass and Shoot with Keeper. 带球者在远端，有人防守；另一玩家在center，无人防守面对门将，需将球打进。</li><li>Run, Pass and Shoot with Keeper. 在Pass and Shoot基础上交换防守者的位置（不带球的玩家有人防守）。</li><li>3 versus 1 with Keeper. 前场三打一，有门将。</li><li>Corner. 标准角球设置，除了允许开角球者进行带球。</li><li>Easy Counter-Attack. 四打一防守，其余无关球员都向球跑动。</li><li>Hard Counter-Attack. 四打二防守。</li><li>11 versus 11 with Lazy Opponents. 全场游戏，只是对手站着不动，只会拦截离自己距离较近的球。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;源github地址：&lt;a href=&quot;https://github.com/google-research/football&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;google football research github page&lt;/a&gt;&lt;/
      
    
    </summary>
    
    
      <category term="RL" scheme="http://www.shihaizhou.com/tags/RL/"/>
    
      <category term="paper" scheme="http://www.shihaizhou.com/tags/paper/"/>
    
  </entry>
  
</feed>
