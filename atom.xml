<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>In Love with CodeCode</title>
  
  <subtitle>Haizhou&#39;s Blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.shihaizhou.com/"/>
  <updated>2019-07-31T16:12:25.935Z</updated>
  <id>http://www.shihaizhou.com/</id>
  
  <author>
    <name>Haizhou Shi</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Google Football Warm Start</title>
    <link href="http://www.shihaizhou.com/2019/07/30/Google-Football-Warm-Start/"/>
    <id>http://www.shihaizhou.com/2019/07/30/Google-Football-Warm-Start/</id>
    <published>2019-07-30T03:42:26.732Z</published>
    <updated>2019-07-31T16:12:25.935Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h1><p>The analytical work on the <a href="https://github.com/google-research/football/" target="_blank" rel="noopener">google research football project</a> involved in this article is completed during the internship at Intel. Thanks again to <em>Qiyuan Gong &amp; Shengsheng Huang</em> for their kind guidance and mentoring, and also many thanks to those who helped me during this period of time. </p><p>About the project description, please refer to the previous post <a href="https://shihaizhou.com/2019/06/12/Google-Research-Football-RL-Environment/" target="_blank" rel="noopener">Google Research Football - RL Environment</a> on my personal website, which is the summary of the <a href="https://github.com/google-research/football/blob/master/paper.pdf" target="_blank" rel="noopener">Google Football paper</a>.</p><h1 id="Working-Environment-Setting"><a href="#Working-Environment-Setting" class="headerlink" title="Working Environment Setting"></a>Working Environment Setting</h1><p>Assuming that we have two local workstations <strong>W</strong> and <strong>L</strong> with Windows and linux installed respectively, one gateway server <strong>G</strong> and one remote server <strong>S</strong>, we now introduce some user-friendly developping tools before getting started. Note that the tools/softwares in this section are not necessary once you have alternatives. </p><p>For <strong>W</strong>: </p><ul><li><strong>Pycharm (professional):</strong> available once you have an edu email, which allows you to deploy, develop, debug remotely. <a href="https://www.jetbrains.com/pycharm/" target="_blank" rel="noopener">https://www.jetbrains.com/pycharm/</a></li><li><strong>MobaXTerm:</strong> powerful remote shell software. It can save your remote ssh session, without typing the password repeatedly. <a href="https://mobaxterm.mobatek.net/" target="_blank" rel="noopener">https://mobaxterm.mobatek.net/</a></li><li><strong>VNC viewer:</strong> will be used when there is need to run graphical programs on the remote server. <a href="https://www.realvnc.com/en/connect/download/viewer/" target="_blank" rel="noopener">https://www.realvnc.com/en/connect/download/viewer/</a></li></ul><p>For <strong>L</strong>:</p><ul><li><strong>oh my zsh:</strong> fancier version of the linux bash, multiple useful plugins for customizing your own bash experience. <a href="https://ohmyz.sh/" target="_blank" rel="noopener">https://ohmyz.sh/</a></li></ul><h2 id="SSH-Tunneling"><a href="#SSH-Tunneling" class="headerlink" title="SSH Tunneling"></a>SSH Tunneling</h2><p>If you are working in a big corporation like Intel, you will need to interact with the remote server through the gateway node, thus understanding SSH tunneling is a must. </p><p>SSH tunneling process is used for the packet forwarding. For example, when <strong>W</strong> cannot directly access the IP address of the remote server <strong>S</strong>, the SSH tunneling mechanisfm can create a process running on the gateway node <strong>G</strong> that listens to one port (on <strong>W</strong>) and forward any request to another port (on <strong>S</strong>). By doing so, we make <strong>W</strong> look like directly connected to the remote server <strong>S</strong>. </p><p>To start a SSH tunneling process, type in the following command on your local workstation:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -N -f -L &lt;port of L&gt;:&lt;IP of G&gt;:&lt;port of S&gt; &lt;username of S&gt;@&lt;IP of S&gt;</span><br></pre></td></tr></table></figure><p>in which the arguments represent different meanings:</p><ul><li><strong>-N:</strong> Do not execute a remote command.  This is useful for just forwarding ports.</li><li><strong>-f:</strong> Requests ssh to go to background just before command execution.</li><li><strong>-L:</strong> Specifies that connections to the given TCP port or Unix socket on the local (client) host are to be forwarded to the given host and port, or Unix socket, on the remote side.</li></ul><p>If you are working on <strong>W</strong>, then the MobaXTerm has a pretty good UI for you to set the tunneling configuration and you don’t need to use this command. The SSH tunneling exists everywhere if there is a requirement for connection through the gateway node. </p><p>For more information about SSH tunneling, please refer to the following websites/blogs. </p><ul><li><a href="http://www.ruanyifeng.com/blog/2011/12/ssh_port_forwarding.html" target="_blank" rel="noopener">SSH tunneling: how it works</a></li><li><a href="https://www.jianshu.com/p/8f262bc444f0" target="_blank" rel="noopener">PyCharm SSH tunneling</a> </li></ul><h1 id="Running-GFootball-Locally"><a href="#Running-GFootball-Locally" class="headerlink" title="Running GFootball Locally"></a>Running GFootball Locally</h1><p>Running Google Football locally doesn’t require any more techniques than mentioned in their <a href="https://github.com/google-research/football" target="_blank" rel="noopener">github repo</a>. First you need to prepare a Linux Environment (on your local workstation <strong>L</strong>). Then get the root authority and download the following packages by runninng ‘apt-get install’. </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install git cmake build-essential libgl1-mesa-dev libsdl2-dev libsdl2-image-dev libsdl2-ttf-dev libsdl2-gfx-dev libboost-all-dev libdirectfb-dev libst-dev mesa-utils xvfb x11vnc libsqlite3-dev glee-dev libsdl-sge-dev python3-pip</span><br></pre></td></tr></table></figure><p>The packages above are mainly for OpenGL rendering (including libgl1-mesa-dev and so on) and VNC remote rendering (including xvfb and x11vnc). There may be sometimes an error showing up indicating that some of the packages couldn’t be installed because the dependency tree can’t be resolved. When encounterd with that, I suggest that you choose another machine with clean environment and start the previous process all over again. </p><p>After the installation of the packages, we then install the google football by pip, to note here that the pip version matters (remember to downgrade it to 18.1), since the ‘parse-dependency-links’ option is not available for the higher version of pip. This installation issue lately has some update, for more information please refer to  <a href="https://github.com/google-research/football/issues/1" target="_blank" rel="noopener">this installation issue</a>. </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install pip==18.1</span><br><span class="line">pip install gfootball[tf_cpu]</span><br></pre></td></tr></table></figure><p>Finally run the simple example of the google football by playing the game yourself, in case that the OpenGL version is not compatible, I suggest adding some ENV parameter as follows. You can check <a href="https://github.com/google-research/football/issues/7" target="_blank" rel="noopener">this issue</a> for more information.  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MESA_GL_VERSION_OVERRIDE=3.2 MESA_GLSL_VERSION_OVERRIDE=150 python3 -m gfootball.play_game</span><br></pre></td></tr></table></figure><p>For the full doc of running google football, please visit the README file in the <a href="https://github.com/google-research/football" target="_blank" rel="noopener">official repo</a>. </p><h1 id="Running-GFootball-Remotely"><a href="#Running-GFootball-Remotely" class="headerlink" title="Running GFootball Remotely"></a>Running GFootball Remotely</h1><h2 id="X-Window-System"><a href="#X-Window-System" class="headerlink" title="X Window System"></a>X Window System</h2><p>When training the google football agent remotely on a server without rendering, there is nothing to worry about. But what if one wants to use the rendered pixels as the environment representation (observation), or wants to supervise the training process in a more intuitive way? Before diving into the solution/actual code to this problem, we need to understand how a Linux graphical system works, since it involves the rendering system. Please check <a href="http://en.wikipedia.org/wiki/X_Window_System" target="_blank" rel="noopener">X Window System</a> on wikipedia.</p><p>The X Window System (X11) is a windowing system for bitmap displays, commonly on Unix-like operating systems. X is of a typical <strong>C/S</strong> architecture, where there is a <strong>X server</strong> residing between the input/output hardware drives (keyboard, mouse, screen) and the user programs, which act as <strong>X clients</strong>. When a user program wants to display something on the screen, it has to pull up a request to the X server, and then the X server needs to find the frame buffer of the screen and execute the rendering commands sent from the client program, i.e., writing bits into the frame buffer. That’s all we need to know in this section. </p><p>When we are running google football on the workstation, the X server and the X client are both on the same machine. While when we try to run it remotely on the server <strong>S</strong> and want to see the rendering results on the local workstation <strong>W/L</strong>, the X server and the X client are sperate: X server on the local <strong>W/L</strong>, while the X client (google football) on the server <strong>S</strong>. </p><p>The all-important fact to know is that those packages needed by rendering should be installed where rendering happens, namely, where the X server resides. </p><h2 id="Local-Linux-System"><a href="#Local-Linux-System" class="headerlink" title="Local Linux System"></a>Local Linux System</h2><p>If the local workstation is installed with the Linux system, then we could use the <a href="https://www.jianshu.com/p/24663f3491fa" target="_blank" rel="noopener"><strong>x11 forwarding mechanism</strong></a>. Which means if you install all the packages on your local workstation, you will make it. </p><p>The x11 forwarding techniques are not tested in this article. Hopefully you will succeed. </p><h2 id="Local-Windows-System"><a href="#Local-Windows-System" class="headerlink" title="Local Windows System"></a>Local Windows System</h2><p><strong><em>Forget it! the MobaXTerm x11 forwarding mechanism doesn’t work!</em></strong> If you try to do so, you will receive a confusing error message which can’t be traced:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;Couldn&apos;t load GL function glBegin: Video subsystem has not been initialized.&quot;</span><br></pre></td></tr></table></figure><p>On Windows system, we could use some simulated X server tools (such as <a href="https://sourceforge.net/projects/xming/" target="_blank" rel="noopener">Xming</a>) to receive the rendering requests from the remote server, and the x11 feature provided by MobaXTerm follows this idea. But the fact that the google football requires so many packages that could only be installed on Linux system makes it impossible to use the x11 forwarding mechanism. </p><p>The basic idea to solve this problem is to put the X server back to the server <strong>S</strong>. Instead of letting the X server write the screen bits into the frame buffer, we use a portion of the hard drive space, pretending that it’s the frame buffer of the screen, and then forward the image to the local workstation <strong>W</strong> via network. </p><h3 id="1-Set-up-the-virtual-frame-buffer-amp-message-forwarding"><a href="#1-Set-up-the-virtual-frame-buffer-amp-message-forwarding" class="headerlink" title="1. Set up the virtual frame buffer &amp; message forwarding"></a>1. Set up the virtual frame buffer &amp; message forwarding</h3><p>First, login to the remote server and install the google football package (as shown in the previous section). </p><p>Then run the following two commands in the shell:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Xvfb :1 -screen 0 800x600x24 &amp;</span><br><span class="line">x11vnc -display :1 -allow &lt;IP of W&gt; -autoport 5900 &amp;</span><br></pre></td></tr></table></figure><p>The first command <code>Xvfb</code> stands for <a href="https://www.x.org/releases/X11R7.6/doc/man/man1/Xvfb.1.xhtml" target="_blank" rel="noopener">X Virtual FrameBuffer</a>. The <code>:1</code> option denotes that this process starts a No.1 server, and the <code>-screen</code> option takes in 2 arguments: screen number and WxHxD, where D represents the color depth (measured by bits) of the screen. </p><p>The second command <code>x11vnc</code> (<a href="https://linux.die.net/man/1/x11vnc" target="_blank" rel="noopener">x11vnc man page</a>) starts a process forwarding the dumped screen output to a certain port, where <code>-display</code> option should be binded with the previously executed <code>Xvfb</code> command; and <code>-allow</code> specifies the IP address allowed; The command will automatically search a port available from a certain port which is specified by  <code>-autoport</code> option. </p><p>You need to remember the listening port number for future use, here we have 5900. </p><p><img src="https://i.loli.net/2019/07/31/5d413b31caa7e82895.png" alt></p><h3 id="2-Set-up-the-SSH-tunneling"><a href="#2-Set-up-the-SSH-tunneling" class="headerlink" title="2. Set up the SSH tunneling"></a>2. Set up the SSH tunneling</h3><p>With the port 5900 set up, we can now forward the message to our local Windows machine. Run the following command: </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -N -f -L 6666:&lt;IP of G&gt;:5900 &lt;username of S&gt;@&lt;IP of S&gt;</span><br></pre></td></tr></table></figure><p>To note here port 6666 is just a random local access point that you can designate at will for later VNC viewer. </p><h3 id="3-VNC-viewer-connection"><a href="#3-VNC-viewer-connection" class="headerlink" title="3. VNC viewer connection"></a>3. VNC viewer connection</h3><p>Open the VNC viewer. Type in <code>localhost:6666</code> and set up the VNC connection. If the connection is built, there will be a warning console popping up as shown in the following figure. Click ‘continue’ and you will be all set. </p><p><img src="https://i.loli.net/2019/07/31/5d413f7ba712732902.png" alt></p><h3 id="4-Google-Football-Training"><a href="#4-Google-Football-Training" class="headerlink" title="4. Google Football Training"></a>4. Google Football Training</h3><p>The training code is provided by google team in the file <code>run_ppo.py</code>. We should designate the exact screen it should be rendered by setting the <code>DISPLAY</code> environment paramter: </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env DISPLAY=:1.0 MESA_GL_VERSION_OVERRIDE=3.2 MESA_GLSL_VERSION_OVERRIDE=150 python3 -m gfootball.examples.run_ppo2 --dump_full_episodes=True --level=academy_run_to_score_with_keeper --render=True</span><br></pre></td></tr></table></figure><p>And the final result is shown. One thing to note here is that the refresh rate of the rendered picture is pretty low, therefore don’t get your expectation too high.</p><p><img src="https://i.loli.net/2019/07/31/5d413eda6288d55623.png" alt></p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>Up to now we have conducted one experiment to reimplement the Shooting Bronze mini-game in FIFA18. The performance of the <a href="https://arxiv.org/abs/1707.06347" target="_blank" rel="noopener">PPO algorithm</a> is actually pretty good while it’s still sometimes influenced by the large training variance. </p><h2 id="Setting"><a href="#Setting" class="headerlink" title="Setting"></a>Setting</h2><p>The football academy provided by the google research team has a very similar setting called <code>academy_run_to_score_with_keeper.py</code> in the <code>scenarios</code> folder, with the starting point of the player set in the middle of the field. By setting the ball nearer to the goal, and with some minor modification of the players’ position, we can reimplement the FIFA Shooting Bronze scenario in the google football. </p><h3 id="Original-Setting-of-academy-run-to-score-with-keeper-py"><a href="#Original-Setting-of-academy-run-to-score-with-keeper-py" class="headerlink" title="Original Setting of academy_run_to_score_with_keeper.py"></a>Original Setting of <code>academy_run_to_score_with_keeper.py</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">builder.SetBallPosition(<span class="number">0.02</span>, <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set current team to home</span></span><br><span class="line">builder.SetTeam(Team.e_Left)</span><br><span class="line">builder.AddPlayer(<span class="number">-1.0</span>, <span class="number">0.0</span>, e_PlayerRole_GK)</span><br><span class="line">builder.AddPlayer(<span class="number">0.0</span>, <span class="number">0.0</span>, e_PlayerRole_CB)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set current team to away</span></span><br><span class="line">builder.SetTeam(Team.e_Right)</span><br><span class="line">builder.AddPlayer(<span class="number">-1.0</span>, <span class="number">0.0</span>, e_PlayerRole_GK)</span><br><span class="line">builder.AddPlayer(<span class="number">0.12</span>, <span class="number">0.2</span>, e_PlayerRole_LB)</span><br><span class="line">builder.AddPlayer(<span class="number">0.12</span>, <span class="number">0.1</span>, e_PlayerRole_CB)</span><br><span class="line">builder.AddPlayer(<span class="number">0.12</span>, <span class="number">0.0</span>, e_PlayerRole_CM)</span><br><span class="line">builder.AddPlayer(<span class="number">0.12</span>, <span class="number">-0.1</span>, e_PlayerRole_CB)</span><br><span class="line">builder.AddPlayer(<span class="number">0.12</span>, <span class="number">-0.2</span>, e_PlayerRole_RB)</span><br></pre></td></tr></table></figure><h3 id="Shooting-Bronze-Reimplementation"><a href="#Shooting-Bronze-Reimplementation" class="headerlink" title="Shooting Bronze Reimplementation"></a>Shooting Bronze Reimplementation</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">builder.SetBallPosition(<span class="number">0.52</span>, <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set current team to home</span></span><br><span class="line">builder.SetTeam(Team.e_Home)</span><br><span class="line">builder.AddPlayer(<span class="number">-1.0</span>, <span class="number">0.0</span>, e_PlayerRole_GK)</span><br><span class="line">builder.AddPlayer(<span class="number">0.5</span>, <span class="number">0.0</span>, e_PlayerRole_CF)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set current team to away</span></span><br><span class="line">builder.SetTeam(Team.e_Away)</span><br><span class="line">builder.AddPlayer(<span class="number">-1.0</span>, <span class="number">0.0</span>, e_PlayerRole_GK)</span><br><span class="line">builder.AddPlayer(<span class="number">-0.3</span>, <span class="number">0.2</span>, e_PlayerRole_LB)</span><br><span class="line">builder.AddPlayer(<span class="number">-0.3</span>, <span class="number">0.1</span>, e_PlayerRole_CB)</span><br><span class="line">builder.AddPlayer(<span class="number">-0.3</span>, <span class="number">0.0</span>, e_PlayerRole_DM)</span><br><span class="line">builder.AddPlayer(<span class="number">-0.3</span>, <span class="number">-0.1</span>, e_PlayerRole_CB)</span><br><span class="line">builder.AddPlayer(<span class="number">-0.3</span>, <span class="number">-0.2</span>, e_PlayerRole_RB)</span><br></pre></td></tr></table></figure><p>One crucial thing to understand here is that the setting procedure is imperative (like pyplot and matlab). The coordinate axis of the ball is absolute, while the coordinate axis of the team is relative, which is shown in the following figure. </p><p><img src="https://i.loli.net/2019/07/31/5d414fba1ec3261321.png" alt></p><h2 id="Parallelism-Performance"><a href="#Parallelism-Performance" class="headerlink" title="Parallelism Performance"></a>Parallelism Performance</h2><p>We tested the performance (accuracy, speed) of the single-machine PPO algorithm. The PPO algorithm runs the episodes parallelly, and then collect all the traces to form a pool and then run the SGD update for several times. The parallelism of the PPO on google football is tested on a workstation with an Intel i9-7900 CPU, the result of which is shown in the following figure, where FPS stands for the total steps per second and the yellow line stands for the standard deviation of the FPS. </p><p><img src="https://i.loli.net/2019/07/31/5d4151f05527d87911.png" alt></p><p>The reason why the number of the environments could be larger than the number of the physical cores and still gain performance improvement is that the PPO implemented by <a href="https://github.com/openai/baselines" target="_blank" rel="noopener">OpenAI baselines</a> package is totally synchronous, which means that the environment stepping and the parameter updating is performed completely sequentially. The future work of the google football is highly likely to be deployed on distributed systems, making PPO algorithm and performance analysis less important. </p><h2 id="Accuray-Performance"><a href="#Accuray-Performance" class="headerlink" title="Accuray Performance"></a>Accuray Performance</h2><p>The pure PPO algorithm achieves a remarkablely high accuracy after 2M steps, sometimes with over 90% (or even 100%). While it may crash and get stuck at some local optimal point. The detailed record of the accuracy is lost, but you could check out the inference demo by clicking <a href="https://drive.google.com/file/d/1bNO5rpUhCeCZY9zPGgVCzgUlqH9QF39n/view?usp=sharing" target="_blank" rel="noopener">this link</a> on google drive. </p><h1 id="GFootball-Code-Analysis"><a href="#GFootball-Code-Analysis" class="headerlink" title="GFootball Code Analysis"></a>GFootball Code Analysis</h1><p>Personally I don’t think diving too deep into the google football environment code is a fantastic idea, for the detailed implementation should always be hidden from us users, unless there are some bugs you can’t wait to be fixed by the developers and you want to DIY (then I guess you must be super super gooood).</p><p>In this section, we will start with the overall structure of the google football project. Then we will list and explain all the exposed interfaces that you might need when developing your own algorithms. Finally we will cover some valuable issues. </p><h2 id="Overall-Structure"><a href="#Overall-Structure" class="headerlink" title="Overall Structure"></a>Overall Structure</h2><p>First of all, the whole project is composed of 2 major parts: <strong>football engine</strong> and <strong>football environment</strong>. The football engine is written in C++ because it involves the OpenGL rendering and should be highly optimized to improve the responsiveness. The football environment is written in Python and is wrapped gym-likely (<a href="https://gym.openai.com/" target="_blank" rel="noopener">OpenAI gym</a>) for better compatibility with the machine learning community. When installing the google football package, it will first compile the football engine and generate a dynamic library called <code>gfootball_engine.so</code>, which could be invoked by Python file using <code>import</code> statement (for more information about how to write the C++/Python Interface, please check the <a href="https://www.boost.org/doc/libs/1_70_0/libs/python/doc/html/index.html" target="_blank" rel="noopener">boost.Python</a> C++ library).</p><p><img src="https://i.loli.net/2019/07/31/5d41ae603455946761.png" alt></p><p>Our main focus is the gfootball environment construction, which is implemented in the directory <code>/gfootball/env</code>. The stream of creating the final environment object <code>FootballEnv</code> (in <code>/gfootball/env/football_env.py</code>) goes through at least three wrappers: </p><ul><li>First,<code>FootballEnvCore</code> (in <code>/gfootball/env/football_env_core.py</code>) interacts directly with the football engine, creates in total 11+11 <code>controllers</code> for every player, and most importantly, realizes the gym-like environment API including <code>reset</code> and <code>step</code>. </li><li>Secondly, <code>FootballEnvWrapper</code> (in <code>/gfootball/env/football_env_wrapper.py</code>) follows the same API standard and implements additional ‘writing_dumps’ function.</li><li>Finally, <code>FootballEnv</code> inherits the <code>FootballEnvWrapper</code>‘s API and loads the player algorithm by <code>_constructin_players</code>. </li></ul><p>After that, multiple wrappers could be used for a much more customized environment, as shown in the function <code>create_single_environment</code> in the file <code>/gfootball/env/__init__.py</code>. For more informationn about the wrappers, please check out the classes in <code>/gfootball/env/wrappers.py</code>.</p><p>The overall structure of the code is demonstrated in the following figure. To note that there might be some changes in the latest release, it’s better for you to check the newest version of the code and not get trapped in this article. </p><p><img src="https://i.loli.net/2019/07/31/5d41ae5fa113687245.png" alt></p><h2 id="Interfaces"><a href="#Interfaces" class="headerlink" title="Interfaces"></a>Interfaces</h2><h3 id="1-General-Configuration"><a href="#1-General-Configuration" class="headerlink" title="1. General Configuration"></a>1. General Configuration</h3><p>The configuration of the environment could be found in file <code>/gfootbal/env/config.py</code> and``. The important configuration paramters that might be used in the future include (you should double check it yourself):</p><ul><li><strong>action_set</strong>: a string deciding allowed actions. The dictionary defining the string-action_set mapping resides in file <code>/gfootball/env/football_action_set.py</code>.</li><li><strong>dump_full_episodes</strong>: a bool deciding whether dumps the full episodes to the disk when training. </li><li><strong>game_difficulty</strong>: a real number from 0 to 1 indicating the responsiveness of the bot, {easy: 0.05, medium: 0.6, hard: 0.95}.</li><li><strong>level</strong>: a string of the scenario name deciding which <code>scenarios/</code> file should imported. </li><li><strong>real_time</strong>: a bool ddeciding whether it’s real time for human player. </li></ul><h3 id="2-Scenario-Configuration"><a href="#2-Scenario-Configuration" class="headerlink" title="2. Scenario Configuration"></a>2. Scenario Configuration</h3><p>It’s implemented in file <code>/gfootball/env/scenario_builder.py</code>, but should be used with the <code>scenario_builder</code> class as shown in any file in the <code>/gfootball/env/scenarios/</code>.  There mainly interfaces that you will use when creating your own customized scenarios:</p><ul><li><strong>SetFlag(name, value)</strong>: directly invoke <code>config.set_scenario_value()</code> , which is defined in the <code>Config</code> class. The related parameters are listed as follows (all of them):<ul><li><em>deterministic</em>: whether there is some stochasticity.</li><li><em>end_episode_on_score</em>: if score then end the episode. </li><li><em>end_episode_on_possession_change</em>: if possession of the ball is changed, then end the episode.</li><li><em>end_episode_on_out_of_play</em>: if out of play, then end the episode.</li><li><em>game_duration</em>: how many steps of the environment before ending it forcely.</li><li><em>offsides</em>: whether offsides judegement is introduced to the games. </li></ul></li><li><strong>SetTeam(team)</strong>: set current team (left/right).</li><li><strong>SetBallPosition(ball_x, ball_y)</strong>: set the position of the ball. </li><li><strong>AddPlayer(x, y, role)</strong>: add a player to the field. </li></ul><p>One typical way of using this interface is to rewrite a scenario builder as <code>/gfootball/scenarios/11_vs_11_easy_stochastic.py</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> . <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_scenario</span><span class="params">(builder)</span>:</span></span><br><span class="line">  builder.SetFlag(<span class="string">'game_duration'</span>, <span class="number">3000</span>)</span><br><span class="line">  builder.SetFlag(<span class="string">'game_difficulty'</span>, <span class="number">0.05</span>)</span><br><span class="line">  builder.SetFlag(<span class="string">'deterministic'</span>, <span class="literal">False</span>)</span><br><span class="line">  <span class="keyword">if</span> builder.EpisodeNumber() % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">    first_team = Team.e_Left</span><br><span class="line">    second_team = Team.e_Right</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    first_team = Team.e_Right</span><br><span class="line">    second_team = Team.e_Left</span><br><span class="line">  builder.SetTeam(first_team)</span><br><span class="line">  builder.AddPlayer(<span class="number">-1.000000</span>, <span class="number">0.000000</span>, e_PlayerRole_GK)</span><br><span class="line">  builder.AddPlayer(<span class="number">0.000000</span>,  <span class="number">0.020000</span>, e_PlayerRole_RM)</span><br><span class="line">  builder.AddPlayer(<span class="number">0.000000</span>, <span class="number">-0.020000</span>, e_PlayerRole_CF)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.422000</span>, <span class="number">-0.19576</span>, e_PlayerRole_LB)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.500000</span>, <span class="number">-0.06356</span>, e_PlayerRole_CB)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.500000</span>, <span class="number">0.063559</span>, e_PlayerRole_CB)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.422000</span>, <span class="number">0.195760</span>, e_PlayerRole_RB)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.184212</span>, <span class="number">-0.10568</span>, e_PlayerRole_CM)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.267574</span>, <span class="number">0.000000</span>, e_PlayerRole_CM)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.184212</span>, <span class="number">0.105680</span>, e_PlayerRole_CM)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.010000</span>, <span class="number">-0.21610</span>, e_PlayerRole_LM)</span><br><span class="line">  builder.SetTeam(second_team)</span><br><span class="line">  builder.AddPlayer(<span class="number">-1.000000</span>, <span class="number">0.000000</span>, e_PlayerRole_GK)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.050000</span>, <span class="number">0.000000</span>, e_PlayerRole_RM)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.010000</span>, <span class="number">0.216102</span>, e_PlayerRole_CF)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.422000</span>, <span class="number">-0.19576</span>, e_PlayerRole_LB)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.500000</span>, <span class="number">-0.06356</span>, e_PlayerRole_CB)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.500000</span>, <span class="number">0.063559</span>, e_PlayerRole_CB)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.422000</span>, <span class="number">0.195760</span>, e_PlayerRole_RB)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.184212</span>, <span class="number">-0.10568</span>, e_PlayerRole_CM)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.267574</span>, <span class="number">0.000000</span>, e_PlayerRole_CM)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.184212</span>, <span class="number">0.105680</span>, e_PlayerRole_CM)</span><br><span class="line">  builder.AddPlayer(<span class="number">-0.010000</span>, <span class="number">-0.21610</span>, e_PlayerRole_LM)</span><br></pre></td></tr></table></figure><h3 id="3-Player-Interface"><a href="#3-Player-Interface" class="headerlink" title="3. Player Interface"></a>3. Player Interface</h3><p>The player interface is well defined and easy to implement. Just all you need to do is to load your own trained model and reimplement the <code>take_action</code> method as the rest of the player files residing in the folder <code>/gfootball/env/players</code>.</p><h3 id="4-Reward-Interface"><a href="#4-Reward-Interface" class="headerlink" title="4. Reward Interface"></a>4. Reward Interface</h3><p>The paper proposes two ways of giving reward to an agent, one of which is called ‘checkpoint’. The checkpoint reward wrapper <code>CheckpointRewardWrapper</code> is implemented in the file <code>/gfootball/wrappers.py</code> and it inherits from the <code>gym.RewardWrapper</code> class. The only thing that we need to do to customize our own reward mechanism is to implement <code>reset()</code> and <code>reward()</code> method. </p><h2 id="Related-Issues"><a href="#Related-Issues" class="headerlink" title="Related Issues"></a>Related Issues</h2><p>Several related issues were raised and (nicely) solved by the google football team. Firstly, <a href="https://github.com/google-research/football/tree/v1.1" target="_blank" rel="noopener">the latest release</a> supports multi-agent training and self-play. Secondly, about the FPS evaluation, the whole environment interacts with the agent in a synchronous way while it could be confusing when compared with playing with human, <a href="https://github.com/google-research/football/issues/41" target="_blank" rel="noopener">this issue</a> was solved just lately. Last, about <a href="https://github.com/google-research/football/issues/36" target="_blank" rel="noopener">not able to change the active player</a>, the only answer we got is that it’s not exposed to us users thus modifying it needs to dig down to the C++ engine. </p><h1 id="Last-Words"><a href="#Last-Words" class="headerlink" title="Last Words"></a>Last Words</h1><p>Thank you for reading this article. I would be super happy even if it only helps you a little bit! Wish you good luck! If you have any question, please do not hesitate to contact me via my email at the bottom! </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Preface&quot;&gt;&lt;a href=&quot;#Preface&quot; class=&quot;headerlink&quot; title=&quot;Preface&quot;&gt;&lt;/a&gt;Preface&lt;/h1&gt;&lt;p&gt;The analytical work on the &lt;a href=&quot;https://github
      
    
    </summary>
    
    
      <category term="notes" scheme="http://www.shihaizhou.com/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>AONLH(7) - Preflop Play (summary)</title>
    <link href="http://www.shihaizhou.com/2019/07/16/AONLH-7-Preflop-Play-summary/"/>
    <id>http://www.shihaizhou.com/2019/07/16/AONLH-7-Preflop-Play-summary/</id>
    <published>2019-07-16T12:03:15.000Z</published>
    <updated>2019-07-17T05:04:45.585Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>对整个翻前轮进行总结，原文中有推荐的范围，建议感兴趣的同学自己去看，这里更多地讨论原理性的内容，所以就不放上来了：</p><blockquote><p>When designing optimal preflop defending ranges, our main goal is to prevent the preflop raiser from being able to open wider than he should theoretically be able to. </p></blockquote><p>当我们设计翻前轮的防守范围时，我们主要的目标是不要让open raiser用过宽的范围open。</p><blockquote><p>We should be more likely to defend our hands by calling in position and re-raising out of position. </p></blockquote><p>当我们防守时，应该更多地在有利位置进行跟注，而在不利位置进行reraise。</p><blockquote><p>3-bet bluffs need folds around 67 to 70 percent of the time to show an immediate profit, and a player should defend around 40 to 50 percent of his 3-bets when facing a 4-bet. </p></blockquote><p>3-bet bluff需要在67-70%的情况下弃牌才有利可图；一名玩家需要在40-50%之间去防守自己的3-bet。</p><blockquote><p>4-bet bluffs need folds around 50 to 60 percent of the time to yield an immediate profit and should call between 50 to 60 percent of the time when facing a 5-bet jam. </p></blockquote><p>4-bet bluff需要在对手50-60%的频率下弃牌才能够获利，而需要50-60%的频率去跟注对手的5-bet all in。</p><blockquote><p>5-bet bluffs usually only need to succeed around 40 to 50 percent of the time to be profitable if they are randomized with the right bluffing hands. Usually, hands like ace-rag suited and low pocket pairs work best as 5-bet bluffs. </p></blockquote><p>5-bet bluff通常只需要在40-50%的频率成功就已经有利可图了，通常像A碎和小口袋对是非常好的5-bet bluff的手牌。</p><blockquote><p>Three-bets on average punish the worst hands in our opponent’s opening range more severely than calling does since calling allows him to see a flop. </p></blockquote><p>3-bet对于对手opening range中的最差手牌的打击比跟注要重因为跟注让我们的对手看见翻牌轮的结果。</p><blockquote><p>The expected value of 3-betting and 4-betting strong but not super premium hands like ace-king and queens in position is probably much lower against an optimal opponent than players expect. With these hands, many players are making a mistake by re-raising when they should call instead. </p></blockquote><p>3-bet或4-bet类似于<strong>AK</strong>和<strong>QQ</strong>这样的强牌而不是超强牌时，面对一个GTO玩家这么做的EV会比我们平时预期得要低。用这些手牌游戏时，跟注有时是个好的选择。</p><blockquote><p>We must make sure we use calling ranges which can connect on many different flop textures so our range will not be transparent. However, it’s also okay to use calling ranges which miss on low board textures since 8 high and lower boards are quite uncommon. </p></blockquote><p>我们必须要让我们的跟著范围能够在翻牌轮和牌面容易连接，这样能使得我们的range不会那么透明。而我们也可以使用和8以上的高张连接更加紧密的range作为我们的跟注范围，因为flop击中8-high以下的概率相当之小。</p><blockquote><p>Hand values can change drastically based on what range they are facing. For example, king-nine offsuit may do well as a cold call in the big blind against a button open since the opening range includes many weaker king-x and nine-x hands. But it may work poorly as a 3-bet since 3-betting makes many dominated hands fold, and out of position, marginal pairs are tough to play for a large pot against a strong range. </p></blockquote><p>手牌的价值随着它们面对的范围改变也会急剧变化。比方说<strong>K9o</strong>可能在<strong>BB</strong>对抗<strong>BTN</strong>位open被用作冷跟范围是一个很好的手牌因为对手的opening range中有非常多的<strong>Kx</strong>和<strong>9x</strong>的手牌；但是它被用来3-bet就不是一个很好的选择，因为3-bet会让非常多比你小的牌弃牌让比你大的牌跟注。在不利位置、小对子在面对大底池强范围的时候是非常难游戏的。</p><blockquote><p>A hand chart will not be perfect until the game of poker is actually solved. But they are certainly useful at identifying leaks and helping newer players. That is it’s more important to understand why hands go into certain ranges than to memorize a hand chart. </p></blockquote><p>Hand Chart在整个扑克问题被解决之前是不可能真正完美的。但是它能够非常好地被用来发现策略的漏洞并且帮助新手玩家快速入门。但真正重要的是理解为什么Hand Chart是这样被构建的，而不是简单地记住所有的手牌组合。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Summary&quot;&gt;&lt;a href=&quot;#Summary&quot; class=&quot;headerlink&quot; title=&quot;Summary&quot;&gt;&lt;/a&gt;Summary&lt;/h1&gt;&lt;p&gt;对整个翻前轮进行总结，原文中有推荐的范围，建议感兴趣的同学自己去看，这里更多地讨论原理性的内容，所以
      
    
    </summary>
    
    
      <category term="poker" scheme="http://www.shihaizhou.com/tags/poker/"/>
    
  </entry>
  
  <entry>
    <title>RL Course Lesson (2) - Model-Free Policy Evaluation</title>
    <link href="http://www.shihaizhou.com/2019/07/15/RL-Course-Lesson-2-Model-Free-Policy-Evaluation/"/>
    <id>http://www.shihaizhou.com/2019/07/15/RL-Course-Lesson-2-Model-Free-Policy-Evaluation/</id>
    <published>2019-07-15T07:49:57.000Z</published>
    <updated>2019-07-16T12:03:50.153Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Policy-Evaluation"><a href="#Policy-Evaluation" class="headerlink" title="Policy Evaluation"></a>Policy Evaluation</h1><p>本章主要解决如何在缺少MDP模型内部的转移函数的情况下进行Policy Evaluation。主要的方法为动态规划、蒙特卡洛（Monte Carlo Policy Evaluation）方法以及Temporal Difference（TD）。</p><p>首先回顾一下价值迭代用到的方法：</p><script type="math/tex; mode=display">V_{k}^{\pi}(s)=r(s,\pi(s)) + \gamma \sum_{s'\in S}{P(s'|s,\pi(s))V_{k-1}^{\pi}(s')}</script><p>其中$V_k^{\pi}(s)$是在策略$\pi$下的$k$-horizon value的精确值；而当$k$足够大的时候又是Infinite horizon value的估计值。Policy Evaluation的核心在于以下的数学期望表达式：策略$\pi$下的状态-价值=当前状态下Return值的数学期望。</p><script type="math/tex; mode=display">V^{\pi}(s)=E_\pi[G_t|s_t=s]</script><p>使用动态规划的Evaluation方法利用了公式：</p><script type="math/tex; mode=display">V^\pi(s)\approx E_\pi[r_t+\gamma V_{k-1}|s_t=s]</script><p>使用这个递推表达式有以下的约束和drawback：首先是公式中的$P(s’|s, \pi(s))$需要MDP Model $M$的具体条件转化概率矩阵；第二使用Bootstrapping方法要求整个过程是马尔科夫的（也就是和状态历史无关）。事实上在绝大部分情况下我们的agent都是不知道自己所处环境的状态转移关系$P$以及奖励模型$R$的，也就是这章要讲的Model-free Policy Evaluation。</p><h1 id="Monte-Carlo-Policy-Evaluation"><a href="#Monte-Carlo-Policy-Evaluation" class="headerlink" title="Monte Carlo Policy Evaluation"></a>Monte Carlo Policy Evaluation</h1><p>Policy Evaluation的目标是：对任意状态$s$，计算$V^{\pi}(s)=E_\pi[G_t|s_t=s]$。最简单的想法就是在当前的环境中能够进行采样，对每个采样的轨迹（trajectory）进行奖励的求和并对总体平均，这样就是一个对于$G_t$的estimate。该方法是一个通用方法，它不需要马尔科夫假设作为支撑，但它也只能被用于episodic MDP上，也就是说每个trajectory必须要有terminate的时候。通常MC方法都采用增量式的更新模式。</p><h2 id="First-Visit-MC-amp-Every-Time-MC"><a href="#First-Visit-MC-amp-Every-Time-MC" class="headerlink" title="First-Visit MC &amp; Every-Time MC"></a>First-Visit MC &amp; Every-Time MC</h2><p>First-Visit MC方法仅仅更新一个新采样Episode中第一次看到的状态的价值函数，这种方法是$V^\pi$的无偏估计。Every-Time MC方法每一次在sample中看到状态都会进行更新，虽然它是$V^\pi$的有偏估计，但是它依旧是一个consistent estimator并且比First-Visit MC具有更好的均方误差（MSE）。</p><h2 id="Variant-of-Every-Time-MC"><a href="#Variant-of-Every-Time-MC" class="headerlink" title="Variant of Every-Time MC"></a>Variant of Every-Time MC</h2><p>在Incremental MC方法当中需要维护一个数组$N(s)$来记录每个状态被访问更新的次数。定义$G_{i,t}$是在第$i$个episode的第$t$个time step的return值，我们将更新状态价值函数的表达式进行一些改写，将后面的项数看作是增量部分，那么$N(s)$就是每一次sample时更新的步幅，可以用$\alpha$表示：</p><script type="math/tex; mode=display">\begin{align}V^{\pi}(s) &= V^{\pi}(s){N(s)-1\over N(s)}+{G_{i,t}\over N(s)} \\&= V^\pi(s) + {1\over N(s)}[G_{i,t}-V^\pi(s)] \\&= V^\pi(s) + \alpha [G_{i,t}-V^\pi(s)]\end{align}</script><ul><li><p>当$\alpha={1\over N(s)}$时，更新的方法和Every-Visit MC相同。</p></li><li><p>当$\alpha\gt {1\over N(s)}$时，倾向于忘记之前的data sample，而对最近的sample较为敏感。这种更新策略对于Non-stationary domain的问题比较有效。</p></li></ul><h2 id="Key-Limitations-of-MC"><a href="#Key-Limitations-of-MC" class="headerlink" title="Key Limitations of MC"></a>Key Limitations of MC</h2><ul><li>首先MC是一个high-variance的方法，通常需要大量的sampled data才能够解决这个问题。</li><li>需要Episodic Setting，轨迹必须终结。</li></ul><h1 id="Temporal-Difference-Learning"><a href="#Temporal-Difference-Learning" class="headerlink" title="Temporal Difference Learning"></a>Temporal Difference Learning</h1><p>TD-learning是一个结合了MC方法和DP方法的Policy Evaluation策略，它采用了bootstrapping的思想，能够支持Infinite-Horizon的问题以及能够做到在每个step取得tuple $(s,a,r,s’)$后都直接进行更新。</p><p>MC方法中，我们用来更新价值的公式为：</p><script type="math/tex; mode=display">V^\pi(s) = V^\pi(s) + \alpha [G_{i,t}-V^\pi(s)]</script><p>我们知道$G$是状态价值函数的estimate，要拿到这个值必须要在episode结束之后。观察Bellman Equation的后半部分，这里的形式是利用MDP model的状态转移概率来写出精确的数学期望，但是在sampling的时候后面的部分会以sample的形式作为下一个状态和reward返回给agent，因此我们可以这个estimate来对Return值$G$进行bootsrappinng</p><script type="math/tex; mode=display">\begin{align}B^\pi V(s) &= r(s,\pi(s)) + \gamma \sum_{s'\in S}P(s'|s,\pi(s))V(s') \\V^\pi(s) &= V^\pi(s) + \alpha([r_t+\gamma V^\pi(s_{t+1})] - V^\pi(s))\end{align}</script><p>我们将TD error定义为更新的数值：</p><script type="math/tex; mode=display">\delta_t = r_t + \gamma V^\pi(s_{t+1}) - V^\pi(s_t)</script><h1 id="Evaluating-Algorithms"><a href="#Evaluating-Algorithms" class="headerlink" title="Evaluating Algorithms"></a>Evaluating Algorithms</h1><p>比较DP/MC/TD三个方法的性质：</p><div class="table-container"><table><thead><tr><th></th><th>DP</th><th>MC</th><th>TD</th></tr></thead><tbody><tr><td>Support Model-Free?</td><td></td><td>T</td><td>T</td></tr><tr><td>Support Non-episodic?</td><td>T</td><td></td><td>T</td></tr><tr><td>Support Non-Markov?</td><td></td><td>T</td><td></td></tr><tr><td>Unbiasd Estimate?</td><td></td><td>T (First-Visit MC)</td></tr></tbody></table></div><p><strong>评价RL算法的重要指标：是否是Biased Estimate？更新的方差大小？数据效率和计算效率？</strong></p><p>MC可以是无偏估计（取决于具体状态函数的更新形式），但该方法具有较高的variance；优点是即便在Function Approximation也能够收敛。</p><p>TD虽然是一个有偏估计，但是却有更小的variance，TD(0)只保证在Tabular Representation下收敛，而不保证在Function Approximation能够收敛。</p><h2 id="Batch-MC-amp-Batch-TD"><a href="#Batch-MC-amp-Batch-TD" class="headerlink" title="Batch MC &amp; Batch TD"></a>Batch MC &amp; Batch TD</h2><p>Batch solution for finite dataset是一种离线算法，能够提高data efficiency。对于已经sample得到的K个episodes，使用MC或TD算法对这K个episodes进行resample并更新参数即可构成该离线算法。</p><p>课程例举了一个实例来说明Batch MC和Batch TD的最终优化结果是不同的。</p><p>课程中认为Batch MC是收敛于min MSE，分析如下：给定一个确定的episode set，我们可以算出什么时候Batch MC的更新为0，也就是说最终每个状态的价值值会收敛到sample出的batch的该状态的平均Return值。这和优化min MSE的结果是相同的（优化函数的导数为0）。</p><script type="math/tex; mode=display">\begin{align}\alpha [\overline{G_{i,t}}-V^\pi(s)] &= 0 \\\Leftrightarrow V^\pi(s) &= \overline{G_{i,t}}\end{align}</script><p>而对于Batch TD(0)方法来说，它最终converge到的是“DP policy $V^\pi$ for the MDP with the maximum likelihood model estimates”.极大似概率的MDP model为以下：</p><script type="math/tex; mode=display">\begin{align}\hat{P}(s'|s,a) &= {1\over N(s,a)}\sum_{k=1}^{K}\sum_{t=1}^{L_k-1}1(s_{k,t}=s,a_{k,t}=a, s_{k,t+1}=s') \\\hat{r}(s,a) &= {1\over N(s,a)}\sum_{k=1}^{K}\sum_{t=1}^{L_k-1}1(s_{k,t}=s,a_{k,t}=a)r_{t,k}\end{align}</script><p>也就是说在给定的sample集上我们能够根据频率构建出关于MDP model的一个估计（利用如上公式），Batch TD(0)收敛到的点就是这个估计的DP policy。同样的分析最终的收敛点为：</p><script type="math/tex; mode=display">\begin{align}\alpha [\overline{r_t+\gamma V^\pi(s_{t+1})}-V^\pi(s)] &= 0 \\\Leftrightarrow V^\pi(s) &= \overline{r_t+\gamma V^\pi(s_{t+1})}\end{align}</script><p>而我们知道等式右边的平均项其实就是reward和带状态转移概率的价值函数的估计。</p><h2 id="Some-Important-Properties"><a href="#Some-Important-Properties" class="headerlink" title="Some Important Properties"></a>Some Important Properties</h2><p>对于TD和MC方法，更新的时间复杂度关于episode长度$L$来说都是$O(L)$。MC比较TD来说它的data efficiency更高一些，但是TD利用了马尔科夫性质。当我们解决具有马尔科夫性质的问题时，利用这个性质往往会比较有帮助。关于为什么TD利用了马尔科夫性质，我的理解是下面的近似替换表达式认为在当前time step的return value = 当前的immediate reward + 下一状态的state value是历史无关的，只和当前状态有关。</p><script type="math/tex; mode=display">G_{i,t}\approx r_t + \gamma V^\pi(s_{t+1})</script><h1 id="MC-Off-Policy-Evaluation"><a href="#MC-Off-Policy-Evaluation" class="headerlink" title="*MC Off-Policy Evaluation"></a>*MC Off-Policy Evaluation</h1><p>当前的policy evaluation都是基于sampling的，是on-policy的方法，on-policy简单来说就是在线跑算法并且在线evaluate。所以本章研究的是如何利用现有的data进行policy evaluation。现在formulate MC Off-Policy Evaluation的问题。</p><ul><li>Aim - estimate value of policy $\pi_1$, $V^{\pi_1}(s)$, givein episodes generated under behavior policy $\pi_2$.<ul><li>$s_1, a_1, r_1, s_2, a_2, r_2, \ldots$ where the actions are sampled from $\pi_2$</li></ul></li><li>If $\pi_2$ is stochastic, can often use it to estimate the value of an alternative policy (formal conditions to follow)</li><li>No requirement that have a model nor that state is Markov. </li></ul><p>Off-policy evaluation存在的主要问题就是两个不同的policy产生出episode和reward的distribution不同。重要性采样（Importance Sampling）的目标是估计一个函数$f(x)$在某一个概率分布$p(x)$下的数学期望，但是现在只有从另一个分布$q(s)$采样出的数据点$x_1,x_2,\ldots,x_n$。首先我们可以很容易地求出这个函数在$q(s)$上的数学期望。</p><script type="math/tex; mode=display">E_{x\sim q}[f(x)] = \int_xq(x)f(x)</script><p>假设$h_j$为所有sample点中的第$j$条轨迹：</p><script type="math/tex; mode=display">h_j = (s_{j,1}, a_{j,1}, r_{j,1}, s_{j,2}, a_{j,2}, r_{j,2}, \ldots,s_{j, L_j})</script><p>那么根据马尔科夫假设以及乘法原理，得出这条轨迹出现的概率为每个状态转移的乘积：</p><script type="math/tex; mode=display">p(h_j|\pi, s=s_{j,1}) = \prod_{t=1}^{L_j-1} \pi(a_{j,t}|s_{j,t})p(r_{j,t}|s_{j,t}, a_{j,t})p(s_{j,t+1}|s_{j,t},a_{j,t})</script><p>虽然整体问题是Model-Free的环境，但从上式得知，导致两个不同的policy分布的地方仅仅在于这策略本身决定action的部分。所以我们容易求出两个策略下轨迹的概率比值，从而得出需要的近似，其中$G(h_j)$为轨迹$h_j$的return value。</p><script type="math/tex; mode=display">V^{\pi_1}(s) \approx \sum_{j=1}^{n} {p(h_j|\pi_1,s)\over p(h_j|\pi_2,s)} G(h_j)</script><p>问题：课程中说这个推导过程不需要马尔科夫假设。但是上面将概率写成了只和当前状态相关的概率连乘积不是已经用到了马尔科夫假设了吗？需要进一步思考和验证。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Policy-Evaluation&quot;&gt;&lt;a href=&quot;#Policy-Evaluation&quot; class=&quot;headerlink&quot; title=&quot;Policy Evaluation&quot;&gt;&lt;/a&gt;Policy Evaluation&lt;/h1&gt;&lt;p&gt;本章主要解决如何在缺
      
    
    </summary>
    
    
      <category term="RL" scheme="http://www.shihaizhou.com/tags/RL/"/>
    
  </entry>
  
  <entry>
    <title>RL Course Lesson (1) - MP, MRP, MDP</title>
    <link href="http://www.shihaizhou.com/2019/07/10/RL-Course-Lesson-1-MP-MRP-MDP/"/>
    <id>http://www.shihaizhou.com/2019/07/10/RL-Course-Lesson-1-MP-MRP-MDP/</id>
    <published>2019-07-10T10:16:03.000Z</published>
    <updated>2019-07-15T07:59:48.346Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Markov-Property"><a href="#Markov-Property" class="headerlink" title="Markov Property"></a>Markov Property</h1><p>马尔科夫性质指的是状态转移的方程与历史状态无关而仅仅和当前状态相关。用数学表达马尔科夫状态$s_t$：</p><script type="math/tex; mode=display">p(s_{t+1}|s_t,a_t)=p(s_{t+1}|h_t,a_t)</script><h1 id="Markov-Process"><a href="#Markov-Process" class="headerlink" title="Markov Process"></a>Markov Process</h1><p>马尔科夫过程也可以被称作马尔科夫链，是一种具有马尔科夫性质的无记忆随机过程。马尔科夫过程由一个有限状态集$S$以及一个状态转移模型$P$构成，其中$P$指定了从某个特定状态转移到另一个状态的概率:</p><script type="math/tex; mode=display">P(s_{t+1}=s'|s_t=s)</script><p>需要注意的是在马尔科夫过程的设定中还没有reward或者action，这两项元素会逐步被加进来。如果状态集$S$是有限的，那么我们可以将$P$用一个矩阵表示：</p><script type="math/tex; mode=display">P = \left[\matrix{P(s_1|s_1) & P(s_2|s_1) & \ldots & P(s_N|s_1)\\                P(s_1|s_2) & P(s_2|s_2) & \ldots & P(s_N|s_2)\\                \vdots & \vdots & \ddots & \vdots \\                P(s_1|s_N) & P(s_2|s_N) & \ldots & P(s_N|s_N)\\}\right]</script><h1 id="Markov-Reward-Process"><a href="#Markov-Reward-Process" class="headerlink" title="Markov Reward Process"></a>Markov Reward Process</h1><p>马尔科夫奖励过程其实就是马尔科夫链（Markov Chain）加上rewards。马尔科夫奖励过程由以下四项元素组成：</p><ul><li>$S$：代表环境或者世界的一个有限状态集</li><li>$P$：一个状态转移模型，声明了$P(s_{t+1}=s’|s_t=s)$</li><li>$R$：一个奖励函数，声明了在某一状态下agent收到的奖励，$R(s_t=s)=E[r_t|s_t=s]$ </li><li>$\gamma$：对于未来奖励进行加权的折扣因子，$\gamma \in [0,1]$ </li></ul><h2 id="Parameter-Explanation"><a href="#Parameter-Explanation" class="headerlink" title="Parameter Explanation"></a>Parameter Explanation</h2><h3 id="Horizon"><a href="#Horizon" class="headerlink" title="Horizon"></a>Horizon</h3><p>注意这个名词很少在项目中被提到，简单来说它就是一个episode的time step数目。horizon可以是无限的，如果任意一个horizon都是有限的，那么我们可以将这样的MRP称作Finite MRP。</p><h3 id="Return"><a href="#Return" class="headerlink" title="Return"></a>Return</h3><p>Return $G_t$ 的中文译名我没有关注，在数学上的含义就是从某个时间步$t$开始沿着某个状态$s$的序列获得的奖励之和：</p><script type="math/tex; mode=display">G_t=r_t+\gamma r_{t+1}+\gamma^{2}r_{t+2}+\gamma^{3}r_{t+3}+\ldots</script><h3 id="State-Value-Function"><a href="#State-Value-Function" class="headerlink" title="State Value Function"></a>State Value Function</h3><p>状态$s$的状态价值函数指的是从状态$s$出发取得的return的期望值：</p><script type="math/tex; mode=display">V(s)=E[G_t|s_t=s]=E[r_t+\gamma r_{t+1}+\gamma^{2}r_{t+2}+\gamma^{3}r_{t+3}+\ldots|s_t=s]</script><h3 id="Discount-Factor"><a href="#Discount-Factor" class="headerlink" title="Discount Factor"></a>Discount Factor</h3><p>折扣因子是小于等于1的正数，这个参数的提出基于两个考虑。第一是能够带来数学上的便利：在其小于1时保证每个状态的状态价值函数的值保证收敛；第二就是符合人在行动时的常见模式：比起远处的reward人们更在意immediate reward。当$\gamma=0$时，意味着模型只考虑immediate reward，成为完全的贪婪模型；当$\gamma=1$时则适用于有限马尔科夫过程，认为未来和当前的reward有相同的重要性。</p><h2 id="Computing-State-Value-Function-for-MRP"><a href="#Computing-State-Value-Function-for-MRP" class="headerlink" title="Computing State Value Function for MRP"></a>Computing State Value Function for MRP</h2><h3 id="Via-simulation"><a href="#Via-simulation" class="headerlink" title="Via simulation"></a>Via simulation</h3><p>首先最直观的方法是通过模拟（simulation）的方法求得每个状态的state value，得到大量的episodes，将所有的return值进行加和求平均即可，<strong>这样做不需要整个环境是符合马尔科夫假设的</strong>。</p><h3 id="Via-matrix-equation"><a href="#Via-matrix-equation" class="headerlink" title="Via matrix equation"></a>Via matrix equation</h3><p>如果马尔科夫假设在应用场景中是合理的，那么我们可以写出一条state value之间的关系式，其含义是 状态$s$的value = 当前状态的immediate reward + 经过折扣的各个状态的价值期望。</p><script type="math/tex; mode=display">V(s)=R(s)+\gamma \sum_{s'\in S}{P(s'|s)V(s')}</script><p>因为对于所有的状态都成立，所以我们可以将上面针对某个状态的等式写成矩阵形式（依然只适用于有限马尔科夫过程）：</p><script type="math/tex; mode=display">\left[\matrix{V(s_1)\\ \vdots \\ V(s_N)}\right] = \left[\matrix{R(s_1)\\ \vdots \\ R(s_N)}\right] + \gamma \left[\matrix{P(s_1|s_1) & \ldots & P(s_N|s_1)\\                P(s_1|s_2) & \ldots & P(s_N|s_2)\\                \vdots & \ddots & \vdots \\                P(s_1|s_N) & \ldots & P(s_N|s_N)\\}\right] \left[\matrix{V(s_1)\\ \vdots \\ V(s_N)}\right]</script><p>更进一步地简化矩阵内部的元素，得到：</p><script type="math/tex; mode=display">V=R+\gamma PV</script><p>最终目标是算出$V$向量，通过矩阵解方程方法，得到：</p><script type="math/tex; mode=display">V=(I-\gamma P)^{-1}R</script><p>根据数值分析中的知识，我们知道矩阵求逆是$O(N^3)$的复杂度。</p><h3 id="Via-dynamic-programming"><a href="#Via-dynamic-programming" class="headerlink" title="Via dynamic programming"></a>Via dynamic programming</h3><p>其实我们可以通过递推的方式来解得最终的表示，不过我们需要证明整个递推表达式是收敛的。</p><script type="math/tex; mode=display">V_{k}(s)=R(s)+ \gamma \sum_{s'\in S}{P(s'|s)V_{k-1}(s')}</script><p>整个动态规划的算法为：</p><ul><li>for all s​，Initialize $V_0(s)=0$</li><li>for k = 1, until convergence<ul><li>for all s in S<ul><li>do update as mentioned</li></ul></li></ul></li></ul><p>每一次iteration的时间复杂度为$O(|S|^2)$，也就是$O(|N|^2)$。</p><h1 id="Markov-Decision-Process"><a href="#Markov-Decision-Process" class="headerlink" title="Markov Decision Process"></a>Markov Decision Process</h1><p>马尔科夫决策过程（MDP）是马尔科夫奖励过程（MRP）+actions。类似的，MDP由以下5组元素构成：</p><ul><li>$S$：代表环境或者世界的一个有限状态集，$s\in S$</li><li>$A$：代表agent能够执行的有限动作集，$a\in A$</li><li>$P$：一个状态转移模型。和MRP不同的是，MDP中的状态转移模型是一个同时关于当前状态$s$和当前行动$a$的函数，声明了$P(s_{t+1}=s’|s_t=s,a_t=a)$</li><li>$R$：一个奖励函数。同样的奖励函数也接受了动作$a$作为其中一个变量，声明了在某一状态下采取某一动作agent收到的奖励，$R(s_t=s,a_t=a)=E[r_t|s_t=s,a_t=a]$ 。需要注意的是这仅仅是这门课程中的表示方法，仔细想一下其实$R$是一个仅关于状态的函数也是正确的。</li><li>$\gamma$：对于未来奖励进行加权的折扣因子，$\gamma \in [0,1]$ </li></ul><p>综上我们可以将MDP简化写为一个五元组$(S,A,P,R,\gamma)$。</p><h2 id="MDP-Policy"><a href="#MDP-Policy" class="headerlink" title="MDP + Policy"></a>MDP + Policy</h2><p>对于给定的Policy，MDP会退化成一个MRP，因为给定的Policy导致了给定的状态转移概率分布，同时也导致了给定的奖励函数分布。</p><script type="math/tex; mode=display">\begin{align}R^\pi(s) &= \sum_{a\in A}\pi(a|s)R(s,a) \\P^\pi(s'|s) &= \sum_{a\in A}\pi(a|s)P(s'|s,a)\end{align}</script><p>以上的退化形式告诉我们能够利用MRP中的评价方法对于某个特定的policy $\pi$ 进行评价，即计算每个状态的价值函数。Iterative的方法递推公式如下所示，这被称作某个特定policy的Bellman Backup，关于Bellman Operator，我们现在只需要知道这是一个用于辅助证明迭代方法最终能够收敛的手段，在未来如果有机会可以进行更详细的了解。</p><script type="math/tex; mode=display">V_{k}^{\pi}(s)=r(s,\pi(s)) + \gamma \sum_{s'\in S}{P(s'|s,\pi(s))V_{k-1}^{\pi}(s')}</script><h1 id="MDP-Control"><a href="#MDP-Control" class="headerlink" title="MDP Control"></a>MDP Control</h1><p>MDP Control目的在于找到一个最优的策略$\pi^*$使得在该策略下的状态价值函数最大化。</p><script type="math/tex; mode=display">\pi^*(s)=\arg\max_{\pi}V^{\pi}(s)</script><p>课程中提到这种情况下一定存在一个唯一的最优价值函数，但这里还需要再进一步研究。对于Infinite Horizon problem，最优策略一定是deterministic &amp; stationary的，也就是最终会坍缩到不依赖于time step。</p><p>寻找最优的Policy即Policy Search有三种思想，除去最简单的Enumeration的暴力方法（如果是Deterministic Policy的话可以将所有的Policy都罗列出来，复杂度为$|A|^{|S|}$），最为重要的两种方法为策略迭代（Policy Iteration，PI）和价值迭代（Value Iteration，VI）。</p><h2 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h2><p>策略迭代的想法是找到最优的value &amp; policy。</p><p>定义State-Action Value $Q$，其含义为在当前状态$s$下采取动作$a$之后再follow策略$\pi$收获的价值：</p><script type="math/tex; mode=display">Q^\pi(s,a) = R(s,a) + \gamma \sum_{s'\in S}P(s'|s,a)V^\pi(s')</script><p>上述公式实际上是采取了未来一步的探索。将这条式子应用于迭代模型，对于当前迭代轮次$i$：每个$(s,a)$ pair都可以算出相应的$Q$值并且可以更新到下一个迭代轮次：</p><script type="math/tex; mode=display">\begin{align}Q^{\pi_i}(s,a) &= R(s,a) + \gamma \sum_{s'\in S}P(s'|s,a)V^{\pi_i}(s') \\\pi_{i+1}(s) &= \arg \max_{a} Q^{\pi_i}(s,a), \forall s \in S\end{align}</script><p>需要证明这样的更新方法最终是一定能够收敛到全局最优解的，也就是说需要证明这是一个Monotonic Improvement。定义策略价值函数的序关系之后再进行一些简单的放缩即可得到整个更新的过程是单调的。</p><script type="math/tex; mode=display">V^{\pi_1} \geq V^{\pi_2}:V^{\pi_1}(s) \geq V^{\pi_2}(s), \forall s \in S</script><p>因为策略更新是单调的，所以整个策略迭代的过程的停止终点就是策略的更新为零的时候，即：</p><script type="math/tex; mode=display">||\pi_i - \pi_{i-1}||_1=0</script><p>同时策略的更新次数是有界的，也就是在之前提到的Enumeration方法的复杂度$|A|^{|S|}$之内一定能够达到最优点。</p><h2 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h2><p>价值迭代的核心思想在于“Maintain optimal value of starting in a state $s$ if have a finite number of steps k left in the episode.”也就是说首先假设未来只有k步，通过迭代不断地将k往后推，从而达到拟合。</p><p>首先介绍Bellman Equation，一个策略的价值函数必须满足以下等式：</p><script type="math/tex; mode=display">V^{\pi}(s)=R^{\pi}(s)+\gamma\sum_{s'\in S}P^{\pi}(s'|s)V^{\pi}(s')</script><p>Bellman Operator也被称作Bellman Backup Operator。算子（Operator）就是作用在一个函数上将函数再进行一次映射。Bellman Operator作用在这里的value function上同时返回一个新的映射到所有状态的函数。</p><script type="math/tex; mode=display">BV(s)=\max_{a}R(s,a)+\gamma \sum_{s'\in S}P(s'|s,a)V(s')</script><p>Value Iteration的算法中的迭代部分公式为：</p><script type="math/tex; mode=display">V_{k+1}(s)=\max_{a}R(s,a)+\gamma \sum_{s'\in S}P(s'|s,a)V_{k}(s')</script><p>从整个迭代从Bellman Backup的角度来看，每一次迭代的过程就是做了以下的操作：</p><script type="math/tex; mode=display">V_{k+1}=BV_k\\\pi_{k+1}(s)=\arg \max_aR(s,a)+\gamma \sum_{s'\in S}P(s'|s,a)V_k(s')</script><p><strong>这里，每个迭代轮次$k$对应的价值$V_k$其实代表了从当前状态向后行动$k$个时间步的最大价值。</strong>对于FInite Horizon问题来说迭代不需要等到Converge，只需要运行最大Horizon步长即可。</p><p>回到策略迭代，同样可以用Bellman Operator来表达策略迭代的过程，$B^\pi$是针对于某个特定的策略$\pi$的Bellman算子：</p><script type="math/tex; mode=display">B^{\pi}V(s)=R^{\pi}(s)+\gamma \sum_{s'\in S}P^{\pi}(s'|s)V(s)</script><p>Policy Evaluation相当于计算$B^\pi$的不动点，只需要不断地进行Bellman Operator的变换直到$V$不再变化即可。</p><script type="math/tex; mode=display">\begin{align}V^\pi &= B^\pi B^\pi \ldots B^\pi V\\\pi_{k+1}(s) &= \arg\max_{a}R(s,a)+\gamma \sum_{s'\in S} P(s'|s,a)V^{\pi_k}(s')\end{align}</script><p>下面介绍Bellman Operator是一个压缩算子（Contraction）。压缩算子$O$指的是对于任何形式的norm，都有以下不等式成立。如果$V’$是上次迭代的结果，那么该不等式是迭代过程收敛的一个充分条件，表明最终的结果会收敛到一个不动点。</p><script type="math/tex; mode=display">|OV-OV'|\leq|V-V'|</script><p>可以证明当折扣因子$\gamma &lt; 1$，Bellman Operator是一个压缩算子，因此说明我们的迭代过程是收敛的。证明过程如下：</p><script type="math/tex; mode=display">\begin{align}||BV_k - BV_j||&=||\max_a{(R(s,a)+\gamma \sum_{s'\in S}P(s'|s,a)V_k(s'))} - \max_{a'}{(R(s,a')+\gamma \sum_{s'\in S}P(s'|s,a)V_j(s'))}||\\&\leq ||\max_a(R(s,a)+\gamma \sum_{s'\in S}P(s'|s,a)V_k(s') - R(s,a)-\gamma \sum_{s'\in S}P(s'|s,a)V_j(s'))|| \\&= ||\max_a \gamma \sum_{s' \in S}P(s'|s,a)(V_k(s')-V_j(s'))||\\&\leq ||\max_a (\gamma \sum_{s' \in S}P(s'|s,a)||V_k(s')-V_j(s')||)||\\&=\gamma||V_k - V_j||\end{align}</script><h2 id="VI-v-s-PI"><a href="#VI-v-s-PI" class="headerlink" title="VI v.s. PI"></a>VI v.s. PI</h2><p>VI算法依次计算Horizon为$k$的最优值，PI则被用于计算一个策略的Infinite Horizon值，并用于选择一个更好的策略，和未来即将介绍的RL重要算法Policy Gradient有非常紧密的联系。</p><h2 id="Policy-Improvement"><a href="#Policy-Improvement" class="headerlink" title="Policy Improvement"></a>Policy Improvement</h2><p>最后再formulate一下利用Policy Iteration进行Policy Improvement的具体流程：</p><p>首先计算当前策略$\pi_i$的$Q$值：</p><script type="math/tex; mode=display">Q^{\pi_i}(s,a) = R(s,a) + \gamma \sum_{s'\in S}P(s'|s,a)V^{\pi_i}(s')</script><p>因为有以下不等式成立：</p><script type="math/tex; mode=display">\begin{align}\max_aQ^{\pi_i}(s,a)&=\max_aR(s,a)+\gamma \sum_{s'\in S}P(s'|s,a)V^{\pi_i}(s')\\&\geq R(s,\pi_i(s))+\gamma \sum_{s'\in S}P(s'|s,\pi_i(s))V^{\pi_i}(s')\\&= V^{\pi_i}(s)\end{align}</script><p>意味着state-action value是严格非减的，通过迭代就能够不断优化策略。定义下一个迭代轮次$i+1$的策略：</p><script type="math/tex; mode=display">\pi_{i+1}(s) = \arg\max_aQ^{\pi_i}(s,a),\forall s\in S</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Markov-Property&quot;&gt;&lt;a href=&quot;#Markov-Property&quot; class=&quot;headerlink&quot; title=&quot;Markov Property&quot;&gt;&lt;/a&gt;Markov Property&lt;/h1&gt;&lt;p&gt;马尔科夫性质指的是状态转移的方程与历
      
    
    </summary>
    
    
      <category term="RL" scheme="http://www.shihaizhou.com/tags/RL/"/>
    
  </entry>
  
  <entry>
    <title>AONLH(6) - Preflop Play (part 5)</title>
    <link href="http://www.shihaizhou.com/2019/06/24/AONLH-6-Preflop-Play-part-5/"/>
    <id>http://www.shihaizhou.com/2019/06/24/AONLH-6-Preflop-Play-part-5/</id>
    <published>2019-06-24T11:08:43.000Z</published>
    <updated>2019-07-13T16:37:44.731Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Defending-Enough-Against-3-bets"><a href="#Defending-Enough-Against-3-bets" class="headerlink" title="Defending Enough Against 3-bets"></a>Defending Enough Against 3-bets</h1><p>我们知道跟注对手的open会迫使我们防守一个更宽的范围，因为这使得我们的对手看到了翻牌。这一概念在我们open之后又防守3-bet时也是同样适用的，因为当我们跟注了对手的raise，他就看到了翻牌并且有机会击中强牌或者能够进行有利可图的bluff。这就迫使我们学习防守相比仅仅4-bet reraise更多的手牌组合。</p><p>假设我们以45%的频率进行<strong>BTN</strong>位的open，一个众多优秀选手采用的<strong>BTN</strong>位open range。当我们使用2.5BB来open并且我们的对手在<strong>BB</strong>位3-bet了9.5BB，他在冒着丢掉8.5BB的风险去赢得4BB。如我们之前章节的分析所述，我们需要在遇到3-bet时至少防守32%的频率。综合我们的open range：我们需要防守14.4%的频率</p><script type="math/tex; mode=display">0.144=0.32\times0.45</script><p>然而，14.4%的范围代表着我们仅仅采用4-bet的频率，现在我们需要找出多少更多的hands应当被加入到calling range中。事实上，<strong>BB</strong>的期望值为零的等式和上一章节中“Defending Enough Against Opens”的结果几乎相同。</p><blockquote><p>(EV when button folds)(frequency button folds) + (average EV when called)(frequency called) – (EV when 4-bet)(facing 4-bet frequency) = 0</p></blockquote><p>现在我们有两种方法解决这个问题。第一种是将我们典型的4-betting range作为已知值插入到公式中去求得<strong>BB</strong>在3-bet时被跟注的平均EV是多少才能保持整体bluff为0EV。另一种想法是先估计当我们跟注防守时<strong>BB</strong>3-bet bluff的EV，然后再去设计我们的防守范围。例如，我们假设<strong>BB</strong>会平均输掉5.5个BB当他的弱3-bet bluff（只是他范围内相对弱的）被跟注，那么我们可以算出他会平均从18BB的底池中取回3BB的筹码。这看起来非常弱，但事实上我们马上就会在接下来的hand chart中看到<strong>BB</strong>的3-bet范围中的最弱牌其实是真的非常弱。从而证实了我们假设的合理性。</p><p>另外，假设当我们在<strong>BTN</strong>位遇到3-bet，我们会以<strong>总共</strong>5%的范围进行4-bet。如果我们使用<strong>KK</strong>-<strong>JJ</strong>、<strong>AK</strong>以及bluff组成的范围进行小size的4-bet raise，这个范围将是合理的。这表示我们4-bet的条件概率（即面对3-bet后的频率）为11.1%。</p><script type="math/tex; mode=display">0.111={0.05\over0.45}</script><p>现在我们可以将这些频率插入之前的公式并求出我们<strong>BTN</strong>位受到3-bet时的calling range。</p><script type="math/tex; mode=display">4(1-X-0.111)-5.5X-8.5\times0.111=0</script><p>解得$X=0.275$。其中4表示当<strong>BTN</strong>弃牌的收益；$1-X-0.111$表示<strong>BTN</strong>弃牌的频率；-5.5表示当<strong>BTN</strong>跟注时的收益；$X$为待计算的<strong>BTN</strong>位跟注频率；0.111位<strong>BTN</strong> 4-bet的频率；-8.5为当<strong>BTN</strong>4-bet时盲位的收益。因此<strong>BTN</strong>需要额外（除4-bet的5%外）以跟注的方式防守他27.5%的open range。这意味着整体12.4%的防守范围。</p><script type="math/tex; mode=display">0.124=0.275\times0.45</script><p>我们可以重复上述过程：假设我们对手bluff被跟注时他只会平均失去4.5BB，那么我们需要用跟注额外防守30.7%的open range。故而综合4-bet和calling，我们需要在open range的40%左右进行防守。现在让我们真切地看一下在不同的open range下<strong>BTN</strong>位的3-bet calling range。虽然有人会说用<strong>QJo</strong>进行跟注比<strong>66o</strong>或是<strong>67s</strong>跟注更好，但是从数学角度来说，如果决策相近，他们的EV值不会相差太多。</p><p>接下来的表格表示了<strong>BTN</strong> raiser平跟<strong>BB</strong>3-bet的范围，其中假设<strong>BB</strong>3-bet范围中的最弱手牌被跟注时平均输掉5.5BB。</p><h3 id="Button-3-Bet-Calling-Range"><a href="#Button-3-Bet-Calling-Range" class="headerlink" title="Button 3-Bet Calling Range"></a>Button 3-Bet Calling Range</h3><div class="table-container"><table><thead><tr><th>Opening Range (%)</th><th>Percentage of Opening Range Called (%)</th><th>Percentage of All Possible Hands Called (%)</th><th>Possible Cold Calling Range</th></tr></thead><tbody><tr><td></td><td></td><td></td></tr></tbody></table></div><p>注意几乎所有的<strong>BTN</strong> flatting ranges都包含了<strong>AA</strong>。<strong>AA</strong>有非常强的阻挡作用因此对对手有非常稳健的胜率（我们几乎可以总是在flop和turn上进行c-bet，如果牌面非常threatening）。<strong>AA成强牌的Board往往不会让我们3-bet flatting range中的手牌成强牌，因此作者认为将AA放入3-bet的平跟范围会更容易获利。KK和QQ也能够拿来平跟3-bet，但是他们在flop上碰到A就会变得比较脆弱，因此慢玩它们会更具风险。</strong></p><p>和之前所提到的一样，一旦<strong>BTN</strong>位的玩家开始频繁open，那么<strong>BB</strong>的3-bet也有了更多获利的机会，<strong>BTN</strong>位玩家将很难阻止他用很弱的手牌进行有效的bluff。这也就表示<strong>BTN</strong>的opening range不应当这么大，仅仅因为我们一旦用超过半数的手牌进行open，防守盲位玩家的挤压将变得非常困难。有一些玩家可能会不同意这一点，但那仅仅是因为他们有很好的翻后技术来应对这种情况。</p><p>在现实中，很多玩家会按照理论提示的相反方式，非常宽松地游戏<strong>BTN</strong>位，这仅仅是因为他们比对手的技术强太多了，另外，当你的对手处于有利位置而且技术又碾压你的时候，你就会更容易犯错误给对手更多的盈利机会。相似的，水平稍微差一些的玩家看到顶级玩家在<strong>BTN</strong>用70%的频率做open，于是也去模仿他们，没过多久整个poker community也就接受了这非理论最优的习惯。从众心理（Mob Mentality）一旦养成而没有独立思考的能力，一个玩家也就没有办法再提高自己了。</p><h1 id="Defending-Against-4-Bets"><a href="#Defending-Against-4-Bets" class="headerlink" title="Defending Against 4-Bets"></a>Defending Against 4-Bets</h1><p>翻前的一个困难的地方是如何去抵抗4bet。我们已经讨论过，在有利位置和不利位置用平跟去对抗4bet从动机和概念上说完全合理的，因此我们在这里不再重复这个思维过程。然而，决定如何去具体地应对4bet是一个难以处理的问题，而且往往没有一个明确的答案。</p><p>我们现在将尝试分析盲位玩家在3bet之后抵抗<strong>BTN</strong>玩家4bet的各种可能的方法。处理这个问题有两种通用的方法：</p><ul><li>设计我们的3bet范围，使得每一手牌在面对4bet时都能舒服地5bet all-in或弃牌。这使我们能够避免在不利位置游戏大底池，但也使得小注码的4bet对抗我们非常奏效。</li><li>对抗4bet时，用一些牌跟注，用另一些牌做5bet。这允许我们在对手给我们极好赔率的时候跟注，而且像<strong>KQs</strong>这样的牌在这种情况下非常适合游戏（因为我们不想弃牌又无法用它有效做5bet）。</li></ul><p>当我们跟注了4-bet，我们需要记住的是只要我们的后续EV大于-9.5BB就合格了，同时这允许我们在flop上以一个高频率进行check-fold，只要能够保持着大于-9.5BB就好了。如果我们在不利位置跟注了4-bet，我们将很难在flop轮上进行防守，对手很容易手持任意两张手牌进行bet。如果他当时的4-bet确实是bluff并且被我们的4-bet跟注，本身他已经投入了19.5BB的筹码，所以让他在post-flop上进行有利可图的bluff是一件可以接受的事，因为如果他激进地用弱牌bluff还是一个-EV的操作。（这一段我是基本按照原文翻译下来的，我想了很久这里的情况应该只有我们处于盲位，才有可能“在不利位置跟注对手的4-bet”，这里贴上原文）</p><blockquote><p>After calling a 4-bet out of position, we are likely not going to be able to prevent our opponent from being able to profitably bet any two cards on the flop. When his 4-bet bluff is called, he’ll have already invested 19.5 big blinds preflop, so allowing him to make a profitable bet post-flop with any two cards is not a problem. That’s because he’ll still lose money by 4-bet bluffing too weak of hands preflop despite the fact that he can often make profitable bluffs post-flop.</p></blockquote><p>最后，如果我们的对手注意到了我们4-bet calling range太弱，他就会在flop轮上频繁地下注。和我们在之前章节中谈到的那样，这个时候慢玩强牌例如<strong>AA</strong>就会比5-bet更加有利可图。</p><p>虽然我们现在已经非常了解4-bet跟注范围背后的数学理论知识，实际应用场景中会有一些关乎范围设计的问题值得引起我们的注意。首先就是即使我们使用了非常激进的3-betting range，我们能够在4-bet flatting range以及5-bett中能够用上的手牌也非常有限。这就要求我们在两个范围（有的时候甚至还需要融合进folding range）中复用一些手牌组合。这会加大我们的难度。比方说当我们在盲位用<strong>AQo</strong> 3-bet了<strong>BTN</strong>的open并且<strong>BTN</strong>玩家采用4-bet回应，如果这时跟注对手的4-bet，<strong>AQo</strong>是一个在flop轮上难以实现自身Equity的手牌典型，就会导致频繁的被剥削；而考虑到<strong>AQo</strong>阻挡了非常多的对手5-bet calling range中的组合（包括<strong>AA</strong>、<strong>AKo+</strong>、<strong>KK</strong>），我们用它来做5-bet也会很好（做跟注则是考虑绝对牌力）。因此两种想法是没有错对之分的。</p><p>类似的，如果我们的4-bet跟注范围充斥了太多的像<strong>AJ</strong>、<strong>KQ</strong>之类的组合，那么就太有迹可循了，这会让我们的策略过于透明从而让我们的对手能够很好地针对我们。最后我们必须记住不论是我们的对手还是我们自己都没有办法采用理论最优的策略进行游戏。基于此，你觉得哪一种玩家在4-bet底池更容易犯错误呢？是在有利位置用两极化策略游戏的玩家还是在不利位置用紧缩范围游戏的玩家？很显然一个polarized range要比condesed range容易游戏很多，这启示我们除非我们对自己的post-flop的技术非常自信，不然还是尽量避免在不利位置进行4-bet flatting。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Defending-Enough-Against-3-bets&quot;&gt;&lt;a href=&quot;#Defending-Enough-Against-3-bets&quot; class=&quot;headerlink&quot; title=&quot;Defending Enough Against 3-bet
      
    
    </summary>
    
    
      <category term="poker" scheme="http://www.shihaizhou.com/tags/poker/"/>
    
  </entry>
  
  <entry>
    <title>AONLH(5) - Preflop Play (part 4)</title>
    <link href="http://www.shihaizhou.com/2019/06/21/AONLH-5-Preflop-Play-part-4/"/>
    <id>http://www.shihaizhou.com/2019/06/21/AONLH-5-Preflop-Play-part-4/</id>
    <published>2019-06-21T01:47:33.000Z</published>
    <updated>2019-06-23T14:44:51.126Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Designing-Defending-Ranges-to-Maximize-EV"><a href="#Designing-Defending-Ranges-to-Maximize-EV" class="headerlink" title="Designing Defending Ranges to Maximize EV"></a>Designing Defending Ranges to Maximize EV</h1><p>虽然之前的章节告诉我们如何去衡量盲位玩家的防守范围是合理的，但是并没有教会我们哪些hands应该被纳入到range当中。新手玩家通常采用以上策略的一种过度简化版本：使用（范围内的）强牌raise for value，中等牌力call，下等牌力进行raise bluff，最差的手牌fold。这样会带来以下问题：</p><ul><li>这会鼓励对手仅仅根据自己手牌的equity进行准确的牌力排序。</li><li>这没有考虑到对手的范围会随着line that we take进行相应的变化从而影响我们手牌的equity。因此，最适宜平跟的hands不一定比最适宜bluff的hands的equity更高。</li></ul><p>因为一个名GTO玩家永远采取EV最高的line，所以我们应当采取在那些range内表现好的3-betting和calling范围。简单的规则没有办法适应GTO玩家的复杂范围，这也就要求我们必须时刻意识到我们对手范围是如何组成的，以及我们的行动（calling, betting and raising）是如何影响对手的范围的。</p><p>让我们继续设计面对<strong>BTN</strong>位open的盲位防守范围。当我们作为盲位玩家跟注<strong>BTN</strong>位对手的open时，我们几乎总是在小底池内面对宽范围（play against a wide range for a small pot）。这也就意味着我们的calling range需要“put more emphasis on flatting with hands which make marginal pairs at a high frequency and can win small and medium sized pots”。显然的，calling range中常见的有<strong>KTs</strong>，<strong>K9s</strong>，<strong>QTs</strong>，<strong>QTs</strong>。当然范围中也需要其他的牌，但是所有的这些牌都能够在flop轮上能够成一个不错的对，从而很好地对抗wide open  range。</p><p>虽然这些手牌在小底池中的表现不错，但是用他们在3-bet底池中游戏会变得比较棘手。那是因为3-bet会导致我们对手的范围强很多而使得我们在不利位置用一个“顶对-中踢脚”游戏大底池变得异常困难。所以在preflop使用类似<strong>K9s</strong>或者<strong>QTo</strong>进行跟注往往是最好的选择，因为它们让对手的范围继续保持很宽同时在击中顶对时我们通常会有更好的踢脚。</p><p>另外，适合3-bet bluff的手牌通常具有在river上成为最强牌力的可能。比方说<strong>57s</strong>或者<strong>45s</strong>，因为他们在后续的翻牌中有可能击中顺子和同花，而拿着这样的牌我们几乎总是能够肯定自己拿的是nuts从而轻松实现自己的equity。通常来说我们会拿上面的牌力进行3-bet bluff，但是拿它们进行跟注也是有利可图的。但需要注意的是，虽然用小的同花连张（small suited connector）具有正的EV，但是当我们真的击中了弱对，又很难立即结束牌局获得盈利。</p><p>需要注意的是，当我们的对手具有位置优势（例如当前讨论的盲位对抗<strong>BTN</strong>）时，他通常会采用跟注的方式防守我们的3-bet，因为这会使得我们在一个不利位置和他游戏一个大底池。因此根据这一心理，我们的3-bet range需要能够在3-bet pot中表现良好。不幸的是，我们3-bet bluff range中并不是所有手牌都能轻松应对对手的跟注，因为按照上一章的分析，我们需要更加激进地3-bet位于<strong>BTN</strong>的对手，所以像<strong>K7s</strong>这样的手牌虽然可能会在post-flop轮上将我们引入困境，但是我们也应当用这样的手牌来平衡我们的range。虽然“顶对-弱踢脚”非常难以游戏，但是为我们的flop checking range保留一些marginal hands还是一定程度上有用的。</p><h1 id="Balance"><a href="#Balance" class="headerlink" title="Balance"></a>Balance</h1><p>德州扑克中最常被误解和被错误应用的概念莫过于“平衡”一个范围（balancing a range）。如果我们的范围不是平衡的，我们的对手就能够经常采取非常有效的line来对抗我们的range。一个平衡的范围其实是用GTO策略（即最大化EV）玩每一手牌的副产物。</p><p>不管我们的range如何平衡，总是有一些公共牌面纹理（Board Texture）对对手的range更加有利。比方说，如果我们的range平均比对手的range弱，那么他就能够更加频繁地下重注，因为他知道我们几乎很少成强牌。</p><blockquote><p>This encourages us to put hands in our preflop range which allow us to connect better on boards which would otherwise miss our range, and we’ll<br>occasionally win a massive pot when we have one of the few possible strong hands in an otherwise weak range.</p></blockquote><p>这一段的文字让我有些困惑，我能够理解更弱的range意味着击中强牌的概率更低，因此对手c-bet的概率更高，但同时一旦击中强牌就能够赢下一个大底池（因为这个事件的发生完全是基于频率的）；但是我不是非常理解“this encourages us to put hands in our preflop range which allow us to connect better on boards”这一段，如果对这一段文字有比较好的理解可以联系我进行交流。</p><p>现在让我们来看一下<strong>CO</strong>对抗<strong>BTN</strong>的实际情况。假设<strong>BTN</strong>按照大多数玩家做的那样在preflop上遇到了<strong>CO</strong>的open会100%频率3-bet<strong>AA</strong>和<strong>AK</strong>。而如果<strong>BTN</strong>玩家没有进行raise，那么在King High的这样的flop面<strong>BTN</strong>位玩家拿到强牌的概率就小了很多，于是<strong>CO</strong>玩家就能够在绝大多数的King High牌面进行c-bet迫使<strong>BTN</strong>位玩家弃牌。注意<strong>BTN</strong>拿到两对+Queen以上踢脚的概率是非常小的。</p><p>以上的挤压产生就意味着<strong>BTN</strong>现在有了改变自己策略的动机。因为对手在King High牌面的激进策略让底池变大，而如果<strong>BTN</strong>此时能够慢玩<strong>AKo+</strong>就能够获得比3-bet更高的收益。另外，通过使用强牌在preflop平跟有时能够赢下非常大的底池：因为有时盲位玩家会进行通过reraise挤压（这就恰恰掉进了我们的圈套）。</p><p>当<strong>CO</strong>意识到<strong>BTN</strong>在King High的牌面上范围中也有<strong>AKo+</strong>的手牌，他就会开始变得保守起来。<strong>你来我往的策略调整过程最终会将玩家们引向GTO策略并建立起相互之间的均衡（Equilibrium）</strong>。<strong>BTN</strong>虽然会经常在preflop轮3-bet自己的<strong>AKo+</strong>，但也会时不时地用它们进行平跟；而<strong>CO</strong>虽然会经常在King High面上激进c-bet，也会时不时地过牌给<strong>BTN</strong>位玩家进行先手行动。</p><p>当我们在<strong>SB</strong>的时候如果从来都不平跟强牌或许也会遇到上述问题，这样的话我们的范围在面对<strong>BB</strong>挤压时就会变得非常脆弱。注意<strong>BB</strong>并不会有这样的问题因为他如果跟注则游戏进入了flop轮。虽然<strong>BB</strong>冷跟的range是紧缩的，但是在<strong>BTN</strong>能够利用这一点之前，flop轮中又会经常出现<strong>BB</strong>击中了two pairs或者sets（the flop will usually put some two pairs and sets in the big blinds range before the button can capitalize on this）。</p><p>因此，我们在<strong>SB</strong>位的flatting range最好放一些强牌。我们没有办法去证明（数学上）这一定就是好的，而且通常遇到挤压时<strong>SB</strong>最好fold并且期待<strong>BTN</strong>位的玩家进行防守。但到目前为止在<strong>SB</strong>位时不时地慢玩<strong>AKo+</strong>是一个理论上正确的策略，尤其当<strong>BB</strong>玩家非常激进并且经常挤压时我们更应当这么做。</p><h1 id="Board-Texture-Frequencies"><a href="#Board-Texture-Frequencies" class="headerlink" title="Board Texture Frequencies"></a>Board Texture Frequencies</h1><p>在设计防守范围时另一个需要考虑的因素就是公共牌面纹理频率（Board Texture Frequencies），这个频率指的是某一种类型的flop公共牌面出现的频率。例如，没有击中King High牌面会比没有击中Seven High牌面更加成问题，因为King High牌面出现的概率要高很多。<strong>我们需要经常思考对方的range击中某一种牌面的概率，这一点是很重要的。</strong></p><p>以下表格展示了flop轮出现某种High Card的概率。Ace High毫无疑问是所有High Card牌面中最经常出现的，而在Eight High以下的牌面是会很少遇到的。</p><div class="table-container"><table><thead><tr><th style="text-align:center">High Card</th><th style="text-align:center">Single High (%)</th><th style="text-align:center">Double High (%)</th><th style="text-align:center">Total (%)</th></tr></thead><tbody><tr><td style="text-align:center">Ace</td><td style="text-align:center">20.4</td><td style="text-align:center">1.3</td><td style="text-align:center">21.7</td></tr><tr><td style="text-align:center">King</td><td style="text-align:center">17.1</td><td style="text-align:center">1.2</td><td style="text-align:center">18.4</td></tr><tr><td style="text-align:center">Queen</td><td style="text-align:center">14.1</td><td style="text-align:center">1.1</td><td style="text-align:center">15.2</td></tr><tr><td style="text-align:center">Jace</td><td style="text-align:center">11.4</td><td style="text-align:center">1.0</td><td style="text-align:center">12.4</td></tr><tr><td style="text-align:center">Ten</td><td style="text-align:center">9.0</td><td style="text-align:center">0.9</td><td style="text-align:center">9.9</td></tr><tr><td style="text-align:center">Nine</td><td style="text-align:center">6.8</td><td style="text-align:center">0.8</td><td style="text-align:center">7.6</td></tr><tr><td style="text-align:center">Eight or less</td><td style="text-align:center">-</td><td style="text-align:center">-</td><td style="text-align:center">14.8</td></tr></tbody></table></div><p>虽然大致理解公共牌面纹理频率是怎样的是有帮助的，但是我们不必要将他们都记下来，<strong>只需要注意到翻出J及J以上的flop占到了67.7%的频率就够了</strong>。正如我们看到的那样，盲位的冷跟范围经常没有办法击中Eight High及以下的flop牌面，这样设计主要是根据上面牌面的出现频率的，所以即便没有击中自己的range，由于它发生的频率非常低所以不需要太过在意。另外，在未来的章节中，我们将会看到在不利位置游戏这种low boards会非常困难。即便我们将更多小的同色连张例如<strong>7h6h</strong>（红桃76）放进我们的平跟范围，在<strong>6c3c2d</strong>（草花6、3和方片2）的牌面上处于不利位置也将会很难进行游戏。而这就意味着在盲位碰上这种牌面，check-fold或许成为了最好的办法。<strong>总而言之，我们需要将range设计得更加容易击中Jack High以上的Board Texture（因为它们出现的频率更高），而不要尝试加入小的连张以期在flop击中顺子。</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Designing-Defending-Ranges-to-Maximize-EV&quot;&gt;&lt;a href=&quot;#Designing-Defending-Ranges-to-Maximize-EV&quot; class=&quot;headerlink&quot; title=&quot;Designing 
      
    
    </summary>
    
    
      <category term="poker" scheme="http://www.shihaizhou.com/tags/poker/"/>
    
  </entry>
  
  <entry>
    <title>AONLH(4) - Preflop Play (part 3)</title>
    <link href="http://www.shihaizhou.com/2019/06/17/AONLH-4-Preflop-Play-part-3/"/>
    <id>http://www.shihaizhou.com/2019/06/17/AONLH-4-Preflop-Play-part-3/</id>
    <published>2019-06-17T08:33:22.000Z</published>
    <updated>2019-06-19T10:22:35.280Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Defending-Enough-Against-Opens"><a href="#Defending-Enough-Against-Opens" class="headerlink" title="Defending Enough Against Opens"></a>Defending Enough Against Opens</h1><p>我们现在已经可以根据之前讨论的概念来建立更加复杂的范围了。我们的最终目标是找到一个双方都没有任何偏离意图的“均衡”的策略，但是不幸的是preflop range是非常难达到这一点的，这是因为我们没有一个非常明确的起点（已知的正确范围）来帮助我们建立理论正确的范围。一个非常简单的例子：我们首先要知道<strong>BTN</strong>的opening range才有可能为盲位（<strong>SB/BB</strong>）设计防守open的范围；而相反的我们如果不知道盲位玩家3-bet和call的频率，我们又很难为<strong>BTN</strong>设计优秀的opening range。所以这就要求我们要采用更加“trial &amp; error”的方法来分析preflop play。</p><p>除此之外，改变策略的某个部分通常会影响到游戏的其他方面。例如说，假设我们想要更松的range去对抗对手的3-bet bluff从而让他bluff成功的概率变低，那么虽然这样做确实迫使对手在不利位置更多地游戏大底池，但同时也弱化了我们的范围从而提升了对手在post-flop上游戏弱手牌的EV。</p><p>为每个位置都采用同样的方法去计算一遍range会让这个过程变得非常冗长，所以这一章我们会先介绍方法论然后在最后给出一章Hand Chart来显示最终的结果。这里还需要重申，完美的范围是不可能被找到的，我们所能做到的只能是“均衡”的范围，即一旦双方都达到了这个范围，他们都不想再偏移自己的策略。</p><p>我们首先讨论盲位应该如何回应<strong>BTN</strong>的open raise。这个范围在6-max的牌桌上经常发生，重要性自然不言而喻（即便如此，在菜鸟牌桌上可能一整晚也遇不到一次）。假设<strong>BTN</strong>位的open size为2.5BB（注意在前面章节中我们已经提到，通常<strong>BTN</strong>位的玩家因为具有绝对的位置优势会做size小一些的open来鼓励对手入池）且3-bet的size为9.5BB，从而有<strong>BTN</strong>的open bluff（这里假定3-bet  bluff就是open range中的最差手牌）不能够在高于62.5%的情况下成功。</p><script type="math/tex; mode=display">1.5X-2.5(1-X)=0</script><p>另外，<strong>SB</strong>的3-bet bluff不能够在高于69.3%的情况下成功</p><script type="math/tex; mode=display">4X-9(1-X)=0</script><p>而<strong>BB</strong>的频率为68.0%。</p><script type="math/tex; mode=display">4X-8.5(1-X)=0</script><p>最后，我们假设当<strong>BTN</strong>玩家4-bet时他会使用19BB的size，虽然面对<strong>SB/BB</strong>的频率不同，但他们的值非常接近。我们可以计算出<strong>BTN</strong>的4-bet必须成功57%左右。</p><script type="math/tex; mode=display">12.5X-16.5(1-X)=0</script><p>检验一下我们以上得出的成果，看看他们是不是没有理论上的矛盾。</p><ul><li><strong>BTN</strong>的open raise不能够在高于62.5%的频率上成功，不然<strong>BTN</strong>位的玩家将会用任意的手牌进行open来赢取盲注的钱。</li><li>在<strong>BTN</strong>位open range中的最弱手牌在被盲位3-bet时会失去2.5BB。</li><li>两个盲位玩家进行3-bet防守的频率总和不能够超过37.5%。</li><li>两个盲位玩家如果只进行call防守，那么他们防守频率的总和要超过37.5%。</li><li>BTN opening range中的最差手牌无所谓进行open或者是fold。</li><li><strong>BB</strong>只会在自己手牌的EV高于-1时对open raise进行冷跟。</li><li><strong>SB</strong>只会在自己手牌的EV高于-0.5时对open raise进行冷跟。</li><li>Hands in the small and big blind flatting range will on average have a higher expected value against weaker hands than stronger hands in the button opening range. </li></ul><p><strong>BB</strong> calling range中的最差手牌无所谓call或者是fold，这意味着这手最弱手牌在面对<strong>BTN</strong>位的opening具有接近-1BB的EV。当<strong>BTN</strong>的opening range中的最弱手牌对于open或者fold无所谓而且当盲位的最弱手牌无所谓call或者是fold，我们会说双方玩家达到了一种均衡。</p><p>我们现在可以写一长串的数学公式（虽然没有办法被解决）来找到<strong>BTN</strong>位open的EV。然而，我们将用一个不那么精确但是更加实用的公式来找出BTN open和盲位defend的合理范围。</p><blockquote><p>(EV when both blinds fold) (frequency both blinds fold) + (average EV when called) (frequency called) - (EV when 3-bet) (facing 3-bet frequency) = 0</p></blockquote><p>上面的公式为书中的原文，但是其中有一点不严谨的地方，那就是EV when 3-bet这一栏的EV本身就是带符号的所以之前的-号应该还是保持+号。故而通用公式为：</p><blockquote><p>(EV when both blinds fold) (frequency both blinds fold) + (average EV when called) (frequency called) + (EV when 3-bet) (facing 3-bet frequency) = 0</p></blockquote><p>我们可以将我们已经知道的值插入上述这条公式，得到：</p><blockquote><p>(1.5)(frequency both blinds fold) + (average EV when called)(frequency called) – (2.5)(facing 3-bet frequency) = 0</p></blockquote><p>如果我们能够知道average EV of the worst hand when called那么这条式子就可解了。但是要知道这个值无异于要解决整个德州扑克游戏。虽然无解，但这条式子也有它有用的地方。比如我们可以先插入一些常见的range来解决这个未知的问题，如果这些range看起来合理，那么我们就可以拿它们来和一些强大的对手对抗，并在未来不断的对抗中refine自己的range。</p><p>首先让我们设计理论上合理的3-bet range。我们知道盲位合起来的3-bet频率必须低于37.5%，而且43%的3-bet在面对4-bet时必须进行防守。因此，一个不错的开头是设计一个5%的value bet hands（<strong>TT+，AJs+, AQo+</strong>）和7.5%的bluff。这样两个盲位各自都是12.5%的3-bet频率。</p><p>这是一个很多优秀玩家都在使用的标准3-bet范围。因为3-bet range中的40%是强牌，所以在整个3-bet range上防守43%将不再是一件难事：只需要混合进一些跟注和一些5-bet bluff就可以填充剩下3%的频率。如果盲位玩家都是12.5%的3-bet频率，那么<strong>BTN</strong>位的玩家总共会面对盲位玩家23.4%的3-bet。</p><script type="math/tex; mode=display">0.234=1-(0.875)^2</script><p>让我们同时假设<strong>SB</strong>和<strong>BB</strong>在没有3-bet时分别以10%和20%的频率跟注。这就意味着至少一名盲位玩家跟注的频率为24.3%，其中，0.875意味着在<strong>SB</strong>跟注后<strong>BB</strong>不进行3-bet的频率；0.775表示<strong>SB</strong>在遇到<strong>BTN</strong> open之后弃牌的频率。</p><script type="math/tex; mode=display">0.243=0.1\times0.875+0.2\times0.775</script><p>现在我们就有了足够的已知值来插入到我们先前的等式中。根据我们的盲位防守策略，我们可以计算出<strong>BTN</strong>位使用open range中的最弱牌进行open的EV。</p><script type="math/tex; mode=display">0.234(-2.5)+(0.243)X+(1-0.234-0.243)(1.5)=0</script><p>其中，$0.234(-2.5)$指的是<strong>BTN</strong>遭遇3-bet时的EV；$(0.243)(X)$为<strong>BTN</strong>遭遇跟注的EV；而$(1-0.234-0.243)(1.5)$为<strong>SB/BB</strong>均弃牌的EV。也就是说，<strong>BTN</strong>玩家open range中的最弱牌被跟注时的EV要维持在-0.82才能够让这手牌无所谓open或fold。</p><p>现在让我们来评估一下这个值的合理性。当<strong>BTN</strong>的open被任意一名盲位玩家跟注，<strong>BTN</strong>在post-flop面上处于有利位置，池内有5.5或者6个BB。这时均衡的范围使得<strong>BTN</strong>的EV为-0.82，也就是说<strong>BTN</strong>在这个底池中拿回1.68BB就算完成任务。现在的问题是：</p><p><strong>&gt;&gt;&gt; 这个-0.82BB的EV推算出的1.68BB，是一个合理的值吗？</strong></p><p>这里作者写到，虽然他也没有办法给出正确的值，但他认为1.68BB绝对不是一个合理的值。因为两个盲位玩家都具有相当松的calling range而同时<strong>BTN</strong>具有绝对的位置优势。而1.68BB意味着在被跟注后的<strong>BTN</strong>具有不到1/3的equity，也就导致<strong>BTN</strong>位的玩家可以用超级松的范围进行open而获得比1.68BB大的平均盈利。故而，这就使得作者相信当前盲位玩家需要比现在使用的数值3-bet/call地更加激进一些。</p><p>以上的几段论述非常好地示范了在受到我们能够理论证明的约束范围之内，虽然我们不能够得出确切的数值，我们却能够通过询问“这些范围是合理的吗”并结合实际的经验对理论得出的结论进行验证。理论的推演通常能够帮助我们看到“什么一定是错的”，而不是“什么一定是对的”。所以我们可以通过理论推演来逐渐优化我们的范围或者面对不同的敌人应用不同的策略。</p><p>你可能会问：“就算<strong>BTN</strong>有一个非常松的+EV open range，那又怎样呢？这不一定就代表着盲位玩家的防守范围太过紧缩，只是说明<strong>BTN</strong>位的位置优势就有这么大呀。”但是事实是，<strong>BTN</strong>位的玩家一旦开始非常宽松地进行open，那么他的open面对对手激进的3-bet将会变得非常脆弱（这一点将在未来的章节”Defending Enough Against 3-Bets”看到）。</p><p>最后，我们在本章中将两个盲位玩家的范围作了等效处理，但是要注意的是这只是为了使问题变成单变量的一种简化。在数学上更加娴熟的读者可以使用这本书的方法进行更加复杂的演算，而针对<strong>SB/BB</strong>更加成熟的冷跟范围也能够被轻松发现。在本章结束之前，让我们对两个盲位玩家采取不同的范围使用我们的公式。我们假设<strong>SB</strong>以16%的频率3-bet并且以8%的频率call，<strong>BB</strong>以14%的频率3-bet并以20%的频率冷跟。我们首先会发现两者3-bet的频率之和是27.8%。</p><script type="math/tex; mode=display">0.278=0.16+0.84\times0.14</script><p>另外，BTN被冷跟的频率为21.7%。</p><script type="math/tex; mode=display">0.217=0.08\times0.86+0.20\times0.74</script><p>这样的话我们解得<strong>BTN</strong>的opening range中的最弱牌EV为-0.29。</p><script type="math/tex; mode=display">0.278\times(-2.5)+0.217X+(1-0.278-0.217)\times1.5=0</script><p>当这手最弱牌被跟注时，<strong>BTN</strong>平均输掉0.29BB。<strong>BTN</strong>的opening range平均应当比盲位玩家的calling range弱，但因为位置优势<strong>BTN</strong>玩家又能够获得多一些的死钱，所以可以认为这个范围是相对合理的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Defending-Enough-Against-Opens&quot;&gt;&lt;a href=&quot;#Defending-Enough-Against-Opens&quot; class=&quot;headerlink&quot; title=&quot;Defending Enough Against Opens&quot;&gt;
      
    
    </summary>
    
    
      <category term="poker" scheme="http://www.shihaizhou.com/tags/poker/"/>
    
  </entry>
  
  <entry>
    <title>Google Research Football - RL Environment</title>
    <link href="http://www.shihaizhou.com/2019/06/12/Google-Research-Football-RL-Environment/"/>
    <id>http://www.shihaizhou.com/2019/06/12/Google-Research-Football-RL-Environment/</id>
    <published>2019-06-12T07:08:18.000Z</published>
    <updated>2019-06-13T02:53:35.142Z</updated>
    
    <content type="html"><![CDATA[<p>源github地址：<a href="https://github.com/google-research/football" target="_blank" rel="noopener">google football research github page</a></p><p>源paper地址：<a href="https://github.com/google-research/football/blob/master/paper.pdf" target="_blank" rel="noopener">google football research paper</a></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>文章提出了一个新的Google Research Football Environment，该环境是一个基于物理引擎的足球环境，非常容易迁移，并且是基于开源licence的。文章同时提出了三个不同难度的full-game sceario，提出了Football Benchmarks来标定模型的表现。同时文章测试了3个常用的强化学习模型（IMPALA, PPO, Ape-X DQN）。最后文章还提出了稍微简单一些的scenario，Football Academy。</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>文章的主要贡献为：</p><blockquote><ul><li>provide the Football Engine, a highly-optimized game engine that simulates the game of football,</li><li>propose the Football Benchmarks, a versatile set of benchmark tasks of varying difficulties that can be used to compare different algorithms,</li><li>propose the Football Academy, a set of progressively<br>harder and diverse reinforcement learning scenarios,</li><li>evaluate state-of-the-art algorithms on both the Football<br>Benchmarks and the Football Academy, providing an extensive set of reference results for future comparison, and</li><li>sprovide a simple API to completely customize and define<br>new football reinforcement learning scenarios.</li></ul></blockquote><h1 id="Football-Engine"><a href="#Football-Engine" class="headerlink" title="Football Engine"></a>Football Engine</h1><p>本文的Football Environment是基于另一个工作<em>GameplayFootball</em> simulator.引擎模拟了整个足球游戏，它接收来自两方球队的input action。该引擎实现了足球比赛的众多方面，包括开球，进球，犯规，角球，点球以及边线球。</p><h2 id="支持的足球规则"><a href="#支持的足球规则" class="headerlink" title="支持的足球规则"></a>支持的足球规则</h2><p>几乎所有的足球规则，甚至包括换人。游戏长度是按照frame来进行计算的，默认的整场游戏是3000 frames，但是这一点可以进行customize，初始player数量以及他们的位置也可以被调整。每方的球员会随着时间变长而疲倦，而每一方只能最多进行3次换人。</p><h2 id="内置AI对手"><a href="#内置AI对手" class="headerlink" title="内置AI对手"></a>内置AI对手</h2><p>内置的AI对手是rule based AI，由<em>GameplayFootball</em> simulator开发。困难指数$\theta$是通过调节对手的决策反应时间来影响对手的难度的。推荐的三个难度等级easy, medium, hard的$\theta$值分别为0.05, 0.6, 0.95，我们还能够将内置的AI对手换为我们自己的算法。然后文章介绍了我们最关心的问题：</p><blockquote><p>Moreover, by default, our non-active players are also con- trolled by another rule-based bot. In this case, the behav- ior is simple and corresponds to reasonable football actions and strategies, such as running towards the ball when we are not in possession, or move forward together with our active player. In particular, this type of behavior can be turned off for future research on cooperative multi-agents if desired.</p></blockquote><p>也就是说除了当前的player是自己控制的，其余的player现阶段是通过rule-based的方式被控制的，但是未来我们可以将这个特性关掉从而借助该环境进行multi-agents的研究。</p><h2 id="State-amp-Observation"><a href="#State-amp-Observation" class="headerlink" title="State &amp; Observation"></a>State &amp; Observation</h2><p>文章定义state为游戏当前的所有状态信息的集合（complete set of data returned by the environment after actions are performed），包括ball position/possession, coordinates of all players, the active player, game state(球员疲惫程度，黄牌，比分等等) and current pixel frame. </p><p>同时文章定义observataion指的是state进行任意一种转换后的结果，该结果是作为input传递给control algorithm的。文章提出了三种representation：</p><ul><li>Pixels： 1280$\times$720 RGB 图像</li><li>Super Mini Map：SMM由四个96$\times$72的矩阵组成，编码了包括了主队、客队、足球以及active player的信息。矩阵是binary的形式，简单来说就是bitmap，表征该位置上是否有上述的物体。</li><li>Floats：一个更加紧凑的representation，115维向量用于表征所有的比赛信息，包括players coordinates, ball possession and direction, active player, or game mode.</li></ul><h2 id="Actions-amp-Accessibility"><a href="#Actions-amp-Accessibility" class="headerlink" title="Actions &amp; Accessibility"></a>Actions &amp; Accessibility</h2><p>动作空间为16个离散化动作，包括八种移动动作对应八个方向、三种传球方向（Short, High, Long）、一种射门动作（Shot）、冲刺动作（Sprint，会影响球员体力值）、停止移动动作（Stop-Moving）、停止冲刺动作（Stop-Sprint）以及不进行动作（Do-Nothing）。</p><p>环境可以用于直接进行玩家和玩家之间的对抗，也可以dueling algorithms。同时游戏可以使用键盘或者gamepad进行。另外replays of several rendering qualities在训练时会被自动保存，便于研究者进行观察。</p><h2 id="随机性"><a href="#随机性" class="headerlink" title="随机性"></a>随机性</h2><p>游戏具有两种模式，可以是随机的或者是确定的。随机性在于同样的状态同样的action可能的导致不同的后果，而确定的模式在同样的策略和同样的状态下总是得到相同的结果。</p><h2 id="API-amp-Performance"><a href="#API-amp-Performance" class="headerlink" title="API &amp; Performance"></a>API &amp; Performance</h2><p>这套Engine是和OpenAI Gym的API兼容的，也就是RL中常见的<code>reset()</code>以及<code>obs, reward, done, info = step(action)</code> 那一套接口，以后有空或许可以对其做一个简单的记录。</p><p>整个Engine是写在经过大量优化的C++代码上的，可以使用GPU进行渲染。实验中在单机16核的机器（Intel Xeon E5-1650 v2 CPU3.5GHz）上每天能够跑25M个step。</p><h1 id="Football-Benchmarks"><a href="#Football-Benchmarks" class="headerlink" title="Football Benchmarks"></a>Football Benchmarks</h1><blockquote><p>Similar to the Atari games in the Arcade Learning Environment, in these tasks, the agent has to interact with a fixed environment and maximize its episodic reward by sequentially choosing suitable actions based on observations of the environment.</p></blockquote><h2 id="Algorithms"><a href="#Algorithms" class="headerlink" title="Algorithms"></a>Algorithms</h2><p>Football Benchmarks的游戏目标是对抗Engine提供的opponent bot取得全场比赛的胜利。同样的，这些benchmarks被分为easy medium 以及 hard三个level。文章采取了三个现阶段比较常用的算法来cover不同的研究场景。PPO用来模拟单机多进程的训练；IMPALA则采用了集群，500个actor的setting；以及Ape-X DQN。这几个算法未来几天有时间可以研究一下。</p><h3 id="IMPALA"><a href="#IMPALA" class="headerlink" title="IMPALA"></a>IMPALA</h3><p>原文地址：<a href="https://arxiv.org/abs/1802.01561" target="_blank" rel="noopener">IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architecture</a></p><p>该算法将learning和acting进行了解耦：单机worker不是将当前policy的gradient传回，而是将experience trajectories传输给center learner，而为了解决off-policy的问题，IMPALA提出了一种actor-critic的更新方法V-trace。本文采用了500个actor，Adam optimizer，进行500M step的训练。</p><h3 id="PPO"><a href="#PPO" class="headerlink" title="PPO"></a>PPO</h3><p>原文地址：<a href="https://arxiv.org/abs/1707.06347" target="_blank" rel="noopener">Proximal Policy Optimization Algorithms</a></p><p>该算法是一个online policy gradient算法，它优化一个clipped surrogate objective。本文实验采用了OpenAI的baseline，在16个并行worker上进行实验。同时采用了CNN。</p><h3 id="Ape-X-DQN"><a href="#Ape-X-DQN" class="headerlink" title="Ape-X DQN"></a>Ape-X DQN</h3><p>原文地址：<a href="https://arxiv.org/abs/1803.00933" target="_blank" rel="noopener">Distributed Prioritized Experience Replay</a></p><p>Ape-X DQN是一个高度scalable的DQN版本，和IMPALA相同的是，该算法也将learning和acting解耦，但是它采用了distributed replay buffer和Q-learning variant consisting of dueling network architectures &amp; double Q-learning。很多超参和IMPALA设置得相同（为了更好比较）。</p><h2 id="Reward"><a href="#Reward" class="headerlink" title="Reward"></a>Reward</h2><p>文章提出了两种设置reward的方法，分别为SCORING和CHECKPOINT。SCORING方法就是全场胜负进行+1/-1的奖励反馈。CHECKPOINT是为了解决sparsity问题而提出的。首先将对手的场地划分为10个区域，越接近对手的球门就说明越有利，当一名球员带球穿越region时就会获得+0.1的reward。</p><blockquote><p>First time our player steps into one region with the ball, the reward coming from that region and all previously unvisited further ones will be collected. In to- tal, the extra reward can be up to +1, the same as scoring a goal. To avoid penalizing an agent that would not go through all the checkpoints before scoring, any non-collected checkpoint reward is added to the scoring reward. Checkpoint rewards are only given once per episode.</p></blockquote><p>文章指出，在绝大部分的representation下这种reward的奖励方式是非马尔科夫的，这种CHECKPOINT的奖励设置方法基于我们自己的domain knowledge：越靠近球门越容易进球。</p><h1 id="Football-Academy"><a href="#Football-Academy" class="headerlink" title="Football Academy"></a>Football Academy</h1><p>文章总共提出了11个mini-games：</p><ul><li>Empty Goal Close. 玩家在box中起始，面对空门将球打进。</li><li>Empty Goal. 玩家在场地中间起始，面对空门将球打进。</li><li>Run to Score. 玩家在场地中间起始带球，身后有5名对手追，面对空门将球打进.</li><li>Run to Score with Keeper. 在Run to Score基础上加上Keeper.</li><li>Pass and Shoot with Keeper. 带球者在远端，有人防守；另一玩家在center，无人防守面对门将，需将球打进。</li><li>Run, Pass and Shoot with Keeper. 在Pass and Shoot基础上交换防守者的位置（不带球的玩家有人防守）。</li><li>3 versus 1 with Keeper. 前场三打一，有门将。</li><li>Corner. 标准角球设置，除了允许开角球者进行带球。</li><li>Easy Counter-Attack. 四打一防守，其余无关球员都向球跑动。</li><li>Hard Counter-Attack. 四打二防守。</li><li>11 versus 11 with Lazy Opponents. 全场游戏，只是对手站着不动，只会拦截离自己距离较近的球。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;源github地址：&lt;a href=&quot;https://github.com/google-research/football&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;google football research github page&lt;/a&gt;&lt;/
      
    
    </summary>
    
    
      <category term="RL" scheme="http://www.shihaizhou.com/tags/RL/"/>
    
      <category term="paper" scheme="http://www.shihaizhou.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>AONLH(3) - Preflop Play (part 2)</title>
    <link href="http://www.shihaizhou.com/2019/06/11/AONLH-3-Preflop-Play-continued/"/>
    <id>http://www.shihaizhou.com/2019/06/11/AONLH-3-Preflop-Play-continued/</id>
    <published>2019-06-11T15:33:05.000Z</published>
    <updated>2019-06-16T19:35:35.070Z</updated>
    
    <content type="html"><![CDATA[<h1 id="EV-of-4-Betting-or-Folding-Against-Opponents-Who-Don’t-Flat-4-Bets"><a href="#EV-of-4-Betting-or-Folding-Against-Opponents-Who-Don’t-Flat-4-Bets" class="headerlink" title="EV of 4-Betting or Folding Against Opponents Who Don’t Flat 4-Bets"></a>EV of 4-Betting or Folding Against Opponents Who Don’t Flat 4-Bets</h1><p>在stack为100BB的情况下，一般来说玩家都不喜欢在不利位置flat 3-bet（flat 3-bet和call 3-bet是同样的意思）。有一种说法是：<strong>永远不要（这一章节我们讨论的都是在preflop轮）在不利位置平跟3-bet，要么选择4-bet要么选择fold</strong>。还有一种说法是：<strong>面对4-bet时，如果你之前的3-bet是在value bet，那么就跟对手5-bet all in；而如果你之前的3-bet是在bluff，那么就弃牌吧，不要去call以免在flop交了厄运</strong>。这两种建议都不是特别好，但是确实帮助我们避免了在超大底池的情况下去做困难的决策，因此对于新手玩家来说是一个有用的建议。而因为flop的复杂情况，我们没有办法直接去比较面对4-bet时flat和5-bet all in的EV值。但是我们却可以分析一下“only 4-betting or folding against a 3-bettor who doesn’t flat 4-bets”的效果。在后面的部分我们会用到这里分析的结果来说明这（指的是only 4-betting or folding against a 3-bet）不是理论的最优解。</p><p>当一个玩家面对3-bet时仅仅fold或4-bet，那么根据上一篇的分析，我们知道了他必须要4-bet他自己25-30%的open range，同时面对5-bet all in必须要在50-60%的情况防守自己的4-bet。将两个范围的平均值相乘$0.152=0.275\times 0.55$，我们得到了该<strong>名玩家在自己open range的15.2%的范围里会跟注5-bet all in。</strong></p><p>15.2%是一个重要的频率，尤其是当我们面对的对手不愿意跟注3-bet时我们更应当把这个频率记在脑子里。当和这些对手对抗时，我们需要防守15.2%的频率，不然的话他们使用ace-rag suited这样的牌进行bluff将变得有利可图（ace-rag通常被称作“A弱”或者“A碎”，指的是ace加上一个2-9的踢脚）。</p><p>接下来我们将观察那些不愿意跟注3-bet的玩家的典型4-bet和5-bet-calling范围。我们可以通过限定住raise first in的范围来得到下面这张表，同时需要注意以下的很多牌都只是estimates因为并没有把所有的combo都计算进来。</p><div class="table-container"><table><thead><tr><th style="text-align:center">Raise First In %</th><th style="text-align:center">4-Bet %</th><th style="text-align:center">4-Bet Range</th><th>5-Bet-Call %</th><th style="text-align:center">5-Bet-Call Range</th></tr></thead><tbody><tr><td style="text-align:center">10</td><td style="text-align:center">2.75</td><td style="text-align:center"><strong>QQ+, AKs, AKo</strong></td><td>1.52</td><td style="text-align:center"><strong>QQ+, AKs</strong></td></tr><tr><td style="text-align:center">15</td><td style="text-align:center">4.13</td><td style="text-align:center"><strong>TT+, AQs+, AKo</strong></td><td>2.28</td><td style="text-align:center"><strong>QQ+, AKs, AKo</strong></td></tr><tr><td style="text-align:center">20</td><td style="text-align:center">5.5</td><td style="text-align:center"><strong>TT+, AJs+, KQs, AQo+</strong></td><td>3.03</td><td style="text-align:center"><strong>QQ+, AQs, AKo</strong></td></tr><tr><td style="text-align:center">25</td><td style="text-align:center">6.88</td><td style="text-align:center"><strong>TT+,  ATs+, KQs, AJo+</strong></td><td>3.78</td><td style="text-align:center"><strong>TT+, AQs, AKo</strong></td></tr><tr><td style="text-align:center">30</td><td style="text-align:center">8.25</td><td style="text-align:center"><strong>TT+,  ATs+, KQs, AJo+</strong></td><td>4.54</td><td style="text-align:center"><strong>TT+, AQs+, AQo+</strong></td></tr><tr><td style="text-align:center">40</td><td style="text-align:center">11</td><td style="text-align:center"><strong>88+, A9s+, KJs+, QJs, ATo+, KJo+</strong></td><td>6.05</td><td style="text-align:center"><strong>TT+,  AJs+, KQs, AJo+</strong></td></tr></tbody></table></div><p>这张表帮助我们了解使用什么样的手牌能够做到effective 3-bet。比方说，如果对抗一个open range为15%的玩家，拿着<strong>QQ</strong>去和他all-in将不是一个好决定，因为他的all-in calling range的最差牌力也是<strong>QQ</strong>，call进来的牌一定比我们更强。<strong>一个大约15%的range通常是一个UTG玩家的open range</strong>，并且在六人牌桌（six max）中不能够拿着<strong>AKo</strong>来做value 3-bet以及value 5-bet和我们上一章Maximum 3-Betting Ranges中的理论是一致的。但是这并不是说<strong>AKo</strong>就一定不能3-bet <strong>UTG</strong>玩家，而是通常用它来跟注3-bet的。</p><p>现在让我们来分析“已经准备好跟注5-bet的4-bet”（4-betting with the intention of calling a 5-bet）的盈利能力。首先我们假设我们的4-bet导致对手弃牌获利12.5BB，并且这件事发生的频率为60%（这在上一章都有迹可循）。这样我们每次4-bet时平均收到7.5BB的对手弃牌收益。</p><script type="math/tex; mode=display">7.5=(0.6)(12.5)</script><p>剩下的40%的频率对手会跟我们all-in，而我们会进行跟注。我们的EV将会是我们的equity乘上最后的底池大小。例如，如果在一个201.5BB的底池中，如果我们的4-bet在对抗对手5-bet all-in range有45%的equity，那么我们的EV为-9.325BB。</p><script type="math/tex; mode=display">-9.325=(0.45)(201.5)-100</script><p>而这只在40%的频率上发生，所以我们“已经准备好跟注5-bet的4-bet”的总EV值为0.77BB。</p><script type="math/tex; mode=display">0.77=(0.60)(7.5)+(0.40)(-9.325)</script><blockquote><p>Note the following tables show how much we expect to win or lose overall for the hand when we 4-bet. So since folding to a 3-bet results in us losing 3.5 big blinds, it’s still more profitable to 4-bet with the intention of calling a 5-bet provided our total expected value is greater than -3.5 big blinds.</p></blockquote><p>注意接下来的表格展示了我们在4-bet并打算进行5-bet跟注的盈利或亏损的EV值，因为对对手的3-bet进行弃牌会亏损3.5BB的死钱，所以任何EV在-3.5BB以上的4-bet手牌都是更好的选择。</p><p><strong>Table No.1: Equity Versus a 5-Bet Jamming Range of KK+, AKs, A5s (1.5%)</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center">Hand</th><th style="text-align:center">Percent Equity When All In</th><th style="text-align:center">Total Expected Value (BB)</th></tr></thead><tbody><tr><td style="text-align:center">QQ</td><td style="text-align:center">35.1</td><td style="text-align:center">-4.2</td></tr><tr><td style="text-align:center">KK</td><td style="text-align:center">42.8</td><td style="text-align:center">2.0</td></tr><tr><td style="text-align:center">AA</td><td style="text-align:center">81.0</td><td style="text-align:center">32.8</td></tr><tr><td style="text-align:center">AQs</td><td style="text-align:center">34.8</td><td style="text-align:center">-4.4</td></tr><tr><td style="text-align:center">AKo</td><td style="text-align:center">37.4</td><td style="text-align:center">-2.4</td></tr><tr><td style="text-align:center">Aks</td><td style="text-align:center">41.5</td><td style="text-align:center">1.0</td></tr></tbody></table></div><p>上表中的Hands通常被用来对抗<strong>UTG</strong>的open。</p><p><strong>Table No.2: Equity Versus a 5-Bet Jamming Range of QQ+, AKs, A5s, AKo (2.9%)</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center">Hand</th><th style="text-align:center">Percent Equity When All In</th><th style="text-align:center">Total Expected Value (BB)</th></tr></thead><tbody><tr><td style="text-align:center">JJ</td><td style="text-align:center">39.4</td><td style="text-align:center">-0.7</td></tr><tr><td style="text-align:center">QQ</td><td style="text-align:center">43.4</td><td style="text-align:center">2.5</td></tr><tr><td style="text-align:center">KK</td><td style="text-align:center">58.7</td><td style="text-align:center">14.8</td></tr><tr><td style="text-align:center">AA</td><td style="text-align:center">84.3</td><td style="text-align:center">35.5</td></tr><tr><td style="text-align:center">AQs</td><td style="text-align:center">33.1</td><td style="text-align:center">-5.8</td></tr><tr><td style="text-align:center">AKo</td><td style="text-align:center">42.5</td><td style="text-align:center">1.8</td></tr><tr><td style="text-align:center">AKs</td><td style="text-align:center">45.4</td><td style="text-align:center">4.1</td></tr></tbody></table></div><p>上表中的Hands通常被用来对抗中位open的玩家。</p><p><strong>Table No.3: Equity Versus a 5-Bet Jamming Range of JJ+, AKs, A5s, AKo (3.3%)</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center">Hand</th><th style="text-align:center">Percent Equity When All In</th><th style="text-align:center">Total Expected Value (BB)</th></tr></thead><tbody><tr><td style="text-align:center">JJ</td><td style="text-align:center">39.7</td><td style="text-align:center">-0.7</td></tr><tr><td style="text-align:center">QQ</td><td style="text-align:center">49.4</td><td style="text-align:center">7.3</td></tr><tr><td style="text-align:center">KK</td><td style="text-align:center">63.1</td><td style="text-align:center">18.4</td></tr><tr><td style="text-align:center">AA</td><td style="text-align:center">83.7</td><td style="text-align:center">34.9</td></tr><tr><td style="text-align:center">AQs</td><td style="text-align:center">35.4</td><td style="text-align:center">-4.0</td></tr><tr><td style="text-align:center">AKo</td><td style="text-align:center">42.7</td><td style="text-align:center">1.5</td></tr><tr><td style="text-align:center">AKs</td><td style="text-align:center">45.5</td><td style="text-align:center">4.2</td></tr></tbody></table></div><p>上表中的Hands通常被用来对抗<strong>CO</strong>的open。</p><p><strong>Table No.4: Equity Versus a 5-Bet Jamming Range of TT+, AJs, AQo (5%)</strong></p><div class="table-container"><table><thead><tr><th style="text-align:center">Hand</th><th style="text-align:center">Percent Equity When All In</th><th style="text-align:center">Total Expected Value (BB)</th></tr></thead><tbody><tr><td style="text-align:center">99</td><td style="text-align:center">38.2</td><td style="text-align:center">-1.7</td></tr><tr><td style="text-align:center">TT</td><td style="text-align:center">41.2</td><td style="text-align:center">0.7</td></tr><tr><td style="text-align:center">JJ</td><td style="text-align:center">47.5</td><td style="text-align:center">5.8</td></tr><tr><td style="text-align:center">QQ</td><td style="text-align:center">56.0</td><td style="text-align:center">12.6</td></tr><tr><td style="text-align:center">KK</td><td style="text-align:center">67.6</td><td style="text-align:center">12.6</td></tr><tr><td style="text-align:center">AA</td><td style="text-align:center">84.7</td><td style="text-align:center">35.8</td></tr><tr><td style="text-align:center">AJo</td><td style="text-align:center">28.5</td><td style="text-align:center">-9.5</td></tr><tr><td style="text-align:center">AJs</td><td style="text-align:center">32.8</td><td style="text-align:center">-6.1</td></tr><tr><td style="text-align:center">AQo</td><td style="text-align:center">36.6</td><td style="text-align:center">-3.0</td></tr><tr><td style="text-align:center">AQs</td><td style="text-align:center">39.9</td><td style="text-align:center">-0.3</td></tr><tr><td style="text-align:center">AKo</td><td style="text-align:center">50.5</td><td style="text-align:center">8.2</td></tr><tr><td style="text-align:center">AKs</td><td style="text-align:center">52.9</td><td style="text-align:center">10.1</td></tr></tbody></table></div><p>上表中的Hands通常被用来对抗<strong>BTN</strong>的open。</p><p>这里假设我们的4-bet会在60%的频率上迫使对手弃牌，但是现实中阻挡效应会影响我们的4-bet成功的frequency。例如，<strong>AA</strong>将会显著地更少碰到对手的5-bet因为它本身阻挡了非常多的5-bet bluff的组合。而同时，它甚至也block掉了对手3-bet的range从而有可能让这手牌到不了4-bet这轮。所以这种复杂的情况让我们无法准确地预测我们的对手会以多少的频率进行5-bet，但是我们仍然能够计算一旦碰上5-bet后的EV。</p><p>一个令人惊讶的事实是当面对<strong>UTG</strong>玩家的3-bet时，除了<strong>AA</strong>是一个明显的盈利手牌之外，<strong>KK</strong>也只具有+2BB的EV，比对手弃牌的EV还要低。而一个<strong>UTG</strong>拿着<strong>QQ</strong>的手牌进行4-bet并打算5-bet跟注就需要做好平均输掉4BB的心理准备。以上的范围帮助我们加强了上一章中Maximum 3-Betting Range中得到的范围。比方说我们不能够在面对<strong>UTG</strong>时3-bet <strong>AK/QQ</strong>还期待这手牌在面对4-bet时能有好的表现。虽然在不利位置面对3-bet时要么4-bet要么弃牌是一个很简单的策略，但是我们以上的分析告诉我们这样的策略有多么强的局限性而且非常低效，相反我们对手两极化的3-bet range告诉了我们我们需要更多地进行防守而不是4-bet。</p><p>最后一点，当我们逐渐深入地讨论这个游戏的理论性，有时停下来思考理论得出的结论和我们实际经验是否一致是非常重要的。那些在任何位置都鲁莽地3-bet或4-bet <strong>AK/QQ</strong>的玩家会在他们落后时抱怨“运气真差”，但实际上这正说明了他们在面对强大对手时犯下了非常严重的错误。</p><h1 id="Flatting-3-Bets-and-4-Bets"><a href="#Flatting-3-Bets-and-4-Bets" class="headerlink" title="Flatting 3-Bets and 4-Bets"></a>Flatting 3-Bets and 4-Bets</h1><p>在我们讨论分析跟注3-bet前，我们必须了解为什么我们需要一个对抗4-bet的calling range。上一章的关于“永远不要在不利位置跟注4-bet”的迷思教导非常多的玩家不论是否在有利位置都不选择跟注。这种策略背后通常隐含着这样的“道理”：因为对手4-bet bluff对抗自己的强牌时也至少有20%的equity，做大这样的底池非常容易被对手BB。这样的道理其实隐含着一个更加重要的原因，那就是他们不想在post-flop上做困难的决定。然而这是一个非常糟糕的方法，而玩家应该要跟注4-bet，原因如下：</p><blockquote><p>Hands in the value 4-betting range are stronger than the weakest hands in the value 3-betting range. If a player 5-bets the weakest hand in his value 3-betting range, he will never be ahead when called. </p></blockquote><p>在value 4-betting range的牌比在value 3-betting range中的最弱牌要强。如果一个玩家5-bet了他value 3-bet中的最弱牌，他在被跟注时永远不会领先。</p><blockquote><p>5-betting ensures either all the money gets in preflop or no one sees a flop. This takes away the positional advantage the 3-bettor would have if he called the 4-bet when in position. </p></blockquote><p>5-bet确保在preflop轮所有的钱都进入到底池或者没有人能够看到flop轮的结果。这样的效果实际上消除了3-bet玩家原本具有的位置优势，而如果3-bet玩家跟注则这种位置优势将会在post-flop得到保留。</p><blockquote><p>Even if a hand has only 20 percent equity against a monster like aces or kings, it likely needs to see the turn or river to become best. Overpairs can ensure all the money gets in on the flop or turn on threatening boards before the opponent gets to see additional cards. In other words, our opponent will have a difficult time realizing the equity of his semi-bluffs. </p></blockquote><p>即使一手牌对抗<strong>AA/KK</strong>这样的超强牌只有20%的equity，它通常需要看到turn或者river才有可能实现它的equity。超对（overpair）能够保证所有的钱在flop轮或者turn轮的threatening board上都进入底池，这将会阻止对手看到额外的公共牌。换句话说，对手将非常难以实现自己semi-bluff的equity。</p><blockquote><p>For 100 big blind stacks, it’s extremely difficult to 5-bet and fold. This means a player can 4-bet very small if he knows his opponent will always respond by 5-betting or folding. </p></blockquote><p>在100BB的stack情况下，5-bet &amp; fold是非常难发生的。这就意味着一个玩家如果知道自己的对手总是弃牌或5-bet，那么他可以做一个size非常小的4-bet。<em>这个非常小的4-bet在我看来的意义是：如果自己是在4-bet bluff，而对手很快进行5-bet raise，那么我们可以损失少一些的筹码量探测到了对手的意图从而更容易弃牌；而如果对手弃牌，则我们就收回底池中的死钱。总而言之就是冒更小的风险获得等量的信息。</em></p><p>在不利位置总是4-bet或者fold这个策略会在对手学会在有利位置进行跟注4-bet之后变得无效。4-bet玩家通过4-bet将没有办法摧毁3-bet玩家的位置优势，反而冒了更大底池的风险在不利位置进行游戏。所以现在用一个平衡的范围对3-bet进行跟注会变得更加合理，对这个范围的唯一要求就是它要能够在众多常见牌面中应对自如。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;EV-of-4-Betting-or-Folding-Against-Opponents-Who-Don’t-Flat-4-Bets&quot;&gt;&lt;a href=&quot;#EV-of-4-Betting-or-Folding-Against-Opponents-Who-Don’t
      
    
    </summary>
    
    
      <category term="poker" scheme="http://www.shihaizhou.com/tags/poker/"/>
    
  </entry>
  
  <entry>
    <title>AONLH(2) - Preflop Play (part 1)</title>
    <link href="http://www.shihaizhou.com/2019/06/05/AONLH-2-Preflop-Play/"/>
    <id>http://www.shihaizhou.com/2019/06/05/AONLH-2-Preflop-Play/</id>
    <published>2019-06-05T09:34:29.000Z</published>
    <updated>2019-06-14T21:54:56.910Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><hr><p>首先德扑游戏具有两个特性：对于人类玩家的记忆来说的较大的策略空间和非完美信息博弈，这使得我们没有办法使用特定的策略解决所有的情况。关于如何解决这种情况，书中一个观察让我印象很深刻：</p><blockquote><p>Second, we should note that the best way to approach a problem may often seem backwards or cause us to pause for a while and come back later when we have more information. For instance, suppose on the flop we want to figure out what our bluffing to value raising ratio should be. To accomplish this, we must first know how often we’re going to bet the river with a balanced range, what bet sizing should be used on the turn and river, and how position effects our range. </p><p>其次，我们应该注意解决这个问题的最好方法往往是从后向前的，或者让我们先暂停一下等有了更多的信息再去解决它。比方说，如果我们想要找到最好的bluff和value raising的比例，那么我们必须首先知道该如何在river上进行平衡的bet，在turn上和在river上bet的sizing，以及位置是如何影响我们的range的。</p></blockquote><p>我对这段话的理解是，从一个玩家的角度出发，每一次翻牌的结果和玩家的决策的结果都是策略空间中的一部分。那么每一个最终被选择的策略（翻出什么牌、下了多少注），都会构成这棵决策树上的一条路径。要理解前期的策略应该被如何执行，就要知道前期策略会导致怎么样的后果，也就是要对未来决策树路径的选择非常清楚。也就是说在执行当前决定之前，牌手必须对下来的情况都有应当对策略。</p><p>另外，虽然有的时候我们不能够找到最优的策略，但是我们能够通过分析就判断出一个行为是不是坏的。这里作者举了一个例子：在任何位置上都用自己的<strong>QQ+/AK</strong>去3-bet对抗<strong>UTG</strong>位置的open raise是一个不好的选择。因为如果每个玩家都用这样的牌去进行3-bet，那么UTG的玩家就会马上意识到自己被别人3-bet的频率太高了。于是为了对抗这种高频的3-bet，<strong>UTG</strong>玩家就会降低自己open raise的频率，拿更加强的牌进行open raise。</p><h1 id="Preflop-3-4-5-betting-Frequencies"><a href="#Preflop-3-4-5-betting-Frequencies" class="headerlink" title="Preflop (3,4,5)-betting Frequencies"></a>Preflop (3,4,5)-betting Frequencies</h1><hr><p>在网络德州中，通常情况下每个玩家的stack size是100BB，常见的open size是3.5BB，3-bet size为10BB~12BB。接下来让我们先考虑防守（通常“防守”指的是call对方的raise）对方3-bet的频率。为了让对方在3-bet bluff的EV为0（书中的描述为Indifferent to bluffing），我们假设自己防守的概率为$x$，则有</p><script type="math/tex; mode=display">5(1-x)-10x = 0</script><p>其中5表示的大小盲的盲注大小1.5BB加上open raise的3.5BB，同时上面这条式子做了一个简化，那就是假定如果我们进行了防守，对手的bluff一定会输。而此时这个频率构成了双方玩家的一个均衡：也就是说如果想要让对方bluff的EV为0，我们需要防守$x$的频率；而对方如果要平衡自己bluff的频率，则他必须要在$1-x$的频率下进行value bet。由此解得作为open raiser我们需要对抗对手3-bet的频率为33.3%。而如果对手的3-bet的size为12BB，则有：</p><script type="math/tex; mode=display">5(1-x)-12x=0</script><p>相应的防守频率为29.4%。这里注意到3-bet进攻的玩家多投入20%的筹码也只会降低open raise玩家4%的防守频率。同时我们还需要注意的是open raise的玩家并不是场上唯一一个防守者，其他后置位的玩家也有可能进行防守。假设<strong>CO</strong>做了3.5BB的open raise，<strong>BTN</strong>做了10BB的3-bet，考虑进其他玩家（此时只剩下了<strong>SB</strong>和<strong>BB</strong>）的防守并假设他们4-bet的频率为3%，我们可以计算出<strong>CO</strong>要进行的防守频率为29.1%：</p><script type="math/tex; mode=display">(1-0.03)^2(1-x)=1-0.33</script><script type="math/tex; mode=display">\Rightarrow x=29.1\%</script><p>但是德州扑克并没有这么简单。通常来讲处于有利位置的玩家倾向于做一个小的3-bet来鼓励他的对手入池并看到河牌，这种时候其他玩家帮助我们进行防守的概率会变高；处于不利位置的玩家则通常把3-bet的size做的更大一些来获取我们更高的弃牌率。总的来说，我们防守3-bet的频率应该在27%到31%之间。</p><p>当对手做了3-bet时，底池已经有15BB-16BB大小。4-bet raise的size通常是22BB-24BB，如果是原本open raise的玩家进行4-bet raise，那么他的成本就是18.5BB-20.5BB，因为之前的3.5BB已经成为了构成底池的死钱。这也就意味着cold 4-bet需要至少需要60%的成功率，而后者只需要55%的成功率。</p><p>在一个stack只有100BB的牌局中，5-bet bluff通常都会使用all-in进行。<em>这样也就导致了5-bet bluff的玩家通常会使用一个weak pocket pair或者一个weak Ace suited来做5-bet bluff，这是因为这两种牌型在被跟注时具有最高的equity。</em> 但是具体的equity对抗对手不同的calling range是不同的，例如：在对抗<strong>JJ+/AK</strong>时，<strong>33o</strong>和<strong>A5s</strong>分别具有32.2%和30.7%的equity；而在对抗<strong>KK+</strong>的范围时，他们两者就分别只有18.4%和26.7%的equity。</p><p>需要注意到绝大多数情况下进行5-bet bluff的玩家和3-bet的玩家是同一个，这样对于这名玩家来说他在3-bet时投入的11BB（10BB和12BB的中间值）就是底池中的死钱，他在进行5-bet时就只有额外的89BB的风险。假设该名玩家的equity为31%，那么他5-bet bluff被跟注的EV为：</p><script type="math/tex; mode=display">EV_{5bet.called} = 201.5(0.31)-89 = -26.5</script><p>而他的对手如果放弃，则他5-bet bluff获利的EV为：</p><script type="math/tex; mode=display">EV_{5bet.folded}=1.5+11+24=36.5</script><p>需要注意的是，这里我们讨论的都是heads-up的对局，因此1.5为盲注大小，11为3-bet大小，24则包括了open raise的3.5BB。为了让bluff保持在一个0EV的状态，则我们可以计算出我们的5-bet bluff必须要在42%的情况下奏效，也就是指对手应当有42%的弃牌率：</p><script type="math/tex; mode=display">36.5x-26.5(1-x)=0</script><script type="math/tex; mode=display">\Rightarrow x = 42\%</script><p>总的来讲，为了要能够有收益，5-bet bluff应当要在40-50%之间奏效才行，而具体的EV值则需要根据具体的5-bet跟注范围、其他bet的sizing进行计算。综上的分析，我们得出了以下频率：</p><ul><li>3-bet bluff通常需要在67-70%的情况下奏效才能盈利。</li><li>4-bet bluff通常需要在54-60%的情况下奏效才能盈利。</li><li>5-bet bluff通常需要在40-50%的情况下奏效才能盈利。</li></ul><p>这是一个非常有趣的观察，那就是虽然3-bet到5-bet的下注大小在不断增大，他们需要成功的条件却是不断宽松，需要成功的频率要求是在不断降低的。如果我们的对手更倾向于call而不是raise，那么3-bet和4-bet的条件又可以放宽一些，因为这又增加了我们在后面几条街上击中并实现equity的概率，而这也常常发生。类似地，我们也可以做这样的说明：</p><ul><li>如果一个玩家从来不flat call对手的3-bet，那么他需要在15-20%的频率下对自己的open raise进行防守。这个频率还可以适当降低因为还有其他的玩家会帮助防守。</li><li>3-bet的玩家如果也不flat call对手的4-bet，那么他需要在40-46%的频率下对自己的3-bet进行防守。</li><li>4-bet的玩家需要在50-60%的频率下call对手的5-bet all-in。</li></ul><p>这些基于频率的观察帮助我们在3-bet、4-bet和5-bet的情况下平衡自己的value raises和bluff raises，虽然它不能够告诉我们具体该怎么做，但是却能够帮助我们判断自己的策略是否严重地倾斜并及时作出修正。</p><h1 id="Preflop-Raise-First-in-Ranges"><a href="#Preflop-Raise-First-in-Ranges" class="headerlink" title="Preflop Raise, First in Ranges"></a>Preflop Raise, First in Ranges</h1><hr><p>在我们讨论该用怎样的range去对抗对手的open之前，我们应当首先了解我们该使用怎样的手牌去open。注意到在flop前的fold具有的EV为0，所以我们应当拿大于0EV的手牌进行open raise。这样就说明了我们open range中的最差手牌的EV应当接近于0，因此我们对一名GTO玩家的open raise range中的最差牌力有以下四项观察：</p><ul><li><p>这手牌的EV如果不是恰好等于0，那么它必须接近于0。一手牌只以一定频率被用来open raise的时候它可能就会达到0EV；而它如果总是被拿来open raise，则它的EV会更倾向于变成负。例如<strong>UTG</strong>拿着<strong>56s</strong>以75%的频率做open raise是一个接近于0EV的行动，但是如果100%频率则非常容易被剥削从而变成-EV的策略。</p></li><li><p>这手牌在面对对手的3-bet是需要经常fold的。因为拿最弱的牌去call对手的3-bet不是一件很好的事。</p></li><li>这手牌在面对对手的call时的EV会大于-3.5BB，这是因为即便是一手非常弱的手牌也有可能在后面几条街上实现它的equity。</li><li>这手牌在看见flop时的EV不会大于+1.5BB。因为德扑是一个0和游戏，如果在open raise range中最差的手牌也能够有大于盲注大小的EV，这就说明cold caller没有使用optimal的策略，他的冷跟策略是-EV的。</li></ul><p>open raiser利用自己range中最弱的牌将自己的3.5BB投入风险以赢得底池中的1.5BB，结合上文中提到的最弱的牌在遭遇cold call和reraise时都是-EV的，那么它需要在至少70%的情况下能够吓走对手才能够获得收益。这样就意味着如果其余的玩家如果构成了超过30%的3-bet频率，那么对于open raiser来说，他再使用这种在遭遇3-bet时fold的牌进行open raise就不再是一件有利可图的事了。</p><h1 id="Maximum-3-bet-Range"><a href="#Maximum-3-bet-Range" class="headerlink" title="Maximum 3-bet Range"></a>Maximum 3-bet Range</h1><hr><p>为了方便起见，我们假设每一个玩家在不同位置采取的3-bet频率都相同。同样的假设<strong>UTG</strong>进行了open raise，剩下5名玩家需要行动。5名玩家构成了30%的3-bet防守频率，假设每名玩家的3-bet频率为$x$，则有下式：</p><script type="math/tex; mode=display">(1-x)^5=1-0.3</script><script type="math/tex; mode=display">\Rightarrow x=6.9\%</script><p>从而我们得出，剩下的每名玩家3-bet的最大range是6.9%。同时我们注意到在前面章节中的结论，一名3-bet玩家在面对对手4-bet时不应当在超过40-46%的频率上fold，这也就意味着40-46%的3-bet range要准备好对抗对手的4-bet，我们将这种3-bet的range称为“Value 3-bet”。下表显示了使用上面这种方法计算出的不同位置的3-bet频率。<em>这里尤其需要注意，举第一行为例，6.9%的频率指的是当<strong>UTG</strong>玩家open时，你作为后置位玩家应该3-bet的频率，而不是当你在<strong>UTG</strong>位置上的3-bet频率。</em></p><div class="table-container"><table><thead><tr><th style="text-align:center">Opening Range</th><th style="text-align:center">Maximum 3-Betting Percentage</th><th style="text-align:center">Value 3-Betting Percentage</th><th style="text-align:center">Value Component of 3-Betting Range</th></tr></thead><tbody><tr><td style="text-align:center">UTG</td><td style="text-align:center">6.9</td><td style="text-align:center">2.76</td><td style="text-align:center">AA-QQ, AK</td></tr><tr><td style="text-align:center">MP</td><td style="text-align:center">8.5</td><td style="text-align:center">3.4</td><td style="text-align:center">AA-JJ, AK, AQs</td></tr><tr><td style="text-align:center">CO</td><td style="text-align:center">11.2</td><td style="text-align:center">4.6</td><td style="text-align:center">JJ+, AJs+, AQo</td></tr><tr><td style="text-align:center">BTN</td><td style="text-align:center">16.3</td><td style="text-align:center">6.52</td><td style="text-align:center">TT+, ATs+, KQs+, AJo+</td></tr></tbody></table></div><p>以上的频率和范围并不是理论正确的策略：我们可以3-bet一些并不是最强牌力的牌并且在遭遇4-bet时进行5-bet bluff。我们也可以通过冷跟preflop来慢玩一些超强牌。但是上面的表可以帮助我们判断别的玩家是否在3-bet上做得过于激进了。也就是说，如果他们在接近于以上的频率进行3-bet，那么preflop raiser如果继续采用自己range中最差牌open就会开始输钱了。同时，preflop raising range中最弱的牌一旦get called，就倾向于输钱，理由如下：</p><ul><li>It is weaker than the average hand in a preflop cold calling range. 它比冷跟preflop raise的范围的平均牌力要弱。</li><li>On the flop, the preflop raiser will often be out of position unless he opened on the button or is just called by a blind. 在flop上，preflop raiser通常都处于不利位置，除非他在<strong>BTN</strong>位open并且仅有盲注跟注。</li><li>If one of the blinds does call (thus letting the preflop raiser see a flop in position), the pot will be smaller since there is less extra dead money from the blinds. 如果大小盲中的一个玩家确实跟注了，虽然这会让open raiser在后面几条街上处于有利位置，但这也导致底池比其他玩家跟注的情况要小一些，因为底池中失去了原来盲注位的死钱。</li></ul><p>接着来到了本文中我个人认为是最重要的一处论述，为了强调它的重要性并且保持原文的忠诚度，我在这里贴上原文。</p><blockquote><p>It’s important to design ranges where all of our theory and beliefs make sense and must not allow ourselves to have a contradicting thought process. We now are equipped with a great set of restrictions, or parameters, which will tell us what we can and cannot do when designing opening ranges and defending ranges from all the positions. </p><p>In order to quickly illustrate this concept, let’s imagine we are playing at an aggressive table and open the worst hand in our theoretically correct UTG opening range for 3.5 big blinds. We are 3-bet 30 percent of the time total and are cold called another 25 percent of the time. Furthermore, let’s be generous and assume our expected value when our open is called is on average 0 EV. We can now plug in these variables to figure out that the expected value of opening the worst hand in an UTG opening range at this table is -0.375 big blinds.</p><script type="math/tex; mode=display">−0.375BB = (0.3)(−3.5BB)+ (0.25)(0BB) + (0.45)(1.5BB)</script><p>That is the UTG player expects to on average lose -0.375 big blinds when he opens the worst hand in his theoretically correct UTG opening range. </p><p>Notice we are not breaking even at this table by opening the worst hand in our theoretically correct raise first in range. Our opponents are playing too aggressively and our weak hand does poorly against opponents who 3-bet 30 percent total. The fact that theoretically correct raises can have a negative expected value against certain opponents shouldn’t surprise anyone. For instance, a theoretically correct bluff on the flop will lose money against opponents who refuse to fold. </p><p>While we would break even if our opponents always 3-bet or folded and they 3-bet 30 percent total, we must not forget to take into account our open will sometimes be called. This is something which can easily be forgotten when performing calculations. It’s easy to think “since I break even when my opponent calls, it doesn’t change anything,” but this is not true. When our opponents fold we win 1.5 big blinds, yet when one of them calls, we win 0 big blinds. </p><p>This showcases the fact that as long as our opponents have any sort of reasonable cold calling ranges, they must be 3-betting significantly less than 30 percent of the time combined. If instead they always reraise with all these hands, we have no incentive to open with the weak hands in our theoretically correct opening range. </p></blockquote><h1 id="About-Noob-Poker"><a href="#About-Noob-Poker" class="headerlink" title="About Noob Poker"></a>About Noob Poker</h1><hr><p>因为在前文中我们假设对于自己的3-bet其余玩家不会采用跟注的方式防守，而总是采取弃牌或是4-bet，所以在一个稍微平衡一些的牌局中，我们马上就能够通过我们设定好的参数计算发现自己的3-bet频率在导致我们输钱。所以对于牌局的考量：对手应对raise的方式是加注还是跟注，他们下注的sizing等等都应该成为我们平衡自己range的参数，基于这种参数的计算应当在进入牌局之前进行。上面这段文字让我特别有感触的点在于，之前自己对于德扑理论了解得不深，看了一些高额德州stack在200-250BB的牌局，在不理解高手下注的尺度和频率的情况下，就盲目地学习他们的玩法，往往在都是新手的牌局中占不到便宜。因为新手的策略往往都是严重偏离GTO的，所以采用GTO的打法（虽然自己只是拙劣地模仿）会导致盈利不足甚至经常在multi-way中被Bad Beat。现在让我们自己来对这种新手局的preflop场景进行一些分析，而不考虑后面几条街的变数。</p><p>刚接触德扑的新手玩家倾向于跟注入池，而不考虑自己和对手的范围或者相对的位置关系。而在进行一些游戏之后会有少量新手玩家学会在preflop弃掉特别差的牌，例如<strong>27o</strong>等等。同时新手玩家对于raise和raise的尺度并不敏感，他们很少自己进行raise，即便他们手中有超强牌力的手牌。于是我们假设在一个标准六人牌桌上，其余5名玩家的在preflop轮的动作频率分布为：</p><div class="table-container"><table><thead><tr><th style="text-align:center">call</th><th style="text-align:center">raise</th><th style="text-align:center">fold</th></tr></thead><tbody><tr><td style="text-align:center">90%</td><td style="text-align:center">0</td><td style="text-align:center">10%</td></tr></tbody></table></div><p>首先我们假设自己的bluff在被跟注之后能够实现的平均equity为10%（事实上这已经将这个情景乐观化了很多），因为对手行动空间上的概率分布已经固定，我们就可以计算出bluff的EV：</p><script type="math/tex; mode=display">EV_{open.bluff}=1.5\times0.1^5+3.5(10\%\times5\times0.9-90\%\times1)(1-0.1^5)=-1.47BB</script><p>其中，$1.5\times0.1^5​$代表剩余5名玩家全部fold并直接获利的期望；$10\%\times5\times0.9​$表示二项分布的独立同分布实验的期望盈利；而$99\%.​$则表示在get called之后输掉的y用于open raise的下注大小。可以非常清楚的发现由于对手的range为固定值，我们的bluff永远都是一个负EV的操作。</p><p>接着我们来计算在这样的牌桌中最大的open range是多少，假设自己的open raise中最差手牌需要具有的平均equity为$x$，则根据前文的假设最差手牌的EV应当接近于0，故而有：</p><script type="math/tex; mode=display">EV_{open.worst}=1.5\times0.1^5+3.5(x\times5\times0.9-(1-x))(1-0.1^5)=0</script><p>解得$x=18.2\%$，而我们知道这里的$x$为平均需要具有的equity，而不是range本身。也就是说当equity近似于pot odds时我们就应该open raise，甚至条件还更加宽松。这一点其实和我们在牌桌上的体验是相同的：首先因为新手对于范围不敏感，所以当你open raise时，其余玩家的跟注等效于盲注大小变大，并没有将其余玩家排除在游戏之外。而且由于对手总体的弃牌率低，使得自己拿到池内盲注的immediate reward的EV降低，也就是说在这种情况下open的size并没有那么关键，所以作任何size的open都是一个0EV的选择。因此从数学上来说，是否做open是一件无关紧要的事，如何在flop轮之后榨取更多的价值，才是一件需要我们更多思考的事。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;hr&gt;
&lt;p&gt;首先德扑游戏具有两个特性：对于人类玩家的记忆来说的
      
    
    </summary>
    
    
      <category term="poker" scheme="http://www.shihaizhou.com/tags/poker/"/>
    
  </entry>
  
  <entry>
    <title>AONLH(1) - Basics</title>
    <link href="http://www.shihaizhou.com/2019/06/04/AONLH-Chap-NO-1-Basics/"/>
    <id>http://www.shihaizhou.com/2019/06/04/AONLH-Chap-NO-1-Basics/</id>
    <published>2019-06-04T13:35:49.000Z</published>
    <updated>2019-06-05T18:48:16.176Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><p>从今年四月份开始一直想要系统地学习一下德州扑克关于GTO（Game Theory Optimal）的理论和技术，但是一直没有时间也没有动力。最近忙完了学习生活中的主要的事情，所以想静下心来学习一下这一部分的内容，并看看有没有机会尝试将它应用在实践当中。经过一些调查，选择了Application Of No-Limit Hold’em这本书。我大致翻阅了一下这本书，还不好有太多的评价，但是整体的结构非常清爽，和绝大多数的英文教材一样由浅入深、由表及里。为了督促自己能够坚持把这本书读完，所以想在自己的blog上记录一下每天阅读的收获，必要部分会进行一些摘录和翻译。今天就将这本书第一章基础内容的summary做一些摘录。</p><h1 id="Chap-No-1-Summary"><a href="#Chap-No-1-Summary" class="headerlink" title="Chap No.1 Summary"></a>Chap No.1 Summary</h1><blockquote><ol><li>Pot odds are the ratio of the current size of the pot to the cost of a potential call. They are critical for determining how often we need to expect to win to justify a call. </li></ol></blockquote><p>赔率指的是当前pot中的筹码量和当前需要call的筹码量，这对于我们是否决定call当前的bet有着至关重要的决定性作用。</p><blockquote><ol><li>Equity tells us the odds a hand will be the best hand by the river against an opponent’s hand or range. </li></ol></blockquote><p>Equity指的是我们的牌在河牌成为相对对手手牌的最强牌的概率。</p><blockquote><ol><li>The expected value of a hand tells us how much money we expect to win when we take a certain line. It considers all money invested in the pot as dead money up to the current decision. </li></ol></blockquote><p>一手牌的期望价值（Expected Value, EV）指的是我们在当前line中期望得到的价值，它将所有在当前决定之前已经投入到pot当中的投资当做死钱（Dead Money）。</p><blockquote><ol><li>Hands with more equity are not necessarily better than hands with less equity. It’s important our hand retains its equity against our opponent’s raising and calling ranges. That’s why hands like the <strong>98s</strong> are almost always better than hands like the <strong>A7o</strong>. </li></ol></blockquote><p>有着更高equity的手牌不一定比更低equity的手牌更好。好的手牌必须要在对方的raise和call下依旧保持它的equity。这就是为什么89同花比A7不同花要更好的原因。</p><blockquote><ol><li><p>Hands cannot simply be ranked from strongest to weakest. Different hands perform better in different spots. For example, <strong>33o</strong>在multiway的局面下比<strong>AQo</strong>要更好，而<strong>AQo</strong>在heads up局面下更好。</p></li><li><p>A polarized range consists of mostly strong and weak hands. A condensed range consists of mostly medium strength hands. A betting or raising range will often but not always be polarized, and a calling range will often be condensed. </p></li></ol></blockquote><p>一个两极化的范围主要由强牌和弱牌组成，一个紧缩的范围则主要由中等牌力的牌构成。一个bet了的和raise了的范围经常但不总是两极化的，一个call的范围则经常是紧缩的。</p><blockquote><ol><li>Our betting range will almost always be polarized on the river. If we bluff too much, a strong opponent will always call with his bluff catchers, and if we bluff too little, he will always fold his bluff catchers. Thus, our goal is to make this player indifferent to calling and folding with his marginal hands. </li></ol></blockquote><p>在河牌bet几乎总是让我们的范围两极化。如果我们bluff的频率过高，一个强大的对手将会总是用他的bluff catcher来call，而如果我们bluff的频率过低，他又会总是在河牌fold掉他的bluff catcher。因此我们的目标是让我们的对手在河牌（如果他的手里是一个中等牌力的bluff catcher）的决策（call还是fold）显得无关紧要。</p><blockquote><ol><li>Hands cannot always be classified as either a value bet or a bluff. This is especially true on the flop when a bet can have both bluffing and value properties. </li></ol></blockquote><p>一手牌不能够总是被分成value bet或是bluff。这一点在翻牌轮上尤其正确，因为有时在翻牌轮上的bet既有bluff也有value bet的性质。</p><blockquote><ol><li>Our opponent should often be close to indifferent between bluffing and folding the worst hands in his bluffing range. </li></ol></blockquote><p>我们需要将我们对手的bluff保持在一个无关紧要的状态（这意味着我们要以一定频率进行适当地支付，将对手在bluff上的EV维持在0左右）</p><blockquote><ol><li>Being in position provides many benefits and greatly increases the profitability of a player’s range. </li></ol></blockquote><p>在优势位置进行游戏为我们提供了非常多的好处并且大幅提升了玩家的盈利能力。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Intro&quot;&gt;&lt;a href=&quot;#Intro&quot; class=&quot;headerlink&quot; title=&quot;Intro&quot;&gt;&lt;/a&gt;Intro&lt;/h1&gt;&lt;p&gt;从今年四月份开始一直想要系统地学习一下德州扑克关于GTO（Game Theory Optimal）的理论和技术，但是
      
    
    </summary>
    
    
      <category term="poker" scheme="http://www.shihaizhou.com/tags/poker/"/>
    
  </entry>
  
</feed>
