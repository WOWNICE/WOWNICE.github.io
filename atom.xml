<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>In Love with CodeCode</title>
  
  <subtitle>Haizhou&#39;s Blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.shihaizhou.com/"/>
  <updated>2020-06-17T08:49:25.362Z</updated>
  <id>http://www.shihaizhou.com/</id>
  
  <author>
    <name>Haizhou Shi</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Legendre Transform and Fenchel Conjugate</title>
    <link href="http://www.shihaizhou.com/2020/06/17/Legendre-Transform-and-Fenchel-Conjugate/"/>
    <id>http://www.shihaizhou.com/2020/06/17/Legendre-Transform-and-Fenchel-Conjugate/</id>
    <published>2020-06-17T05:57:34.000Z</published>
    <updated>2020-06-17T08:49:25.362Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://www.andrew.cmu.edu/course/33-765/pdf/Legendre.pdf" target="_blank" rel="noopener">CMU tutorial on Legendre transform</a></li><li><a href="https://www.youtube.com/watch?v=vgLq90cOI_M" target="_blank" rel="noopener">Youtube short explanation on Legendre transform</a></li><li><a href="http://www.onmyphd.com/?p=legendre.fenchel.transform" target="_blank" rel="noopener">http://www.onmyphd.com/?p=legendre.fenchel.transform</a></li></ul><h1 id="Legendre-Transform"><a href="#Legendre-Transform" class="headerlink" title="Legendre Transform"></a>Legendre Transform</h1><h2 id="Information-in-Functions"><a href="#Information-in-Functions" class="headerlink" title="Information in Functions"></a>Information in Functions</h2><p>函数的本质是映射，映射包含了信息，那么如何衡量一个函数包含的信息量？对于离散函数，一种想法是在值域上计算信息熵通过衡量值域上分布的混乱度来进行估算，但这并没有包含关于这个函数的所有信息：定义域信息、N-阶导函数信息都被丢失了。同时下面的一个简单函数我们就已经很难找到一种quantify它所包含的信息：</p><script type="math/tex; mode=display">f:\left\{\begin{array}{l}\mathbb{R} \rightarrow \mathbb{R}_{0}^{+} \\x \mapsto x^{2}\end{array}\right.</script><p>计算函数包含的绝对信息也许不可行，但是受到可数集上定义”size”方法的启发，我们或许可以找到一种定义两个函数包含信息量的相对关系。例如函数$y=f(x)$和它的inverse function $x=f^{-1}(y)$ 就应该包含有等量的 information，原因是它们在被表示成集合上的映射图时是完全一样的。</p><p>或许我们可以将一个函数transform成另一个函数而保持他们的信息不变，Legendre Transform就是在这个语境下的一种function transform方法。</p><h2 id="Aim"><a href="#Aim" class="headerlink" title="Aim"></a>Aim</h2><p>现有一个函数 $\mathrm{y}: x \mapsto \mathrm{y}(x)$, 它包含了很多信息，例如对于每一个给定的 $x$ 对应的函数值 $y$. 同时它也包含了<strong>slope</strong> at any given $x$. Legendre Transform的目标是将原函数转换为一个关于slope $p$ 的函数，同时保证这两个函数是具有同样的信息的。定义：</p><script type="math/tex; mode=display">p:=\mathrm{y}^{\prime}(x)</script><p><strong>Failed attempt.</strong> 一个非常直观的想法就是我们将 $x$ 用 $p$ 来表示，即 $x = \mathrm y ^{\prime -1}(p)$, 然后反向代回原来的函数 $\mathrm y$. 由此我们得到以下的transformed $\tilde{\mathrm y}$: </p><script type="math/tex; mode=display">\mathrm{y}(x) \rightarrow \tilde{\mathrm{y}}(p)=\mathrm{y}(x(p))=\mathrm{y}\left(\mathrm{y}^{\prime-1}(p)\right)</script><p>那么我们需要验证这个transform导致了一部分信息的丢失，最好的方法是通过counter example，对于函数：</p><script type="math/tex; mode=display">\mathrm{y}: x \mapsto \frac{1}{2}\left(x-x_{0}\right)^{2}</script><p>根据上式我们计算出：</p><script type="math/tex; mode=display">\tilde{\mathrm{y}}(p) = \mathrm{y}(x(p))=\frac{1}{2}\left(x(p)-x_{0}\right)^{2}=\frac{1}{2} p^{2}</script><p>我们发现最终的表达式里关于 $x_0$ 的信息丢失了：对于由不同 $x_0$ 构成的函数集合，它们都transform到了同一个函数上。但是我们只需要</p><h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><p><strong>Definition</strong> (Legendre Transformation). <strong>Legendre Transformation</strong> from a function $\mathrm y(x)$ to a new function $\mathrm y^\star (p)$ is defined as follow, where $p=\mathrm y^\prime (x)$ and no information is lost iff function $\mathrm y$ is <strong>convex</strong> (or concave, which is omitted in this blog):</p><script type="math/tex; mode=display">\mathrm y^\star (p) := \sup_x\{xp - \mathrm y(x)\}</script><p>为什么函数需要是convex: 因为只有convex的函数才能够建立 $x$ 和 $p$ 的一一映射，这也是Legendre Transform的一大限制。同时需要注意的是 $p=\mathrm y^\prime (x)$ 不是先于这个定义的条件，而是通过解决这个关于 $x$ 的最大化问题得到的结果：关于 $x$ 求导并使其导数为0，得到：</p><script type="math/tex; mode=display">0=\frac{\partial}{\partial x}\{x p-\mathrm y(x)\}=p-\mathrm y^{\prime}(x)</script><p>所以以上定义式等价于以下去掉 $\sup$ 符号的式子（在网上更多见的版本）：</p><script type="math/tex; mode=display">\mathrm y^\star (p) := xp - \mathrm y(x), \quad \operatorname{where} \space p=\mathrm y^\prime (x)</script><h2 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h2><h3 id="Geometric-interpretation"><a href="#Geometric-interpretation" class="headerlink" title="Geometric interpretation"></a>Geometric interpretation</h3><p>在空间上我们可以认为Legendre Transformation建立了某点 $x$ 对应斜率 $p$ 和该点tangent的截距之间的函数关系。具体证明非常简单，作图即可。以下是帮助理解的图例：</p><p><img src="https://i.loli.net/2020/06/17/A68WCO34VatnkGF.png" alt="image.png"></p><h3 id="Inverse-of-derivatives"><a href="#Inverse-of-derivatives" class="headerlink" title="Inverse of derivatives"></a>Inverse of derivatives</h3><p>经过transform之后的函数 $\mathrm y^\star(p)$ 关于其自变量的导数和原函数有以下关系：</p><script type="math/tex; mode=display">\mathrm{y}^{\star \prime} = \frac{\partial \mathrm{y}^{\star}(p)}{\partial p}=\underbrace{\frac{\partial \mathrm{y}(x)}{\partial x}}_{p} \frac{\partial x}{\partial p}-x(p)-\frac{\partial x}{\partial p} p=-x(p) =-\mathrm{y}^{\prime-1}</script><h3 id="Inverse-of-Legendre-Transformation"><a href="#Inverse-of-Legendre-Transformation" class="headerlink" title="Inverse of Legendre Transformation"></a>Inverse of Legendre Transformation</h3><p>The Legendre Transformation of the Legendre Transformation of a function $\mathrm y$ is $\mathrm y$ itself, which is easy to prove. </p><script type="math/tex; mode=display">^{\star \star} \mathrm{y}=\mathrm{y}</script><p>同时因为这条式子，Legendre Transformation是一个不会导致函数信息损失的transformation. </p><h3 id="Finding-the-min-value"><a href="#Finding-the-min-value" class="headerlink" title="Finding the min value"></a>Finding the min value</h3><p>在一些情况下我们不方便直接去求原函数的最小值 $\mathrm y(x)_{min}$，但我们知道 $\mathrm y(x)$ 在取到极小值的时候一阶导数 $p=\mathrm y^\prime (x)=0$. 从而我们在知道了Legendre Transformation的情况下可以直接求极小值：</p><script type="math/tex; mode=display">\begin{align}&\mathrm y^\star(0)=0x - \mathrm y(x)_{\min} \\\Rightarrow \space &\mathrm y(x)_{\min} = -\mathrm y^\star(0)\end{align}</script><h1 id="Fenchel-Conjugate"><a href="#Fenchel-Conjugate" class="headerlink" title="Fenchel Conjugate"></a>Fenchel Conjugate</h1><p>寻找一个非凸函数的凸共轭的目标是：对于在（某个定义域内）任意斜率的直线，我们要找出这个使得该直线最靠近该非凸函数的截距值。如下图，对于给定斜率为 $s$ 的直线，我们想要找到截距 $b$ 使直线尽量靠近函数。最简单的想法是对所有 $x$ 求导，在导数等于零的点中找到一个最接近的直线。下图中，非常明显 $b_0$ 优于 $b_1$. </p><p><img src="https://i.loli.net/2020/06/17/sBnIEid1bKouwDp.png" alt="image.png"></p><p>This more general rule applies to non-differentiable or non-convex functions. So, when a line with slope $\mathbf s$ crosses $f(\mathbf x)$, we have:</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{s}^{T} \mathbf{x}+b &=f(\mathbf{x}) \\b &=f(\mathbf{x})-\mathbf{s}^{T} \mathbf{x}\end{aligned}</script><p>and we want the smallest value of $b$ for all $x$. Then:</p><script type="math/tex; mode=display">\begin{aligned}b &=\inf _{\mathbf{x}}\left(f(\mathbf{x})-\mathbf{s}^{T} \mathbf{x}\right) \\&=\inf _{\mathbf{x}}\left(-\left(\mathbf{s}^{T} \mathbf{x}-f(\mathbf{x})\right)\right) \\&=-\sup _{\mathbf{x}}\left(\mathbf{s}^{T} \mathbf{x}-f(\mathbf{x})\right)\end{aligned}</script><p>所以从这里来看我们上面定义的Legendre Transform实际上是希望能够建立一个从slope到截距的映射，而这里我们的Fenchel Conjugate也是为了建立这样的映射：$f^{\star}(\mathbf{s})=-b$. 故而我们得到了Fenchel Conjugate的表示：</p><script type="math/tex; mode=display">f^{\star}(\mathbf{s})=-b=\sup _{\mathbf{x}}\left(\mathbf{s}^{T} \mathbf{x}-f(\mathbf{x})\right)</script><p>This is the <strong>Legendre-Fenchel transform</strong>, also known as <strong>convex conjugate</strong>. Note that now the transform is <strong>not reversible</strong>, i.e., you cannot get the original function by applying the transform to the transform. On the other hand, <strong>the transform of the transform is convex</strong>, even if the original function is not.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.andrew.cmu.edu/course/33-765/pdf/Legendre.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
  </entry>
  
  <entry>
    <title>Generative Adversarial Networks</title>
    <link href="http://www.shihaizhou.com/2020/06/16/Generative-Adversarial-Networks/"/>
    <id>http://www.shihaizhou.com/2020/06/16/Generative-Adversarial-Networks/</id>
    <published>2020-06-16T11:52:31.000Z</published>
    <updated>2020-06-18T02:06:08.338Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://math.stackexchange.com/questions/2435464/show-that-max-function-on-mathbb-rn-is-convex" target="_blank" rel="noopener">Prooving max function is convex.</a></li></ul><h1 id="Vanilla-GAN"><a href="#Vanilla-GAN" class="headerlink" title="Vanilla GAN"></a>Vanilla GAN</h1><p>Deep Learning研究的问题本质上是希望学习从一个数据空间到另一个数据空间的映射；更准确地，一个distribution向另一个distribution的映射。在分类任务中，这两个分布分别为合法数据点的分布向0-1分布的映射；在回归任务中，这两个分布分别为特征空间内数据点的分布向预测空间内的分布。</p><p>在GAN的语境下，我们希望能够找到一个generator $G$ 将任意一个随机分布（在实践中通常是 Normal Distribution 或 Uniform Distribution）映射到生成网络分布 $P_G$ 中，并且我们希望 $P_G$ 和 $P_{data}$ 的分布能够尽量接近，divergence被用来度量两个分布的相似度，常见的为KL divergence和JS divergence. 以下为优化目标。</p><script type="math/tex; mode=display">G^* = \arg\min_G \operatorname{Div}(P_G, P_{data})</script><p>GAN的核心思想在于将寻找最优generator $G$ 的过程刻画成一个由generator和discriminator两者构成的minmax game：</p><script type="math/tex; mode=display">G^* = \arg \min_G \max_DV(D, G)</script><p>其中的优化目标函数为$V$</p><script type="math/tex; mode=display">V(D,G)=\mathbb E_{x\sim P_{data}}[\log D(x)] + \mathbb E_{x\sim P_{G}}[\log (1-D(x=G(z))]</script><h2 id="Relation-to-JS-Divergence"><a href="#Relation-to-JS-Divergence" class="headerlink" title="Relation to JS Divergence"></a>Relation to JS Divergence</h2><p>下面我们证明以上的优化函数在discriminator取最优的情况下等价于 $P_G$ 和 $P_{data}$ 的 JS divergence $\operatorname{JS}(P_G | P_{data})$. </p><p>首先我们将上式</p><script type="math/tex; mode=display">\begin{align}V(D,G) &=\mathbb E_{x\sim P_{data}}[\log D(x)] + \mathbb E_{x\sim P_{G}}[\log (1-D(x=G(z))] \\&= \int P_{data}(x)\log D(x) dx + \int P_{G}(x)\log (1-D(x=G(z))) dx \\&= \int [P_{data}(x)\log D(x) +  P_{G}(x)\log (1-D(x=G(z)))] dx \end{align}</script><p>从优化discriminator function $D$ 的角度，我们希望通过构造 $D$ 使得上式对于给定的generator $G$ 能够被最大化。最优的 $D(x)$ 在任意一个 $x$ 的取值下都是最优的：</p><script type="math/tex; mode=display">D^* = [D(x)]^* \quad \forall x \in \operatorname{dom}_V</script><p>对于给定的输入 $x$，对应该输入的最优解 $ [D(x)]^*$ 为：</p><script type="math/tex; mode=display">\begin{align}[D(x)]^* =\arg \max_{D(x)} [P_{data}(x)\log D(x) +  P_{G}(x)\log (1-D(x))] \end{align}</script><p>对该式两边关于$D(x)$求偏导并令其为0，得到：</p><script type="math/tex; mode=display">\frac{\partial }{\partial D(x)}[P_{data}(x)\log D(x) +  P_{G}(x)\log (1-D(x))]= \frac{P_{data}}{D(x)}-\frac{P_{G}}{1-D(x)}=0</script><p>由此解得，</p><script type="math/tex; mode=display">[D(x)]^* = \frac{P_{data}(x)}{P_{data}(x)+P_G(x)}</script><p>将 $D^*$ 带入 $V(D,G)$ 则有以下推论：</p><script type="math/tex; mode=display">\begin{align}V(D^*,G) &= E_{x\sim P_{data}}\left[\log \frac{P_{data}(x)}{P_{data}(x) + P_G(x)}\right] + E_{x\sim P_{G}}\left[\log \frac{P_{G}(x)}{P_{data}(x) + P_G(x)}\right] \\& \operatorname{let:}\quad {P_M=\frac{(P_{data}+P_G)}{2}} \\&=E_{x\sim P_{data}}\left[\log \frac{P_{data}(x)/2}{P_M(x)}\right] + E_{x\sim P_{G}}\left[\log \frac{P_{G}(x)/2}{P_M(x)}\right] \\&= -2\log 2 + \operatorname{KL}(P_{data}\|P_M) + KL(P_G\|P_M)   \\&=-2\log 2 + 2 \operatorname{JS}(P_G\|P_{data})\end{align}</script><p>因此优化GAN的训练目标近似等价于优化两个distribution之间的JS divergence. 从另外一个角度来看，任意的discriminator $D$ 函数是 $\operatorname{JS}(P_{data} | P_G)$ 的一个下界，优化两者之间的divergence的逻辑就是先找到这个divergence的一个下界，然后最大化这个下界，然后以这个下界作为优化目标的estimate，再优化目标。</p><h1 id="f-GAN"><a href="#f-GAN" class="headerlink" title="f-GAN"></a>f-GAN</h1><p>Vanilla GAN的优化目标是JS divergence $\operatorname{JS}(P_{data} | P_G)$, 而f-GAN这篇工作则把满足一定特性的不同divergence都归在了同一个f-divergence的框架下，并且提出了对应在minmax game中优化函数的表达式。</p><h2 id="The-f-divergence-Family"><a href="#The-f-divergence-Family" class="headerlink" title="The f-divergence Family"></a>The f-divergence Family</h2><p>A large class of different divergences are the so called <strong>f-divergences</strong>, also known as the Ali-Silvey distances. Given two distributions $P$ and $Q$ that possess, respectively, an absolutely continuous density function $p$ and $q$ with respect to a base measure $dx$ defined on the domain $\mathcal{X}$ , we define the f-divergence:</p><script type="math/tex; mode=display">D_{f}(P \| Q)=\int_{\mathcal{X}} q(x) f\left(\frac{p(x)}{q(x)}\right) \mathrm{d} x</script><p>where the generator function $f: \mathbb R_+ \rightarrow \mathbb R$ is a <strong>convex</strong>, lower-semicontinuous function satisfying $f (1) = 0<br>$.</p><h2 id="Estimating-f-divergence"><a href="#Estimating-f-divergence" class="headerlink" title="Estimating f-divergence"></a>Estimating f-divergence</h2><p>在f-GAN原文中，作者称利用了convex conjugate来对下界进行estimate. 但是由于在上一章节作者将f-divergence中的函数限定在了convex function集合上，所以使用的Fenchel conjugate也就退化成了Legendre Transformation. 这一退化对于下面estimate的推导非常关键. </p><p>首先我们写出一个任意一个函数的Fenchel conjugate</p><script type="math/tex; mode=display">f^{*}(t)=\sup _{u \in \operatorname{dom}_{f}}\{u t-f(u)\}</script><p>Since $f$ is convex, we have</p><script type="math/tex; mode=display">f(u)=f^{*}(f^*(u))=\sup _{t \in \operatorname{dom}_{f^*}}\{u t-f^*(t)\}</script><p>将上式带入 f-divergence, we have</p><script type="math/tex; mode=display">D_{f}(P \| Q)=\int_{\mathcal{X}} q(x) \sup _{t \in \operatorname{dom}_{f^{*}}}\left\{t \frac{p(x)}{q(x)}-f^{*}(t)\right\} \mathrm{d} x</script><p><strong>Definition</strong> (Jensen Inequality). If $X$ is a random variable and $\varphi$ is a convex function, then following inequality holds. </p><script type="math/tex; mode=display">\varphi(\mathbb{E}[X]) \leq \mathbb{E}[\varphi(X)]</script><p>同时因为 $\sup$ 和 $\max$ 一样都是convex function. 所以利用Jensen Inequality，得到</p><script type="math/tex; mode=display">\begin{aligned}D_{f}(P \| Q) &=\int_{\mathcal{X}} q(x) \sup _{t \in \operatorname{dom}_{f^{*}}}\left\{t \frac{p(x)}{q(x)}-f^{*}(t)\right\} \mathrm{d} x \\& \geq \sup _{T \in \mathcal{T}}\left(\int_{\mathcal{X}} p(x) T(x) \mathrm{d} x-\int_{\mathcal{X}} q(x) f^{*}(T(x)) \mathrm{d} x\right) \\&=\sup _{T \in \mathcal{T}}\left(\mathbb{E}_{x \sim P}[T(x)]-\mathbb{E}_{x \sim Q}\left[f^{*}(T(x))\right]\right)\end{aligned}</script><p>where $t=T(x)$, $T: \mathcal{X} \rightarrow \mathbb{R}$. 接下来我们推导原文中略过的部分：在Legendre Transformation的语境下我们可以求出相对紧的下界 $T^*$ (注意不要和conjugate的符号混淆). </p><p>首先因为Legendre Transformatio中 $f$ is convex, 所以我们可以求出 $f^*(t)=\sup_x\{xt-f(x)\}$ 的唯一解：</p><script type="math/tex; mode=display">\begin{align}\max_t \space &xt-f(x)\\\Rightarrow \space & 0= \frac{\partial}{\partial x}\left[xt-f(x)\right] = t-f^\prime(x) \\\Rightarrow \space & t = f^\prime(x)\\\Leftrightarrow \space & x = (f^{\prime})^{-1}(t)\end{align}</script><p>将最后的结果代回 $f^*(t)$ 得到</p><script type="math/tex; mode=display">f^*(x) =x f^{\prime-1}(x)-f(f^{\prime-1}(x))</script><p>对于 $f^*(x)$ 求关于自变量 $x$ 的一阶导数得到</p><script type="math/tex; mode=display">\begin{align}\frac{\partial f^*(x)}{\partial x} &= f^{\prime-1}(x) + x \frac{\partial f^{\prime-1} (x)}{\partial x} - \frac{\partial f(f^{\prime-1} (x))}{\partial f^{\prime-1} (x)} \frac{\partial f^{\prime-1} (x)}{\partial x} \\&= f^{\prime-1}(x) + x \frac{\partial f^{\prime-1} (x)}{\partial x} - f^{\prime}(f^{\prime-1} (x))\frac{\partial f^{\prime-1} (x)}{\partial x} \\&= f^{\prime-1}(x)\end{align}</script><p>利用以上结果，我们希望去找到对应的 $t=T(x)$ 表达式使得 $\sup$ 能够被最大化，解得</p><script type="math/tex; mode=display">T^{*}(x)=f^{\prime}\left(\frac{p(x)}{q(x)}\right)</script><p>以下展示了不同的f-divergence和对应的最优estimate $T^*$ </p><p><img src="https://i.loli.net/2020/06/17/oOAszpE5t8W6rqj.png" alt="image.png"></p><h2 id="Variational-Divergence-Minimization-VDM"><a href="#Variational-Divergence-Minimization-VDM" class="headerlink" title="Variational Divergence Minimization (VDM)"></a>Variational Divergence Minimization (VDM)</h2><p>To this end, we follow the generative-adversarial approach and use two neural networks, $Q$ and $T$ . $Q$ is our generative model, taking as input a random vector and outputting a sample of interest. We parametrize $Q$ through a vector $\theta$ and write $Q_\theta$. $T$ is our variati onal function, taking as input a sample and returning a scalar. We parametrize $T$ using a vector ω and write $T_\omega$ .</p><script type="math/tex; mode=display">F(\theta, \omega)=\mathbb{E}_{x \sim P}\left[T_{\omega}(x)\right]-\mathbb{E}_{x \sim Q_{\theta}}\left[f^{*}\left(T_{\omega}(x)\right)\right]</script><p>以上给出了基于f-divergence的minmax game. </p><h1 id="WGAN"><a href="#WGAN" class="headerlink" title="WGAN"></a>WGAN</h1><p>WGAN使用了在f-GAN family 之外的用于衡量两个分布的距离函数: Earth-Mover (EM) distance or Wasserstein-1. </p><p><strong>Definition</strong> (Earth-Mover Distance). The <strong>Earth-Mover distance</strong> between two distribution $\mathbb P_r$ and $\mathbb P_g$ is defined as</p><script type="math/tex; mode=display">W\left(\mathbb{P}_{r}, \mathbb{P}_{g}\right)=\inf _{\gamma \in \Pi\left(\mathbb{P}_{r}, \mathbb{P}_{g}\right)} \mathbb{E}_{(x, y) \sim \gamma}[\|x-y\|]</script><p>where $\Pi (\mathbb P_r, \mathbb P_g)$ denotes the set of all joint distributions $\gamma (x,y)$ whose marginals are respectively $\mathbb P_r$ and $\mathbb P_g$. Intuitively, $\gamma (x,y)$ indicates how much “mass” must be transported from $x$ to $y$ in order to transform the distributions $\mathbb P_r$ into the distribution $\mathbb P_g$. The EM distance then is the “cost” of the optimal transport plan. </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://math.stackexchange.com/questions/2435464/show-that-max-function-on-mathbb-rn-is-
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
      <category term="GAN" scheme="http://www.shihaizhou.com/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>Contrastive Learning from the Perspective of Manifold Learning</title>
    <link href="http://www.shihaizhou.com/2020/06/15/Contrastive-Learning-from-the-Perspective-of-Manifold-Learning/"/>
    <id>http://www.shihaizhou.com/2020/06/15/Contrastive-Learning-from-the-Perspective-of-Manifold-Learning/</id>
    <published>2020-06-15T14:02:30.000Z</published>
    <updated>2020-06-15T14:30:15.516Z</updated>
    
    <content type="html"><![CDATA[<p>In Isomap method, the target of representation space is to keep the Geodesic Distance between arbitrary two points: </p><script type="math/tex; mode=display">\min_{\theta} \mathbb E[|d_{G}(x_i, x_j)-d_E(f(x_i;\theta), f(x_j;\theta))|]</script><p>In contrastive learning, the target of representation space is that there exists a critic that can distinguish positive/negative sample pairs. The procedure of optimizing the contrastive loss is</p><script type="math/tex; mode=display">\min_{\theta} \mathbb E[|d_{G}(x_i, x_j)-d_E(f(x_i;\theta), f(x_j;\theta))|]</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;In Isomap method, the target of representation space is to keep the Geodesic Distance between arbitrary two points: &lt;/p&gt;
&lt;script type=&quot;ma
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
      <category term="Contrastive" scheme="http://www.shihaizhou.com/tags/Contrastive/"/>
    
      <category term="Manifold" scheme="http://www.shihaizhou.com/tags/Manifold/"/>
    
  </entry>
  
  <entry>
    <title>Manifold Learning</title>
    <link href="http://www.shihaizhou.com/2020/06/14/Manifold-Learning/"/>
    <id>http://www.shihaizhou.com/2020/06/14/Manifold-Learning/</id>
    <published>2020-06-14T11:51:27.000Z</published>
    <updated>2020-06-16T11:45:38.547Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://scikit-learn.org/stable/modules/manifold.html" target="_blank" rel="noopener">sklearn page introducing different manifold learning methods</a></li><li><a href="http://www.cad.zju.edu.cn/reports/%C1%F7%D0%CE%D1%A7%CF%B0.pdf" target="_blank" rel="noopener">Prof.He’s slides on manifold learning</a></li><li><a href="https://www.youtube.com/watch?v=yBwpo-L80Mc" target="_blank" rel="noopener">hopkins lecture on LLE</a></li></ul><h1 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h1><p>High-dimensional datasets can be very difficult to visualize. While data in two or three dimensions can be plotted to show the inherent structure of the data, equivalent high-dimensional plots are much less intuitive. To aid visualization of the structure of a dataset, the dimension must be reduced in some way.</p><p>Manifold Learning can be thought of as an attempt to generalize linear frameworks like PCA to be sensitive to non-linear structure in data. Though supervised variants exist, the typical manifold learning problem is unsupervised: it learns the high-dimensional structure of the data from the data itself, without the use of predetermined classifications.</p><p><strong>Definition</strong> (Geodesic Distance). A geodesic line is the shortest path between two points on a curved surface, like Earth (referring to the following figure). </p><p><img src="https://i.loli.net/2020/06/14/y6joT37BIbcMdPu.png" alt="image.png"></p><p>我认为在Manifold Learning这个领域的最重要假设在于：足够接近的数据点的测地线距离等于他们的欧氏距离。也就是说一小簇点在高维空间可以被看做分布在一个平面上，进而满足线性关系。</p><h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><p>以下方法建立在共同的notation下：我们有一高维数据点集合 $X=\{x_i\}_{i=1}^{N} \sub \mathbb R^D$，现有函数 $f: \mathbb R^D \rightarrow \mathbb R^d$为将高维数据空间映射到表征空间的映射函数，映射后的表征空间$(Y, \rho)$ 是一个欧拉空间，其中：</p><script type="math/tex; mode=display">\begin{align}Y &=\{y_i\}_{i=1}^{N} \sub \mathbb R^d \\\rho_{ij} &=\rho(y_i, y_j) = \|y_i-y_j\|_2\end{align}</script><p>需要注意的是，在Manifold Learning中的$f$经常是没有办法用于新样本的推断的。</p><h2 id="Multidimensional-Scaling-MDS"><a href="#Multidimensional-Scaling-MDS" class="headerlink" title="Multidimensional Scaling (MDS)"></a>Multidimensional Scaling (MDS)</h2><p>本质上来说，Manifold Learning的学习目标是希望在表征空间上保持整个数据分布的geometry property. 因为我们认为高维数据点在高维空间是分布在一个流型上的，所以距离度量方式并不应当采用欧氏距离，例如如下的优化目标就不合适：</p><script type="math/tex; mode=display">\min_f\sum_{i,j}(\rho_{ij}-d_{ij})^2</script><p>其中$d_{ij}=d_E(x_i,x_j)$，表示在样本空间的欧氏距离。但基于manifold learning的假设，也就是在一个数据点的很小邻域内，样本间可以近似看作是线性的，因此我们可以只考虑每一个点的邻近点的欧氏距离需要在表征空间中被近似，所以以下的优化目标可以看做保持了样本分布的geometry attributes：</p><script type="math/tex; mode=display">\min_f \sum_{i=1}^N\sum_{j\sim i}(\rho_{ij}-d_{ij})^2</script><p>以上的优化目标就是MDS的主体部分，它是non-trivial的，虽然我们可以使用梯度下降进行优化。</p><h3 id="MDS-on-dot-product"><a href="#MDS-on-dot-product" class="headerlink" title="MDS on dot product"></a>MDS on dot product</h3><p>考虑MDS的使用dot product作为度量两个数据点的距离：$K_{ij}=x_i^Tx_j$. 全局优化目标为：</p><script type="math/tex; mode=display">\min_f \sum_{i,j}(y_i^Ty_j -K_{ij})^2</script><p>于是等价于优化以下目标</p><script type="math/tex; mode=display">\min \|Y^TY-K\|_F^2</script><p>也就是说MDS在dot product情况下退化 成了基于SVD的dimensional reduction method. </p><script type="math/tex; mode=display">\begin{align}K &= U\Sigma U^T \\Y &= U_{1:d}\Sigma_{i:d}\end{align}</script><h2 id="Isomap"><a href="#Isomap" class="headerlink" title="Isomap"></a>Isomap</h2><p>One of the earliest approaches to manifold learning is the Isomap algorithm, short for Isometric Mapping.  Isomap seeks a lower-dimensional embedding which maintains geodesic distances between every two points. </p><h3 id="Estimating-Geodestic-Distance"><a href="#Estimating-Geodestic-Distance" class="headerlink" title="Estimating Geodestic Distance"></a>Estimating Geodestic Distance</h3><p>We could estimate the geodestic distance by constructing an adjacency graph on which the shortest distance between two nodes is the estimation of their geodestic distance. We can set the adjacency matrix following the rule: </p><script type="math/tex; mode=display">W_{i j}=\left\{\begin{array}{cc}1 & \left\|x_{i}-x_{j}\right\|<\epsilon \\0 & \text { otherwise }\end{array}\right.</script><p>在sklearn主页上，该算法步骤主要由三步构成，第一是为每个点构建Nearest Neighbors，连接这些neighbors就得到了一个在manifold上的连通图；接着需要计算任意两个点之间的geodestic distance；最后使用Partial Eigenvalue Decomposition求得降维后的表示。</p><p>Isomap使用 MDS 计算映射后的坐标$y$，使得映射坐标下的欧氏距离与原来的测地线距离尽量相等.</p><script type="math/tex; mode=display">\min _{f} \sum_{i, j}\left(\rho_{ij} - d_{M}\left(x_{i}, x_{j}\right) \right)^{2}</script><h2 id="Locally-Linear-Embedding"><a href="#Locally-Linear-Embedding" class="headerlink" title="Locally Linear Embedding"></a>Locally Linear Embedding</h2><p>Locally linear embedding (LLE) seeks a lower-dimensional projection of the data which preserves distances within local neighborhoods. It can be thought of as a series of local Principal Component Analyses which are globally compared to find the best non-linear embedding.</p><p>“流形在局部可以近似等价于欧氏空间”是 LLE 分析方法的出发点。LLE认为一个中心数据点 $x_i$ 能够被处于其小邻域内的点$\{x_j\}_{j \sim i}$线性重构，重构的权重可能是我们想要在表征空间上保留的geometry attributes。</p><script type="math/tex; mode=display">x_i \approx \sum_{j\sim i} W_{ij}x_j</script><p>因而该算法由下面两步构成，首先计算点之间的最优重构权重需要解决以下的优化问题，对$d$维的表征空间，我们需要使用$k\geq d$的kNN来寻找邻接节点。</p><script type="math/tex; mode=display">\begin{align}&\min_{W}\sum_{i=1}^{N}\left\|x_{i}-\sum_{j \sim i} W_{i j} x_{j}\right\|^{2} \\&s.t. \sum_{j\sim i}W_{ij}=1,\quad \forall i \in \{1, 2, \cdots, N\}\end{align}</script><p>接着以此作为目标使映射空间中的表征保留该权重，其中对于表征空间上的单位向量的限制是为了防止0向量这个trivial solution.</p><script type="math/tex; mode=display">\begin{align}\min_Y &\sum_{i=1}^{N} \left\|y_{i}-\sum_{j \sim i} W_{i j} y_{j}\right\|^{2} \\s.t. &\sum_{i=1}^{N} y_i = 0 \\& YY^T = I_d\end{align}</script><h2 id="Laplacian-Eigenmap"><a href="#Laplacian-Eigenmap" class="headerlink" title="Laplacian Eigenmap"></a>Laplacian Eigenmap</h2><p>是一种Spectral Embedding方法，可以说是从Spectral Clustering衍生出来的，根本的目标依旧是希望在高维空间接近的点在降维之后也更为接近。我们的优化目标为：</p><script type="math/tex; mode=display">J(y) = \sum_{i,j}W_{ij}(y_i-y_j)^2=y^{\dagger}Ly</script><p>我们可以将optimization problem进行一些转化：</p><script type="math/tex; mode=display">\arg\min y^{\dagger}Ly \Leftrightarrow \arg\min \langle Ly, y\rangle</script><p>它们都受到了以下约束：</p><script type="math/tex; mode=display">\begin{align}y^{\dagger} L y &=1 \\y^{\dagger} D 1 &=0\end{align}</script><p>其中第一条是约束向量的scale，第二条是为了避免 $y=1$的trivial solution.  </p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>意图对Manifold Learning不使用传统解析解而使用NN+BP的方法进行优化，并观察在representation空间上的表现。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/manifold.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;sk
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
      <category term="Manifold" scheme="http://www.shihaizhou.com/tags/Manifold/"/>
    
  </entry>
  
  <entry>
    <title>Understanding Contrastive Representation Learning</title>
    <link href="http://www.shihaizhou.com/2020/06/11/Understanding-Contrastive-Representation-Learning-through-Alignment-and-Uniformity-on-the-Hypersphere/"/>
    <id>http://www.shihaizhou.com/2020/06/11/Understanding-Contrastive-Representation-Learning-through-Alignment-and-Uniformity-on-the-Hypersphere/</id>
    <published>2020-06-11T13:14:56.000Z</published>
    <updated>2020-06-15T11:09:08.003Z</updated>
    
    <content type="html"><![CDATA[<h1 id="QA"><a href="#QA" class="headerlink" title="QA"></a>QA</h1><p><strong>Q1.</strong> For classification tasks, when we use contrastive learning as the self-supervised objective, the uniformity of the representations is optimized but how the linear-separability is naturally optimized along with uniformity?<br><strong>A1.</strong> In my opinion, the reason that class concentration happens is ultimately the inductive bias of neural networks. Afterall, unrestricted function class, there definitely are encoders that are very aligned and uniform, but gives useless features, in terms of linear classification at least. If you believe the inductive bias of NN tends to lead to “smooth” solutions, then intuitively class concentration happens. It is certainly difficult to argue about this formally though.</p><p><strong>Q2.</strong> For tasks requiring structured output (e.g. reconstruction), I could understand that uniformity is desired since we want the representation to be as different as possible so that when constructing output, different samples are not going to be confused. Then I wonder if the contrastive method is better than the Autoencoder-based method (in this scenario). My assumption is “no” since the contrastive method also pushes similar samples away from each other, while it’s against our instinct that similar inputs shall have similar representations.<br><strong>A2.</strong> I’d say it depends on how you choose positive pairs in contrastive learning.  Surely this could be true if you ask two random crops of the same image to have the same features. </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;QA&quot;&gt;&lt;a href=&quot;#QA&quot; class=&quot;headerlink&quot; title=&quot;QA&quot;&gt;&lt;/a&gt;QA&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;Q1.&lt;/strong&gt; For classification tasks, when we use contrastive
      
    
    </summary>
    
    
      <category term="Contrastive" scheme="http://www.shihaizhou.com/tags/Contrastive/"/>
    
  </entry>
  
  <entry>
    <title>Disentangled Representation Learning via Mutual Information Estimation</title>
    <link href="http://www.shihaizhou.com/2020/06/10/Learning-Disentangled-Representations-via-Mutual-Information-Estimation/"/>
    <id>http://www.shihaizhou.com/2020/06/10/Learning-Disentangled-Representations-via-Mutual-Information-Estimation/</id>
    <published>2020-06-10T09:03:39.000Z</published>
    <updated>2020-06-10T10:39:59.943Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/abs/1912.03915" target="_blank" rel="noopener">Learning Disentangled Representations via Mutual Information Estimation</a></li></ul><h1 id="Method-Description"><a href="#Method-Description" class="headerlink" title="Method Description"></a>Method Description</h1><p>Given a pair of images sharing some attributes, we aim to create a low-dimensional representation which is split into two parts: a shared representation that captures the common information between the images and an exclusive representation that contains the specific information of each image.</p><p><strong>Two stages of training.</strong> First, the shared representation is learned via cross mutual information estimation and maximization. Secondly, mutual information maximization is performed to learn the exclusive representation <strong>while minimizing the mutual information</strong> between the shared and exclusive representations.</p><p><img src="https://i.loli.net/2020/06/10/MBR4Nm38ZoYDCHS.png" alt="image.png"></p><p>两个样本的representation包括了shared information and exclusive information. </p><p>首先在训练的第一阶段，我们希望他们的<strong>shared representation能够尽量相同</strong>，于是文章采用了叫做cross mutual information maximization的方法，将 (sample，local patch, representation) 三元组 $(X, C_X, R_X)$ 和另一个样本的三元组$(Y, C_Y, R_Y)$进行”交叉互信息最大化”:</p><script type="math/tex; mode=display">\max [I(X, R_Y)+I(Y, R_X)] + [I(C_X,R_Y)+I(C_Y,R_X)]</script><p>接着，我们希望他们的<strong>exclusive representation能尽量不同</strong>。于是文章在第二阶段，固定了生成shared representation的模型参数，同时在两方面训练网络：最大化合并之后的representation的global/local MI，同时最小化exclusive/shared MI. </p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>关于实验的和整个任务的setting，有以下的问题和作者进行交流。</p><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><p>Dear Eduardo Hugo Sanchez,</p><p>After reading your wonderful paper “Learning Disentangled Representations via Mutual Information Estimation”, I have one question regarding the setup of your training procedure, which is:</p><p>How do you decide between which two images the MI is maximized during training? Say in Colorful MNIST, if you put the images of the same digit together and follow your obejective, then can I conclude that you’re actually telling the model to learn a (linear, in some cases) seperable representation regarding the digit classification? Below is how I come to this conclusion:<br>If we are maximizing the MI between all the images containing the same digit, then during the process of MI maximization, we will shuffle the whole batch of the data in order to form the negative samples, which will be feed into the critic function. Since the critic function has to distinguish the samples like (X,Y)=(black7, red7) and the shuffled samples like (X,Y’)=(black7, yellow10), we are explicitly telling the model to classify the digits.<br>If we train the whole network in a totally unsupervised way, i.e., training sample pairs like (X,Y)=(black7, yellow10) are randomly showing up asking the model to learn the shared information between them, how the method is able to learn the disentangled representations is really confusing me… </p><p>Furthermore, did you do the ablation study of the “cross mutual information maximization” technique? How did it go? </p><h2 id="Reply-from-author"><a href="#Reply-from-author" class="headerlink" title="Reply from author"></a>Reply from author</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1912.03915&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Learning Disentangled R
      
    
    </summary>
    
    
      <category term="MI" scheme="http://www.shihaizhou.com/tags/MI/"/>
    
  </entry>
  
  <entry>
    <title>InfoAE - Unpublished</title>
    <link href="http://www.shihaizhou.com/2020/06/09/InfoAE-Unpublished/"/>
    <id>http://www.shihaizhou.com/2020/06/09/InfoAE-Unpublished/</id>
    <published>2020-06-09T04:17:07.000Z</published>
    <updated>2020-06-09T08:21:02.053Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/abs/1904.08613" target="_blank" rel="noopener">arxiv preprint paper “DISENTANGLED REPRESENTATION LEARNING WITH INFORMATION MAXIMIZING AUTOENCODER”</a></li><li><a href="https://github.com/ABedychaj/InfoAE" target="_blank" rel="noopener">InfoAE github repo (not sure)</a></li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>该工作将 GAN 和 AE 进行了融合，虽然和InfoGAN一样提到了想要maximize MI，但是该篇工作并没有使用InfoMax或是Variational InfoMax直接对他们的MI进行优化，而是采用了一个classifier迫使encoder编码分类信息，最终文章纯粹依靠非监督信息在分类任务上取得了非常高的得分。</p><h1 id="InfoAE"><a href="#InfoAE" class="headerlink" title="InfoAE"></a>InfoAE</h1><p><img src="https://i.loli.net/2020/06/09/kJjnc1KpyzSQg37.png" alt="image.png"></p><p>InfoAE可以分为Conditional GAN和Autoencoder两个部分，如上图所示。</p><p><strong>Conditional GAN</strong>的部分对应于上图的红色流程+黄色流程：首先 $z$ 是prior random noise，$c$ 是latent code；在实验中取 $c$ 为 $K=10$ 的one-hot coding. 经过Generator Network $G$ 之后映射到了latent representation space $r$；接着 $r$ 通过Decoder Network得到了fake sample $\hat {x_g}$. 将 fake sample 和 true sample进行对比从而得到了GAN loss. 同时因为该方法是为了下游的分类任务服务，所以我们希望Encoder能够将分类信息进行编码。但是对于一个完全无标签的非监督学习样本$x$来说，我们是没有办法显式地为他进行分类的，唯一编码了分类信息的变量就是latent code $c$. 因此我们将得到的中间表示 $r$ 重新经过一个Classifier Network，将得到的分类结果重新映射回 $c$. </p><p>在<strong>Autoencoder</strong>部分就是最简单的Reconstruction Error，对应于上图中的绿色流程。</p><p>除此之外，InfoAE 还将 Autoencoder 和 Generator 生成的 <strong>representation space进行了alignment</strong>. 具体方法也是采用GAN的discriminative training将两个分布的divergence变小。我的问题是：为什么需要将原本的input和representation进行concatenation再最小化两个分布的divergence？为什么不可以直接将两个representation进行discriminative training?</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>We have evaluated the model on MNIST dataset and received outstanding results. InfoAE is trained on MNIST training data without any labels. After trainning, We encoded the test data with Encoder, E and got classification label with the Classifier, C. Then we clustered the test data according to label and received classification accuracy of 98.9 (±.05), which is better than the popular methods as shown in Table 1.</p><p><img src="https://i.loli.net/2020/06/09/vfh5pcbKreg8xna.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1904.08613&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;arxiv preprint paper “D
      
    
    </summary>
    
    
      <category term="MI" scheme="http://www.shihaizhou.com/tags/MI/"/>
    
      <category term="GAN" scheme="http://www.shihaizhou.com/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>InfoGAN</title>
    <link href="http://www.shihaizhou.com/2020/06/08/InfoGAN/"/>
    <id>http://www.shihaizhou.com/2020/06/08/InfoGAN/</id>
    <published>2020-06-08T11:37:22.000Z</published>
    <updated>2020-06-09T04:32:47.958Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/abs/1606.03657" target="_blank" rel="noopener">InfoGAN paper</a></li><li><a href="https://pdfs.semanticscholar.org/61f4/f67fc0e73fa3aef8628aae53a4d9b502d381.pdf" target="_blank" rel="noopener">Understanding Mutual Information and its Use in InfoGAN</a></li><li><a href="https://github.com/Natsu6767/InfoGAN-PyTorch" target="_blank" rel="noopener">pytorch-infogan github repo</a></li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p><strong>Problem of unsupervised representation learning.</strong> While unsupervised learning is ill-posed because the relevant downstream tasks are unknown at training time, a disentangled representation, one which explicitly represents the salient attributes of a data instance, should be helpful for the relevant but unknown tasks. Thus, to be useful, an unsupervised learning algorithm must in effect correctly guess the likely set of downstream classification tasks without being directly exposed to them.</p><p><strong>How to learn disentangled representation in this paper.</strong> In this paper, we present a simple modification to the generative adversarial network objective that encourages it to learn interpretable and meaningful representations. We do so by maximizing the mutual information between a fixed small subset of the GAN’s noise variables and the observations, which turns out to be relatively straightforward.</p><h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><h2 id="Mutual-Information-for-Inducing-Latent-Codes"><a href="#Mutual-Information-for-Inducing-Latent-Codes" class="headerlink" title="Mutual Information for Inducing Latent Codes"></a>Mutual Information for Inducing Latent Codes</h2><p>在GAN的生成中，generator将prior distribution映射到样本空间的过程并没有对“如何利用这个distribution”进行限制. As a result, it is possible that the noise will be used by the generator in a highly entangled way, causing the individual dimensions of $z$ to not correspond to semantic features of the data.</p><p>In this paper, rather than using a single unstructured noise vector, we propose to decompose the input noise vector into two parts: (i) $z$, which is treated as source of incompressible noise; (ii) $c$, which we will call the latent code and will target the salient structured semantic features of the data distribution. </p><p>We provide the generator network with both the incompressible noise z and the latent code c, so the form of the generator becomes $G(z, c)$. However, in standard GAN, the generator is free to ignore the additional latent code $c$ by finding a solution satisfying $P_G(x|c) = P_G(x)$. 为了解决trivial codes的问题，我们认为code $c$ 和生成结果generator distribution $G(z, c)$之间有较强的dependency，也就是说 $I(c; G(z,c))​$ 应当较高. 最终，InfoGAN优化的目标为以下：</p><script type="math/tex; mode=display">\min _{G} \max _{D} V_{I}(D, G)=V(D, G)-\lambda I(c ; G(z, c))</script><h2 id="Variational-Mutual-Information-Maximization"><a href="#Variational-Mutual-Information-Maximization" class="headerlink" title="Variational Mutual Information Maximization"></a>Variational Mutual Information Maximization</h2><p>PASS. We will use InfoMax method to reproduce the experimental results. </p><p><img src="https://i.loli.net/2020/06/08/toTXH429CPzJhG8.png" alt="image.png"></p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>GAN的实验部分通常都比较偏经验性，并没有太好的Numerical Metrics来进行比较。值得一提的是，InfoGAN和之前看到的一篇未投稿工作一样，在MNIST数据集上验证时将latent code设置成了$K=10$ 的one-hot coding。我认为在这种setting下，representation进入下游分类任务取得很好的成绩是因为10-way classification是关于下游任务非常重要的信息泄露。以下我们记录一下使用InfoMax来优化InfoGAN的实验。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1606.03657&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InfoGAN paper&lt;/a&gt;&lt;/li&gt;

      
    
    </summary>
    
    
      <category term="MI" scheme="http://www.shihaizhou.com/tags/MI/"/>
    
      <category term="GAN" scheme="http://www.shihaizhou.com/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>Matplotlib Cookbook</title>
    <link href="http://www.shihaizhou.com/2020/06/07/Matplotlib-Cookbook/"/>
    <id>http://www.shihaizhou.com/2020/06/07/Matplotlib-Cookbook/</id>
    <published>2020-06-07T08:09:35.000Z</published>
    <updated>2020-06-07T15:17:27.947Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3D-Drawing"><a href="#3D-Drawing" class="headerlink" title="3D Drawing"></a>3D Drawing</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.gcf()  </span><br><span class="line">ax = fig.add_subplot(<span class="number">241</span>, projection=<span class="string">'3d'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Drawing code here.</span></span><br></pre></td></tr></table></figure><h2 id="Dynamic-Drawing"><a href="#Dynamic-Drawing" class="headerlink" title="Dynamic Drawing"></a>Dynamic Drawing</h2><p>In Jupyter Notebook: use <code>display</code> in <code>Ipython</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    <span class="comment"># clear the current figure first</span></span><br><span class="line">    plt.clf() </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Drawing code here.</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># wait=True avoid picture on&amp;off.</span></span><br><span class="line">    display.display(plt.gcf())</span><br><span class="line">    display.clear_output(wait=<span class="literal">True</span>)</span><br><span class="line">    time.sleep(<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;3D-Drawing&quot;&gt;&lt;a href=&quot;#3D-Drawing&quot; class=&quot;headerlink&quot; title=&quot;3D Drawing&quot;&gt;&lt;/a&gt;3D Drawing&lt;/h2&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
      <category term="Code" scheme="http://www.shihaizhou.com/tags/Code/"/>
    
  </entry>
  
  <entry>
    <title>Discriminative Clustering by Regularized Information Maximization</title>
    <link href="http://www.shihaizhou.com/2020/06/04/Discriminative-Clustering-by-Regularized-Information-Maximization/"/>
    <id>http://www.shihaizhou.com/2020/06/04/Discriminative-Clustering-by-Regularized-Information-Maximization/</id>
    <published>2020-06-04T11:02:37.000Z</published>
    <updated>2020-06-07T15:13:29.158Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://papers.nips.cc/paper/4154-discriminative-clustering-by-regularized-information-maximization.pdf" target="_blank" rel="noopener">ICLR-2010 paper “Discriminative Clustering by Regularized Information Maximization”.</a></li><li><a href="https://arxiv.org/abs/1907.13625" target="_blank" rel="noopener">ICLR-2020 paper “On Mutual Information Maximization for Representation Learning”</a>.</li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>本文是采用MI来做Unsupervised Discriminative Clustering的工作，在”On Mutual Information Maximization for Representation Learning”中被提到，其中的表述如下：</p><blockquote><p>It is folklore knowledge that maximizing MI does not necessarily lead to useful representations. Already Linsker (1988) talks in his seminal work about constraints, while a manifestation of the problem in clustering approaches using MI criteria has been brought up by Bridle et al. (1992) and subsequently addressed using regularization by Krause et al. (2010).</p></blockquote><p>我们想要探究的是为什么maximizing MI不能够带来下游任务表现的提升？虽然前人的工作已经经验性地展示了这一点：MI的优化和下游任务表现的提升是disproportional的，通过提供constraints使得MI作为一个有效的目标，那么什么样的constraint才是好的constraint？设计一个新的constraint需要遵守怎样的guideline？这些也是我们想要研究的方向。这篇添加了regularization的工作可能是一个比较好的起始点。</p><p>这篇文章的动机在于当前的clustering方法没有很好地引入probabilistic model, 因此文章：</p><blockquote><p>We propose a principled probabilistic approach to discriminative clustering, by formalizing the problem as unsupervised learning of a conditional probabilistic model.</p><p>We identify two fundamental, competing quantities, class balance and class separation, and develop an information theoretic objective function which trades off these quantities.</p><p>Our approach corresponds to maximizing mutual information between the empirical distribution on the inputs and the induced label distribution, regularized by a complexity penalty. Thus, we call our approach Regularized Information Maximization (RIM).</p></blockquote><h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><h2 id="Discriminative-Clustering"><a href="#Discriminative-Clustering" class="headerlink" title="Discriminative Clustering"></a>Discriminative Clustering</h2><p>给定数据集$X$，目标是学习到一个conditional model $p(y|x, W)$ with parameters $W$ which predicts a distribution over label values $y \in \{1,\cdots,K\}$ given an input vector $x$.</p><h2 id="Information-Maximization"><a href="#Information-Maximization" class="headerlink" title="Information Maximization"></a>Information Maximization</h2><p>将Information Maximization方法应用到unsupervised clustering中不需要显示得定义cluster number. 同时因为最终的representation就是离散的类别标签，所以我们可以直接估计input-label两者的MI. 以下是作者论述构建出MI作为优化目标的clustering method.</p><p>首先，discriminative clustering方法的第一个原则是不希望在数据分布密集的地方划分decision boundary. 前人的工作说明了conditional entropy term $-\frac{1}{N} \sum_{i} H\left\{p\left(y | \mathbf{x}_{i}, \mathbf{W}\right)\right\}$ very effectively captures the cluster assumption when training probabilistic classifiers <strong>with partial labels</strong>. </p><p>但是仅仅优化conditional entropy也是不够的。我们知道降低conditional entropy的目标是希望我们probabilistic model对于自己分类结果更加“确定”. 而因为我们的聚类方法没有对$K$的数量进行限制，所以”simply removing decision boundaries”，将所有的样本都归为同一类就能够骗过我们的优化目标，而得到非常糟糕的discriminative model. </p><p>为了避免以上提到的degenerate solution, 文章提出要做到类间平衡 (class balance). 计算每个类别的marginal label distribution，我们得到：</p><script type="math/tex; mode=display">\hat{p}(y ; \mathbf{W})=\int \hat{p}(\mathbf{x}) p(y | \mathbf{x}, \mathbf{W}) d \mathbf{x}=\frac{1}{N} \sum_{i} p\left(y | \mathbf{x}_{i}, \mathbf{W}\right)</script><p>所以类间平衡的要点在于使每个类别尽量能够均匀地分布，即最大化类分布的熵。结合两者，我们得到了最大化MI的优化目标：</p><script type="math/tex; mode=display">I_{\mathbf{W}}\{y ; \mathbf{x}\}=H\{\hat{p}(y ; \mathbf{W})\}-\frac{1}{N} \sum_{i} H\left\{p\left(y | \mathbf{x}_{i}, \mathbf{W}\right)\right\}</script><p>在clustering任务中，前人也已经发现了这个优化目标有trivial solution：”However, they note that $I_{\mathbf{W}}\{y ; \mathbf{x}\}$ may be trivially maximized by a conditional model that classifies each data point $x_i$ into its own category $y_i$, and that classifiers trained with this objective tend to fragment the data into a large number of categories”. </p><h2 id="Regularized-Information-Maximization"><a href="#Regularized-Information-Maximization" class="headerlink" title="Regularized Information Maximization"></a>Regularized Information Maximization</h2><p>在此基础上，文章提出了使用一个正则项 $R(W;\lambda)$ 对MI进行约束：This term penalizes conditional models with complex decision boundaries in order to yield sensible clustering solutions. Our objective function is</p><script type="math/tex; mode=display">F(\mathbf{W} ; \mathbf{X}, \lambda)=I_{\mathbf{W}}\{y ; \mathbf{x}\}-R(\mathbf{W} ; \lambda)</script><p>在Multilogit-regression任务中文章给出了参数的模作为正则项。本质上正则项就是为了防止过多的decision boundary被发现从而约束RIM的类别数量，一般来说，$\lambda$ 越大类数越小。</p><h1 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h1><p><img src="https://i.loli.net/2020/06/07/jEHZYqNQc8oF4Sn.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://papers.nips.cc/paper/4154-discriminative-clustering-by-regularized-information-m
      
    
    </summary>
    
    
      <category term="MI" scheme="http://www.shihaizhou.com/tags/MI/"/>
    
  </entry>
  
  <entry>
    <title>Deep Graph InfoMax</title>
    <link href="http://www.shihaizhou.com/2020/06/02/Deep-Graph-InfoMax/"/>
    <id>http://www.shihaizhou.com/2020/06/02/Deep-Graph-InfoMax/</id>
    <published>2020-06-02T09:07:44.000Z</published>
    <updated>2020-06-02T09:08:44.029Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>Deep Graph Infomax (DGI) is a general approach for learning node representations within graph-structured data in an unsupervised manner. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. </p><h2 id="Structure-of-Introduction"><a href="#Structure-of-Introduction" class="headerlink" title="Structure of Introduction"></a>Structure of Introduction</h2><ul><li>Generalizing node-level representation learning is important, large-scale graph data is often with no labels. -&gt; thus unsupervised representation learning method is much more important. </li><li>Existing methods most rely on random walk-based objective, which<ul><li>over-emphasize proximity information at the expense of structural information</li><li>is highly influenced by hyperparameter choice</li><li>when introduced with stronger encoder, hard to tell whether the representation has meaningful signals. </li></ul></li><li>Deep InfoMax on image data, maximize the global/local mutual information. This encourages the encoder to carry the type of information that is present in all locations (and thus are globally relevant), such as would be the case of a class label.</li><li>We are the first work applying it to the graph data. </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Motivation&quot;&gt;&lt;a href=&quot;#Motivation&quot; class=&quot;headerlink&quot; title=&quot;Motivation&quot;&gt;&lt;/a&gt;Motivation&lt;/h1&gt;&lt;p&gt;Deep Graph Infomax (DGI) is a general 
      
    
    </summary>
    
    
      <category term="MI" scheme="http://www.shihaizhou.com/tags/MI/"/>
    
  </entry>
  
  <entry>
    <title>Data Simulation Cookbook</title>
    <link href="http://www.shihaizhou.com/2020/05/31/Data-Simulation/"/>
    <id>http://www.shihaizhou.com/2020/05/31/Data-Simulation/</id>
    <published>2020-05-31T11:47:14.000Z</published>
    <updated>2020-06-07T08:32:29.399Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Multi-Variate-Gaussion"><a href="#Multi-Variate-Gaussion" class="headerlink" title="Multi-Variate Gaussion"></a>Multi-Variate Gaussion</h2><p>Need to provide the mean and covariance matrix. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mean = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">cov = [[<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>,<span class="number">1</span>]]</span><br><span class="line">data_size = <span class="number">100</span></span><br><span class="line">X = np.random.multivariate_normal(mean, cov, data_size)</span><br></pre></td></tr></table></figure><h2 id="2D-ring"><a href="#2D-ring" class="headerlink" title="2D-ring"></a>2D-ring</h2><p>Produce a ring for given $(x, y, r)$ triplet. The first one use the famous formula adopted in VAE. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ring</span><span class="params">(x, y, r, data_size)</span>:</span>,</span><br><span class="line">    X = np.random.multivariate_normal([<span class="number">0</span>, <span class="number">0</span>], [[<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>,<span class="number">1</span>]], data_size)</span><br><span class="line">    Z = X / <span class="number">10</span> + X / np.sqrt(np.square(X).sum(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure><h2 id="3D-globule"><a href="#3D-globule" class="headerlink" title="3D-globule"></a>3D-globule</h2><p>Produce <code>len(centers)</code> gaussian globules. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">globule_3d</span><span class="params">(centers, datasize)</span>:</span></span><br><span class="line">    res = []</span><br><span class="line">    <span class="keyword">for</span> center <span class="keyword">in</span> centers:</span><br><span class="line">        offsets = []</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">            offsets.append(np.random.normal(<span class="number">0.</span>,<span class="number">1.</span>,datasize).reshape([datasize,<span class="number">1</span>]))</span><br><span class="line">        res.append(center + np.hstack(offsets))    </span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Multi-Variate-Gaussion&quot;&gt;&lt;a href=&quot;#Multi-Variate-Gaussion&quot; class=&quot;headerlink&quot; title=&quot;Multi-Variate Gaussion&quot;&gt;&lt;/a&gt;Multi-Variate Gaussi
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
      <category term="Code" scheme="http://www.shihaizhou.com/tags/Code/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch - CNNs</title>
    <link href="http://www.shihaizhou.com/2020/05/28/Pytorch-CNNs/"/>
    <id>http://www.shihaizhou.com/2020/05/28/Pytorch-CNNs/</id>
    <published>2020-05-28T13:39:09.000Z</published>
    <updated>2020-06-10T15:51:56.349Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html" target="_blank" rel="noopener">pytorch official guide “training CNN classifiers on CIFAR-10”</a></li></ul><h1 id="Functions"><a href="#Functions" class="headerlink" title="Functions"></a>Functions</h1><h3 id="torch-nn-Conv2d"><a href="#torch-nn-Conv2d" class="headerlink" title="torch.nn.Conv2d"></a>torch.nn.Conv2d</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Conv2d(</span><br><span class="line">    in_channels, </span><br><span class="line">    out_channels, </span><br><span class="line">    kernel_size, </span><br><span class="line">    stride=<span class="number">1</span>, </span><br><span class="line">    padding=<span class="number">0</span>, </span><br><span class="line">    dilation=<span class="number">1</span>, </span><br><span class="line">    groups=<span class="number">1</span>, </span><br><span class="line">    bias=<span class="literal">True</span>, </span><br><span class="line">    padding_mode=<span class="string">'zeros'</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>Pytorch和tensorflow中（也和我们一般理解的）的图像表示形式不同：torch中的channel维在height和width之前，所以input的tensor形状为$(N,C_{\text{in}}, H, W)$. </p><h3 id="torch-nn-ZeroPad2d"><a href="#torch-nn-ZeroPad2d" class="headerlink" title="torch.nn.ZeroPad2d"></a>torch.nn.ZeroPad2d</h3><p>在二维Tensor的左右上下四个方向添加zero-padding. </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pad = nn.ZeroPad2d(padding=(1, 2, 3, 4))</span><br><span class="line">y = pad(x)</span><br></pre></td></tr></table></figure><h3 id="torchvision-transforms-Pad"><a href="#torchvision-transforms-Pad" class="headerlink" title="torchvision.transforms.Pad"></a>torchvision.transforms.Pad</h3><p>有的时候我们希望对于数据的处理都封装在load_data这一层中，所以我们希望能够使用torchvision里的<code>transform.Compose</code>. 以下展示了将MNIST上<code>(1, 28, 28)</code>的数据转换为能够匹配用于CIFAR-10的DCGAN架构的代码。注意Pad需要在<code>ToTensor()</code>层前，因为该方法适配于PIL Image对象而不是Tensor对象。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([transforms.Pad(padding=(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>), fill=<span class="number">0</span>),</span><br><span class="line">                                transforms.ToTensor(),</span><br><span class="line">                                transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))])</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html&quot; target=&quot;_blank&quot; rel=
      
    
    </summary>
    
    
      <category term="Code" scheme="http://www.shihaizhou.com/tags/Code/"/>
    
  </entry>
  
  <entry>
    <title>Contrastive Predictive Coding</title>
    <link href="http://www.shihaizhou.com/2020/05/28/Contrastive-Predictive-Coding/"/>
    <id>http://www.shihaizhou.com/2020/05/28/Contrastive-Predictive-Coding/</id>
    <published>2020-05-28T02:17:22.000Z</published>
    <updated>2020-06-08T15:44:27.967Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li>paper “Contrastive Predictive Coding”</li></ul><p>The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples.</p><p>Predictive coding在信号处理领域是常见的unsupervised learning方法. 本文的方法流程为：</p><ul><li>First, we compress high-dimensional data into a much more compact latent embedding space in which conditional predictions are easier to model. </li><li>Secondly, we use powerful autoregressive models in this latent space to make predictions many steps in the future. </li><li>Finally, we rely on Noise-Contrastive Estimation for the loss function. </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;paper “Contrastive Predictive Coding”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The key insight of our model is to learn such
      
    
    </summary>
    
    
      <category term="Contrastive" scheme="http://www.shihaizhou.com/tags/Contrastive/"/>
    
      <category term="MI" scheme="http://www.shihaizhou.com/tags/MI/"/>
    
  </entry>
  
  <entry>
    <title>Information Competing Process for Learning Diversified Representations</title>
    <link href="http://www.shihaizhou.com/2020/05/27/Information-Competing-Process-for-Learning-Diversified-Representations/"/>
    <id>http://www.shihaizhou.com/2020/05/27/Information-Competing-Process-for-Learning-Diversified-Representations/</id>
    <published>2020-05-27T06:57:20.000Z</published>
    <updated>2020-05-28T02:17:06.212Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/abs/1906.01288" target="_blank" rel="noopener">NIPs-2019 paper “Information Competing Process for Learning Diversified Representations”</a></li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>Aiming to enrich the information carried by feature representations, ICP separates a representation into two parts with different mutual information constraints. The separated parts are forced to accomplish the downstream task independently in a competitive environment which prevents the two parts from learning what each other learned for the downstream task. Such competing parts are then combined synergistically to complete the task.</p><h2 id="Representation-Collaboration"><a href="#Representation-Collaboration" class="headerlink" title="Representation Collaboration"></a>Representation Collaboration</h2><p>The Competitive Collaboration method is the most relevant to our work. It defines a three-player game with two competitors and a moderator, where the moderator takes the role of a critic and the two competitors collaborate to train the moderator. Unlike Competitive Collaboration, the proposed ICP enforces two (or more) representation parts to be complementary through different mutual information constraints for the same downstream task by a competitive environment, which endows the capability of learning more discriminative and disentangled representations.</p><h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><h2 id="Objectives"><a href="#Objectives" class="headerlink" title="Objectives"></a>Objectives</h2><p>首先将supervised和self-supervised两个setting中的objective进行一下统一. In supervised setting, $t$ represents the label of input $xt$. In self-supervised setting, $xt$ represents the input $tx$ itself. This leads to the unified objective function linking the representation $rx$ of input $x$ and target $t$ as:</p><script type="math/tex; mode=display">\max \mathcal I(r,t)</script><h2 id="Seperating-Representations"><a href="#Seperating-Representations" class="headerlink" title="Seperating Representations"></a>Seperating Representations</h2><p>Directly separate the representation $r$ into two parts $[z, y]$. Specifically, we constrain the information capacity of representation part $z$ while increasing the information capacity of representation part $y$.</p><script type="math/tex; mode=display">\max [\mathcal{I}(r, t)+\alpha \mathcal{I}(y, x)-\beta \mathcal{I}(z, x)]</script><h2 id="Representation-Competition"><a href="#Representation-Competition" class="headerlink" title="Representation Competition"></a>Representation Competition</h2><p>ICP prevents $z$ and $y$ from knowing what each other learned for the downstream task, which is realized by enforcing $z$ and $y$ independent of each other.</p><script type="math/tex; mode=display">\max [\mathcal{I}(r, t)+\alpha \mathcal{I}(y, x)-\beta \mathcal{I}(z, x)+\mathcal{I}(z, t)+\mathcal{I}(y, t)-\gamma \mathcal{I}(z, y)]</script><p>在形式上和我们想要做的InfoBal非常接近，我们将以上式子带入unsupervised learning的情境下，也就是target $t$ = input $x$:</p><script type="math/tex; mode=display">\begin{align}&\max [\mathcal{I}(r, t)+\alpha \mathcal{I}(y, x)-\beta \mathcal{I}(z, x)+\mathcal{I}(z, t)+\mathcal{I}(y, t)-\gamma \mathcal{I}(z, y)] \\\Leftrightarrow& \max [\mathcal{I}(r, x)+\alpha \mathcal{I}(y, x)-\beta \mathcal{I}(z, x)+\mathcal{I}(z, x)+\mathcal{I}(y,x)-\gamma \mathcal{I}(z, y)] \\\Leftrightarrow& \max [\mathcal{I}(r, x)+(\alpha+1) \mathcal{I}(y, x)+(1-\beta) \mathcal{I}(z, x)-\gamma \mathcal{I}(z, y)] \\\Leftrightarrow& \max [\mathcal{I}(r, x)+\alpha^\prime \mathcal{I}(y, x)+\beta^\prime \mathcal{I}(z, x)-\gamma \mathcal{I}(z, y)] \\\end{align}</script><p>where $\alpha^\prime &gt; 1$ and $\beta^\prime &lt; 1$. 如果把$z$和$y$分别看作数据的两个view，那么中间两项就代表不同view和$X$ 之间的相关度；第一项代表全局representation和input之间的相关度；而最后一项就是view之间的InfoMin项。</p><p><img src="https://i.loli.net/2020/05/27/W6RfgsN5GdKo9Ab.png" alt="image.png"></p><h2 id="Minimizing-MI"><a href="#Minimizing-MI" class="headerlink" title="Minimizing MI"></a>Minimizing MI</h2><p>本文采用的最小化MI的方法来源于variational inference. 同Maximizing MI的方法类似的，因为是为了最小化，所以我们希望能够找到$I(x,z)$的upper bound，然后直接去最小化这个upper bound. 首先我们有：</p><script type="math/tex; mode=display">\begin{align}\mathcal{I}(z, x) &=\iint P(z, x) \log \frac{P(z, x)}{P(z) P(x)} d x d z \\&= \iint P(z, x) \log P(z | x) d x d z-\int P(z) \log P(z) d z\end{align}</script><p>Let $Q(z)$ be a variational approximation of $P(z)$, we have:</p><script type="math/tex; mode=display">K L[P(z) \| Q(z)] \geq 0 \Rightarrow \int P(z) \log P(z) d z \geq \int P(z) \log Q(z) d z</script><p>将该不等式带入MI的定义式，我们得到了一个tractable upper bound (why???). </p><script type="math/tex; mode=display">\mathcal{I}(z, x) \leq \iint P(z | x) P(x) \log \frac{P(z | x)}{Q(x)} d x d z=\mathbb{E}_{x \sim P(x)}[K L[P(z | x) \| Q(z)]]</script><p>which enforces the extracted $z$ conditioned on $x$ to a predefined distribution $Q(z)$ such as a standard Gaussian distribution.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.01288&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;NIPs-2019 paper “Inform
      
    
    </summary>
    
    
      <category term="MI" scheme="http://www.shihaizhou.com/tags/MI/"/>
    
  </entry>
  
  <entry>
    <title>InfoBal - Ideas (failed)</title>
    <link href="http://www.shihaizhou.com/2020/05/26/InfoBal-Ideas/"/>
    <id>http://www.shihaizhou.com/2020/05/26/InfoBal-Ideas/</id>
    <published>2020-05-26T14:25:21.000Z</published>
    <updated>2020-06-10T10:43:43.561Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/abs/1907.13625" target="_blank" rel="noopener">ICLR-2020 paper “On Mutual Information Maximization for Representation Learning”</a></li><li><a href="https://arxiv.org/abs/1906.05849" target="_blank" rel="noopener">paper “Contrastive Multiview Coding”</a></li><li><a href="https://arxiv.org/abs/2005.10243" target="_blank" rel="noopener">paper “What Makes for Good Views for Contrastive Learning”</a></li><li><a href="https://arxiv.org/abs/1906.01288" target="_blank" rel="noopener">NIPs-2019 paper “Information Competing Process for Learning Diversified Representations”</a></li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p><a href="https://arxiv.org/abs/1906.05849" target="_blank" rel="noopener">CMC</a>的动机在于：重要的信息在多个view中被共享，所以我们应当最大化多个view的mutual information. 在<a href="https://arxiv.org/abs/2005.10243" target="_blank" rel="noopener">CMC作者的延续性工作</a>中，他认为只考虑view-invariant的信息是不够的，最佳的信息应当是和下游任务高度相关的，因此提出了InfoMin principle, 其核心思想是在input的两个view连同整体input和下游任务标签的MI保持一致的前提下，尽量使两个view之间的MI降低。而<a href="https://arxiv.org/abs/1907.13625" target="_blank" rel="noopener">ICLR-2020的文章</a>则将以往提出的三个MI优化目标（分别为global/local MI, CMC, CPC）都归入了统一的multiview理论框架下，认为这三个优化目标是这个框架下平行的优化目标。</p><p>我们探究global/local MI和CMC之间的关系，希望探究出它们的关系以及尝试构造一个完全基于MI的优化目标，希望这个优化目标能够让我们的representation学会在复杂特征信息中学会平衡，并且使模型能够学到不同方面和性质的information. </p><h1 id="Theory"><a href="#Theory" class="headerlink" title="Theory"></a>Theory</h1><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p>Suppose the input dataset is $X$. Consider the simplest setting of only 2 views $V_1$ and $V_2$ of $X$, which is parameterized by $\theta_1$ and $\theta_2$:</p><script type="math/tex; mode=display">\begin{align}V_1 &= g_1(X;\theta_1) \\V_2 &= g_2(X;\theta_2)\end{align}</script><p>In original InfoMax principle, the objective of learning representation of single view is to maximize:</p><script type="math/tex; mode=display">I(V; X)</script><p>When there are multiple views involved, we want them to be simultaneously maximized:</p><script type="math/tex; mode=display">\sum_i^2 I(V_i;X)</script><p>In InfoMin principle, the objective is to minimize the MI between different views <strong>under certain (complicated) constraints</strong>. </p><script type="math/tex; mode=display">I(V_1; V_2)</script><p>The above two objectives without constraints may not be so favorable analytically, since they both have trivial solutions which result in useless representation. </p><ul><li>For the original InfoMax principle: let function $g$ be bijective function, all the information of $X$ is preserved and the MI is maximized, no good form of representation is learned. </li><li>For the multiview InfoMin principle: let $V_1$ and $V_2$ output constant vectors, and their MI is 0, which is minimized.  </li></ul><h2 id="InfoBal-Objective"><a href="#InfoBal-Objective" class="headerlink" title="InfoBal Objective"></a>InfoBal Objective</h2><p>To solve this problem, notice that there exists an inequality: </p><script type="math/tex; mode=display">I(V_1;V_2) \leq \min(I(X;V_1), I(X;V_2))</script><p>which yields:</p><script type="math/tex; mode=display">\frac{1}{2}\sum_i^2 I(V_i;X) - I(V_1;V_2) \geq 0</script><p>So we can change our objective of representation learning with $M$ views to maximizing the following:  </p><script type="math/tex; mode=display">J(\Theta) = \frac{1}{M}\sum_i^MI(V_i;X) - \frac{2}{M(M-1)}\sum_{1\leq i < j\leq M}I(V_i,V_j) \geq 0</script><p>where $\Theta=\{\theta_1, \theta_2, \cdots, \theta_M\}$ represents the set of the view encoders, and is guaranteed bounded. </p><p>The reason why this formula is better than previous ones is that it helps the model to balance the information capacity and multiview diversity of the representation. And also in the two extreme cases where two single objectives can’t deal with morbid representation, the new objective function equally hates them: </p><p><strong>In the first morbid scenario</strong>, when every $V_i$ preserves all the info about $X$, i.e., </p><script type="math/tex; mode=display">I(V_i; X)=I(X;X)=H(X), \forall i\in\{1,\cdots,M\}</script><p>which is quite favorable, since the views (representations) are super powerful. While without constraints, we can assume the happening of the worst: each view is exactly identical, which makes the multiview setting not learning diverse representations. This case is discouraged by our new objective:</p><script type="math/tex; mode=display">\begin{align}J(\Theta) &= \frac{1}{M}\sum_i^MI(V_i;X) - \frac{2}{M(M-1)}\sum_{1\leq i < j\leq M}I(V_i,V_j) \\&= \frac{1}{M} \sum H(X) - \frac{2}{M(M-1)}\sum H(X) \\&= 0\end{align}</script><p><strong>In the second morbid scenario</strong>, when MI across the views are minimized, which means $V_i$ is irrelevant to each other, i.e., $I(V_i, V_j)=0$, this InfoMin situation could lead to extreme loss of information:</p><script type="math/tex; mode=display">I(V_i;X) = 0, \forall i\in\{1,\cdots,M\}</script><p>while in our objective, this is also discouraged:</p><script type="math/tex; mode=display">\begin{align}J(\Theta) &= \frac{1}{M}\sum_i^MI(V_i;X) - \frac{2}{M(M-1)}\sum_{1\leq i < j\leq M}I(V_i,V_j) \\&= \frac{1}{M} \sum 0 - \frac{2}{M(M-1)}\sum 0 \\&= 0\end{align}</script><h2 id="Global-Local-InfoBal"><a href="#Global-Local-InfoBal" class="headerlink" title="Global/Local InfoBal"></a>Global/Local InfoBal</h2><p>The final representation $R$ is the result of view aggregation:</p><script type="math/tex; mode=display">R = f(\{V_i\}_{i=1}^M;\phi)</script><p>In global/local MI maximization, the target is to maximize the Mutual Information between the global and the local representation of the data:</p><script type="math/tex; mode=display">J(\Theta, \phi) = \frac{1}{M}\sum_i^MI(V_i;R)</script><p>which lacks theoretical derivation why this is a more favorable objective. We could also do this adaptation to our new objective by replacing original input $X$ with final representation $R$:</p><script type="math/tex; mode=display">J(\Theta,\phi) = \frac{1}{M}\sum_i^MI(V_i;R) - \frac{2}{M(M-1)}\sum_{1\leq i < j\leq M}I(V_i,V_j)</script><p><img src="https://i.loli.net/2020/06/02/uFQgKn3bLodySl5.png" alt="image.png"></p><h2 id="Weighted-InfoBal"><a href="#Weighted-InfoBal" class="headerlink" title="Weighted InfoBal"></a>Weighted InfoBal</h2><p>Another potential optimiztion for InfoBal is to assign two weight matrix: view weight and correlation weight (这个名字不确定). </p><script type="math/tex; mode=display">J(\Theta,\phi) = \frac{1}{M}\sum_i^M w_i I(V_i;R) - \frac{2}{M(M-1)}\sum_{1\leq i < j\leq M} W_{i,j} I(V_i,V_j)</script><h2 id="Optimizing-InfoBal"><a href="#Optimizing-InfoBal" class="headerlink" title="Optimizing InfoBal"></a>Optimizing InfoBal</h2><p>One way is to <strong>adversarially optimize</strong> the InfoBal training objective, since the InfoMin is a minmax problem.</p><p><img src="https://i.loli.net/2020/05/26/iuf73oWIjRVbq2n.png" alt="image.png"></p><p>Another way is to <strong>found the upper bound</strong> of the objective and minimize the upper bound, which is introduced in another similar paper (refer to <a href="https://arxiv.org/abs/1906.01288" target="_blank" rel="noopener">NIPs-2019 paper “Information Competing Process for Learning Diversified Representations”</a>). This one involves the knowledge of variational inference, which will be investigated in the future. </p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>将InfoBal模型的几个部分进行整理和划分，从Representation生成模型的角度一共能够分为以下四个部分：</p><ul><li>View Encoder: 从raw input进行多个view的编码模块</li><li>View Aggregator: 从view representation聚合成最终representation的模块</li><li>InfoMax critic: 用于MI maximization的parameterized DV representation模块</li><li>InfoMin discriminator: 最小化view之间的MI的discriminator模块</li></ul><p>而从InfoBal的优化目标来看，不同的term对于模型参数的影响也是不同的，下图展示了InfoBal的流程，以及不同loss term对InfoBal encoder架构的影响。</p><p><img src="https://i.loli.net/2020/06/02/uFQgKn3bLodySl5.png" alt="image.png"></p><p>We conducted the experiments on CIFAR10 dataset using the 2-view setting: instead of simultaneously maximizing the MI between the representation and the feature map vectors, we split the feature map into top/bottom parts and maximize the MI. </p><p>To evaluate the quality of the final representation, we plug the representation into a one-layer neural classifier as DIM does. We train the classifier for 4 epochs, which is by test converged, and evaluate the accuracy of the classifier. </p><p><img src="https://i.loli.net/2020/06/04/dclaybIKpjvnZxh.png" alt="image.png"></p><p>Change into JSD critic function, and we have the result: </p><p><img src="https://i.loli.net/2020/06/04/wLOtzi7qvHxA2N6.png" alt="image.png"></p><p>When trained in much longer period, we can even find out that the InfoBal objective is constantly doing harm to the representation quality. </p><p><img src="https://i.loli.net/2020/06/06/hudjfHWQ4As3Lia.png" alt="image.png"></p><p>The figure above is the experimental result, about which we have some questions and concerns: </p><ul><li>The lower bound of MI is increasing along training, while the downstream task accuracy is not improved (as much) as the MI estimate. </li><li>The improvement compared to the original starting points is rather marginal. </li><li>InfoBal objective is constantly hurting the performance of the representation learning model, despite the fact that the lower bound MI estimate value is nearly the same, which also implies that the MI is not so relevant to the representation quality. </li></ul><p>Up to now, we can conclude that the new objectice InfoBal is a failed attempt. One thing worth trying though, is to reimplement the DIM’s global/local objective in multiple (way more than 2) views and shed some light on our future exploration.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1907.13625&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ICLR-2020 paper “On Mut
      
    
    </summary>
    
    
      <category term="MI" scheme="http://www.shihaizhou.com/tags/MI/"/>
    
  </entry>
  
  <entry>
    <title>What Makes for Good Views for Contrastive Learning</title>
    <link href="http://www.shihaizhou.com/2020/05/26/What-Makes-for-Good-Views-for-Contrastive-Learning/"/>
    <id>http://www.shihaizhou.com/2020/05/26/What-Makes-for-Good-Views-for-Contrastive-Learning/</id>
    <published>2020-05-26T02:17:14.000Z</published>
    <updated>2020-05-28T07:16:30.680Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/abs/2005.10243" target="_blank" rel="noopener">paper “What Makes for Good Views for Contrastive Learning”</a></li><li><a href="https://arxiv.org/pdf/1406.2661.pdf" target="_blank" rel="noopener">paper “GAN”</a></li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>Despite the success of the Contrastive Multiview Coding (CMC), the influence of different view choices has been less studied. In this paper, we use empirical analysis to better understand the importance of view selection, and argue that we should reduce the mutual information (MI) between views while keeping task-relevant information intact. To verify this hypothesis, we devise unsupervised and semi-supervised frameworks that learn effective views by aiming to reduce their MI. </p><h2 id="Structure-of-Introduction"><a href="#Structure-of-Introduction" class="headerlink" title="Structure of Introduction"></a>Structure of Introduction</h2><ul><li>CMC relies on the fundamental assumption that important information is share across views, which means it’s view-invariant. </li><li>Then which viewing conditions should it be invariant to?</li><li>We therefore seek representations with enough invariance to be robust to inconsequential variations but not so much as to discard information required by downstream tasks.</li><li>We investigate this question in two ways.<ul><li>Optimal choice of views depends critically on the downstream task.</li><li>For many common ways of generating views, there is a sweet spot in terms of downstream performance where the mutual information (MI) between views is neither too high nor too low.</li></ul></li><li>InfoMin principle: A good set of views are those that share the minimal information necessary to perform well at the downstream task.</li></ul><h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><p><strong>Definition 4.1.</strong> (Sufficient Encoder) The encoder $f_1$ of $v_1$ is sufficient in the contrastive learning framework if and only if $I(v_1; v_2) = I(f_1(v_1); v_2)$.</p><p><strong>Definition 4.2.</strong> (Minimal Sufficient Encoder) A sufficient encoder $f_1$ of $v_1$ is minimal if and only if $I(f_1(v_1);v_1) \leq I(f(v_1);v_1) \forall f$, that are sufficient. Among those encoders which are sufficient, the minimal ones only extract relevant information of the contrastive task and will throw away other information.</p><p><strong>Definition 4.3.</strong> (Optimal Representation of a Task) For a task $\mathcal T$ whose goal is to predict a semantic label $y$ from the input data $x$, the optimal representation $z^\star$ encoded from $x$ is the minimal sufficient statistic with respect to $y$. 以上说明了$z^\star$保留了所有用于和task $\mathcal T$相关的信息，因此被称作optimal的。</p><h2 id="InfoMin-Principle"><a href="#InfoMin-Principle" class="headerlink" title="InfoMin Principle"></a>InfoMin Principle</h2><p>作者认为good view一定是和下游任务相关联的，The following InfoMin proposition articulates which views are optimal supposing that we know the specific downstream task $\mathcal T$ in advance.</p><p><img src="https://i.loli.net/2020/05/26/gDBvHGEa9cWwMtF.png" alt="image.png"></p><p>在附录中从Proposition 4.1能够推论出，遵循constraints</p><script type="math/tex; mode=display">I\left(\mathbf{v}_{1} ; \mathbf{y}\right)=I\left(\mathbf{v}_{2} ; \mathbf{y}\right)= I(\mathbf{x} ; \mathbf{y})</script><p>得到的view最优解满足以下：</p><script type="math/tex; mode=display">I\left(\mathbf{v}_{1}^{*} ; \mathbf{v}_{2}^{*}\right) = I(\mathbf{x} ; \mathbf{y})</script><p>在这个阶段为了说明不同view之间的information是如何影响downstream task的，文章构造了Colorful-Moving-MNIST数据集，简单来说就是带有背景的移动MNIST手写数字视频数据。</p><p><img src="https://i.loli.net/2020/05/26/z3H4UbjQFkqxdsa.png" alt="image.png"></p><p>假设一个数据点是$x_{1:t}$，那么将第一个view固定为$v_1=x_{1:k}$. 在这样的设置下，因为MNIST的手写数字移动方式是固定的，所以我们可以从前2帧推知后面帧的行动，因此$I(v_1, x)$是最大化的。同时构造出三个对应不同下游任务的$v_2$，通过最大化两个view之间的MI来学到representation，最后将这个representation带入下游任务中检查其表现，得到了以下表格，证明了不同view之间分享的information将会影响representation在下游任务上的表现。</p><p><img src="https://i.loli.net/2020/05/26/ifzZwYcdBXgrMex.png" alt="image.png"></p><h2 id="Unsupervised-InfoMin"><a href="#Unsupervised-InfoMin" class="headerlink" title="Unsupervised InfoMin"></a>Unsupervised InfoMin</h2><p>该工作实现了最小化MI的方法：那就是采用adversarial startegy解决以下的minmax problem:</p><script type="math/tex; mode=display">\min _{g} \max _{f_{1}, f_{2}} I_{N C E}^{f_{1}, f_{2}}\left(g(X)_{1} ; g(X)_{2: 3}\right)</script><p>这里我们复习一下GAN中adversarial training method. </p><p><img src="https://i.loli.net/2020/05/26/iuf73oWIjRVbq2n.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2005.10243&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;paper “What Makes for G
      
    
    </summary>
    
    
      <category term="MI" scheme="http://www.shihaizhou.com/tags/MI/"/>
    
  </entry>
  
  <entry>
    <title>Contrastive Multiview Coding</title>
    <link href="http://www.shihaizhou.com/2020/05/25/Contrastive-Multiview-Coding/"/>
    <id>http://www.shihaizhou.com/2020/05/25/Contrastive-Multiview-Coding/</id>
    <published>2020-05-25T08:16:33.000Z</published>
    <updated>2020-06-08T15:44:19.595Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/abs/1906.05849" target="_blank" rel="noopener">paper “Contrastive Multiview Coding”</a></li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>对于人类，同一事物在不同sensory channel的信息是高度相关的. We study this hypothesis under the framework of multiview contrastive learning, where we learn a representation that aims to maximize mutual information between different views of the same scene but is otherwise compact.</p><h2 id="Structure-of-Introduction"><a href="#Structure-of-Introduction" class="headerlink" title="Structure of Introduction"></a>Structure of Introduction</h2><ul><li>Autoencoders treat bits equally. </li><li>We revisit the classic hypothesis that the good bits are the ones that are shared between multiple views of the world. This hypothesis corresponds to the inductive bias that the way you view a scene should not affect its semantics.</li><li>Our goal is therefore to learn representations that capture information shared between multiple sensory channels but that are otherwise compact (i.e. discard channel-specific nuisance factors). </li><li>Our main contribution is to set up a framework to extend these ideas to any number of views. </li></ul><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>文章从Multiview的角度将两大类方法进行了对比，分别为Predictive Learning和Contrastive Learning. </p><h2 id="Predictive-Learning"><a href="#Predictive-Learning" class="headerlink" title="Predictive Learning"></a>Predictive Learning</h2><p>Autoencoder方法被归为Predictive Learning的范畴中。这类方法最大的问题在于其优化目标objective假设了每个pixel之间是相互独立的，thereby reducing their ability to model correlations or complex structure. 为什么叫predictive learning呢，是因为在Multiview的setting下，我们希望建立一个从view1-&gt;representation-&gt;view2的映射，最小化这个预测之间的loss. 这个形式和autoencoder有些类似，关于multiview predictive learning和autoencoder之间的关系，我们做以下图示进行说明： </p><p><img src="https://i.loli.net/2020/05/25/OaH7mYLwsuPNrU8.png" alt="image.png"></p><p>其实在这样的角度理解multiview predictive coding，就是不同view之间不能够有信息沟通。<strong>相当于我们在AE的架构上做了更多的locality的限制，同时认为不同view之间在信息量上是等价的（能够从一个view推测出另一个view）</strong>，而该篇工作就建立在这个假设的进一步发展上：</p><blockquote><p>The good bits are the ones that are shared between multiple views of the world. </p></blockquote><p>因此使用Mutual Information来最优化不同view之间共享的信息就成了该篇工作的重点和亮点。</p><h2 id="Contrastive-Learning"><a href="#Contrastive-Learning" class="headerlink" title="Contrastive Learning"></a>Contrastive Learning</h2><p>Multiview Contrastive Learning的基本思想是：将同一个样本的不同view为一个正样本对$\left\{v_{1}^{i}, v_{2}^{i}\right\}_{i=1}^{N}$，对于第i个样本我们能够构造不同样本的不同view为一个负样本对$\left\{v_{1}^{i}, v_{2}^{j}\right\}_{j=1}^{K}$。通过训练一个critic function $h_\theta$来区分正负样本对，从而得到representation. 构造contrast loss为以下：</p><script type="math/tex; mode=display">\mathcal{L}_{\text {contrast}}=-\underset{S}{\mathbb{E}}\left[\log \frac{h_{\theta}(x)}{h_{\theta}(x)+\sum_{i=1}^{k} h_{\theta}\left(y_{i}\right)}\right]</script><p>从而计算$V_1$和$V_2$之间的contrast loss为：</p><script type="math/tex; mode=display">\mathcal{L}_{\text {contrast }}^{V_{1}, V_{2}}=-\mathbb E_{\left\{v_{1}^{1}, v_{2}^{1}, \ldots, v_{2}^{k+1}\right\}}\left[\log \frac{h_{\theta}\left(\left\{v_{1}^{1}, v_{2}^{1}\right\}\right)}{\sum_{j=1}^{k+1} h_{\theta}\left(\left\{v_{1}^{1}, v_{2}^{j}\right\}\right)}\right]</script><p>对称化两个view之间的loss：</p><script type="math/tex; mode=display">\mathcal{L}\left(V_{1}, V_{2}\right)=\mathcal{L}_{\text {contrast}}^{V_{1}, V_{2}}+\mathcal{L}_{\text {contrast}}^{V_{2}, V_{1}}</script><p>对于Multiview的setting，作者还给出了两种经常遇到的情况，分别是view中有一个Core View和全连接view的情况。</p><p><img src="https://i.loli.net/2020/05/25/Ou9tqDnbzIGhlN2.png" alt="image.png"></p><p>根据以上分别可以构造两个Loss function: </p><script type="math/tex; mode=display">\begin{align}\mathcal{L}_{C}&=\sum_{j=2}^{M} \mathcal{L}\left(V_{1}, V_{j}\right) \\\mathcal{L}_{F}&=\sum_{1 \leq i<j \leq M} \mathcal{L}\left(V_{i}, V_{j}\right)\end{align}</script><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>文章一共做了三组实验：</p><ul><li>Two established image representation learning benchmarks: <strong>ImageNet and STL-10</strong></li><li>Video representation learning tasks with 2 views: image and optical flow modalities</li><li>More than 2 views. </li></ul><h3 id="How-does-mutual-information-affect-representation-quality"><a href="#How-does-mutual-information-affect-representation-quality" class="headerlink" title="How does mutual information affect representation quality?"></a>How does mutual information affect representation quality?</h3><p>文章认为，Mutual Information Maximization不是让CMC成功的关键，关键是在于CMC背后关于representation的假设：“好的信息应该在不同的view之间被共享”，从而最大化不同view之间representation的MI，才能导致好的representation. </p><p>文章在高清2K的图片中random crop $64\times 64$的patch来进行Mutual Informatio之间的训练，唯一变量是patch之间间隔的pixel距离，通过两个view之间的MI estimation的差距来比较MI对最终downstream classification的影响，结果如下右图所示（左图表示的不同的RGB通道之间的关联，结果有些违反常识）.</p><p><img src="https://i.loli.net/2020/05/25/gsbQRm5A6ef7o3F.png" alt="image.png"></p><p>根据右图显示的结果，作者得出结论：</p><blockquote><p>Here we see that views with too little or too much MI perform worse; a sweet spot in the middle exists which gives the best representation. That there exists such a sweet spot should be expected. If two views share no information, then, in principle, there is no incentive for CMC to learn anything. If two views share all their information, no nuisances are discarded and we arrive back at something akin to an autoencoder or generative model, that simply tries to represent all the bits in the multiview data.</p><p>These experiments demonstrate that the relationship between mutual information and representation quality is meaningful but not direct. Selecting optimal views, which just share relevant signal, may be a fruitful direction for future research.</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.05849&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;paper “Contrastive Mult
      
    
    </summary>
    
    
      <category term="Contrastive" scheme="http://www.shihaizhou.com/tags/Contrastive/"/>
    
      <category term="MI" scheme="http://www.shihaizhou.com/tags/MI/"/>
    
  </entry>
  
  <entry>
    <title>On MI Maximization for Representation Learning</title>
    <link href="http://www.shihaizhou.com/2020/05/22/On-MI-Maximization-for-Representation-Learning/"/>
    <id>http://www.shihaizhou.com/2020/05/22/On-MI-Maximization-for-Representation-Learning/</id>
    <published>2020-05-22T10:51:40.000Z</published>
    <updated>2020-06-06T07:44:04.095Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/abs/1907.13625" target="_blank" rel="noopener">ICLR-2020 paper “On Mutual Information Maximization for Representation Learning”</a></li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators.</p><h2 id="Structure-of-Introduction"><a href="#Structure-of-Introduction" class="headerlink" title="Structure of Introduction"></a>Structure of Introduction</h2><ul><li>What is MI: definition</li><li>Fundamental properties of MI<ul><li>Firstly, MI is invariant under reparametrization of the variables - namely, if $X^\prime = f_1(X)$ and $Y^\prime = f_2(Y)$ are homeomorphisms (i.e. smooth invertible maps), then $I (X ; Y ) = I (X ^\prime ; Y ^\prime )$.</li><li>Secondly, estimating MI in high-dimensional spaces is a notoriously difficult task, and in practice one often maximizes a tractable lower bound on this quantity. </li><li>Any distribution-free high-confidence lower bound on entropy requires a sample size exponential in the size of the bound. </li></ul></li><li>In fact, we show that <strong>maximizing tighter bounds on MI can result in worse representations.</strong> </li><li>In addition, we establish a <strong>connection to deep metric learning</strong> and argue that this interpretation may be a plausible explanation of the success of the recently introduced methods. </li></ul><h2 id="Multi-view-formulation-of-the-MI-maximization"><a href="#Multi-view-formulation-of-the-MI-maximization" class="headerlink" title="Multi-view formulation of the MI maximization"></a>Multi-view formulation of the MI maximization</h2><p>In the image classification setting, for a given image $X$, let $X^{(1)}$ and $X^{(2)}$ be different, possibly overlapping views of $X$, for instance the top and bottom halves of the image. These are encoded using encoders $g_1$ and $g_2$ respectively, and the MI between the two representationsg $g_1(X^{(1)})$ and $g_2(X^{(2)})$ is maximized, </p><script type="math/tex; mode=display">\max _{g_{1} \in \mathcal{G}_{1}, g_{2} \in \mathcal{G}_{2}} \quad \hat I\left(g_{1}\left(X^{(1)}\right) ; g_{2}\left(X^{(2)}\right)\right)</script><p>where $\hat I$ represents the sample based MI estimator of the true MI $I(X;Y)$ and the function classes $\mathcal{G}_{1}$ and $\mathcal{G}_{2}$ can be used to specify structural constraints on the encoders. One thing to note here is that two encoders $g_1$ and $g_2$ often share parameters. </p><p>It gives us plenty of modeling flexibility, as the two <strong>views</strong> can be chosen to capture completely different aspects and modalities of the data, for example:</p><ul><li>In the basic form of <strong>DeepInfoMax</strong> $g_1$ extracts global features from the entire image $X^{(1)}$ and $g_2$ local features from image patches $X^{(2)}$, where $g_1$ and $g_2$ correspond to activations in different layers of the same convolutional network. (也就是上一节介绍的global/local-MI Maximization)</li><li><strong>Contrastive multiview coding (CMC)</strong> generalizes the objective in to consider multiple views $X^{(i)}$, where each $X^{(i)}$ corresponds to a different image modality (e.g., different color channels, or the image and its segmentation mask).</li><li><strong>Contrastive predictive coding(CPC)</strong> incorporates a sequential component of the data. Concretely, one extracts a sequence of patches from an image in some fixed order, maps each patch using an encoder, aggregates the resulting features of the first $t$ patches into a context vector, and maximizes the MI between the context and features extracted from the patch at position $t+k$</li></ul><h2 id="MI-Estimators"><a href="#MI-Estimators" class="headerlink" title="MI Estimators"></a>MI Estimators</h2><h3 id="InfoNCE"><a href="#InfoNCE" class="headerlink" title="InfoNCE"></a>InfoNCE</h3><script type="math/tex; mode=display">I(X ; Y) \geq \mathbb{E}\left[\frac{1}{K} \sum_{i=1}^{K} \log \frac{e^{f\left(x_{i}, y_{i}\right)}}{\frac{1}{K} \sum_{j=1}^{K} e^{f\left(x_{i}, y_{j}\right)}}\right] \triangleq I_{\mathrm{NCE}}(X ; Y)</script><p>其中$\{(x_i, y_i)\}_{i=1}^K$是joint distribution中采样出来的$K$个样本. 背后的动机就是想要训练一个discriminator来区分对应的$(x_i, y_i)$和不相互对应的$(x_i, y_j)$. </p><h3 id="KL-divergence-from-NWJ"><a href="#KL-divergence-from-NWJ" class="headerlink" title="KL divergence from NWJ"></a>KL divergence from NWJ</h3><script type="math/tex; mode=display">I(X ; Y) \geq \mathbb{E}_{p(x, y)}[f(x, y)]-e^{-1} \mathbb{E}_{p(x)}\left[\mathbb{E}_{p(y)} e^{f(x, y)}\right] \triangleq I_{\text {NWJ }}(X ; Y)</script><p>这就是通用的MI紧的下界.</p><h1 id="Biases-in-Approximate-MI-Maximization"><a href="#Biases-in-Approximate-MI-Maximization" class="headerlink" title="Biases in Approximate MI Maximization"></a>Biases in Approximate MI Maximization</h1><p>组成当前approximate MI maximization for representation learning方法的两个部分为以下</p><ul><li>Encoder: influenced by network structures / parameters. </li><li>Critics: the function tells between the </li><li>Estimators: the lower bound to optimize.</li></ul><p>围绕这两个部分文章做了四方面的实验探究这两个部分对approximate MI maximization学到的representation quallity的影响，为以下：</p><ul><li>encoders are bijective. (not reported in this blog)</li><li>encoders that can model both invertible and non-invertible functions. (not reported in this blog)</li><li>different critics: simple / loose bound would lead to high-capacity critics. </li><li>encoder is more important than architectures. </li></ul><h2 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h2><p><strong>Motivation</strong>: Our goal is to provide a minimal set of easily reproducible empirical experiments to understand the role of MI estimators, critic and encoder architectures when learning representations via the objective. </p><p><strong>Dataset</strong>: To this end, we consider a simple setup of learning a representation of <strong>the top half of MNIST handwritten digit images</strong>. Try to maximize the mutual information of the representation between the representation of the top half and the bottom half of the images. Using bilinear critic: $f(x,y)=x^TWy$. </p><p><strong>Evaluation</strong>: Following the widely adopted <strong>downstream linear evaluation protocol</strong>. </p><h2 id="Higher-Capacity-Critics-Worse-Downstream-Tasks"><a href="#Higher-Capacity-Critics-Worse-Downstream-Tasks" class="headerlink" title="Higher Capacity Critics: Worse Downstream Tasks"></a>Higher Capacity Critics: Worse Downstream Tasks</h2><p>In the previous section we have established that MI and downstream performance are only loosely connected. Clearly, maximizing MI is not sufficient to learn good representations and there is a non-trivial interplay between the architectures of the <strong>encoder, critic, and the underlying estimators.</strong></p><p>这部分的内容将会主要研究critic architecture如何影响representation quality. Normally, a higher capacity critic should allow for a tighter lower-bound on MI, while this section shows that looser bounds (i.e., simpler critic function) lead to better representation quality. </p><p>Compared 3 different architectures of the critic functions: </p><ul><li>a bilinear critic, </li><li>a separable critic $f(x,y) = \phi_1(x)^⊤\phi_2(y)$ ($\phi_1$, $\phi_2$ are MLPs with a single hidden layer with 100 units and ReLU activations, followed by a linear layer with 100 units; comprising 40k parameters in total) </li><li>an MLP critic with a single hidden layer with 200 units and ReLU activations, applied to the concatenated input [x, y] (40k trainable parameters).</li></ul><p>实验结果如下所示，支持了”<strong>looser bounds (i.e., simpler critic function) lead to better representation quality</strong>“ 的结论. </p><p><img src="https://i.loli.net/2020/05/23/GU5RDcObT8SJA1x.png" alt="image.png"></p><h2 id="Encoder-Architecture-More-Important-than-Specific-Estimator"><a href="#Encoder-Architecture-More-Important-than-Specific-Estimator" class="headerlink" title="Encoder Architecture: More Important than Specific Estimator"></a>Encoder Architecture: More Important than Specific Estimator</h2><p>简单概括这一部分的实验，作者比较了两个encoder architecture: ConvNet和MLP，分别在$I_{NCE}$和$I_{NWJ}$这两个estimator上测试下游任务accuracy. 发现encoder architecure的影响比estimator要更大。</p><p>To ensure that both network architectures achieve the same lower bound $I_{EST}$ on the MI, we minimize $L_{t}\left(g_{1}, g_{2}\right)=| I_{\mathrm{EST}}\left(g_{1}\left(X^{(1)}\right) ; g_{1}\left(X^{(2)}\right)\right)-t |$ instead of solving original information maximization problem, for two different values $t = 2, 4$.</p><p><img src="https://i.loli.net/2020/05/23/azm3dnruVLEJD49.png" alt="image.png"></p><p>实验结果如下图所示. 值得注意的是，实验图给出的曲线看起来像是有一个提升的过程，但是和他们的起始点比较提升是相对marginal的。同时我们有理由推测作者将MNIST作为主要的实验数据集是因为InfoMax在更为复杂的数据集上表现得非常不理想：例如在附录G中的CIFAR-10，作者汇报的结果为以下图示，可以看到表现并不理想，下游任务的提升非常微小。</p><p><img src="https://i.loli.net/2020/06/06/EAwQdirJBugYSUT.png" alt="image.png"></p><h1 id="Deep-Metric-Learning"><a href="#Deep-Metric-Learning" class="headerlink" title="Deep Metric Learning"></a>Deep Metric Learning</h1><p>Given sets of triplets, namely an anchor point $x$, a positive instance $y$, and a negative instance $z$, the goal is to learn a representation $g(x)$ such that the distances between $g(x)$ and $g(y)$ is smaller than the distance between $g(x)$ and $g(z)$, for each triplet. 从这个角度也就不难看出为什么会称MI maximization中的函数为critic function. 而InfoNCE又可以被写作：</p><script type="math/tex; mode=display">\begin{align}I_{\mathrm{NCE}}&=\mathbb{E}\left[\frac{1}{K} \sum_{i=1}^{K} \log \frac{e^{f\left(x_{i}, y_{i}\right)}}{\frac{1}{K} \sum_{j=1}^{K} e^{f\left(x_{i}, y_{j}\right)}}\right] \\&=\log K-\mathbb{E}\left[\frac{1}{K} \sum_{i=1}^{K} \log \left(1+\sum_{j \neq i} e^{f\left(x_{i}, y_{j}\right)-f\left(x_{i}, y_{i}\right)}\right)\right]\end{align}</script><p>后面这一项也就是希望训练一个metric function使得正负样本之间的distance差距能够尽量变大。</p><h1 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h1><h3 id="Alternative-measures-of-information"><a href="#Alternative-measures-of-information" class="headerlink" title="Alternative measures of information"></a>Alternative measures of information</h3><p>While MI has appealing theoretical properties, it is clearly not sufficient for this task—it is hard to estimate, invariant to bijections and can result in suboptimal representations which do not correlate with downstream performance. Therefore, a new notion of information should account for both the amount of information stored in a representation and the geometry of the induced space necessary for good performance on downstream tasks. One possible avenue is to consider extensions to MI which explicitly account for the modeling power and computational constraints of the observer, such as the recently introduced F-information.</p><h3 id="Going-beyond-the-widely-used-linear-evaluation-protocol"><a href="#Going-beyond-the-widely-used-linear-evaluation-protocol" class="headerlink" title="Going beyond the widely used linear evaluation protocol"></a>Going beyond the widely used linear evaluation protocol</h3><p>While it was shown that learning good representations under the linear evaluation protocol can lead to reduced sample complexity for downstream tasks (Arora et al., 2019), some recent works (Bachman et al., 2019; Tian et al., 2019) report marginal improvements in terms of the downstream performance under a non-linear regime. Related to the previous point, it would hence be interesting to further explore the implications of the evaluation protocol, in particular its importance in the context of other design choices. We stress that a highly-nonlinear evaluation framework may result in better downstream performance, but it defeats the purpose of learning efficiently transferable data representations.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1907.13625&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ICLR-2020 paper “On Mut
      
    
    </summary>
    
    
      <category term="MI" scheme="http://www.shihaizhou.com/tags/MI/"/>
    
  </entry>
  
  <entry>
    <title>Variational Auto-Encoders</title>
    <link href="http://www.shihaizhou.com/2020/05/22/Variational-Auto-Encoders/"/>
    <id>http://www.shihaizhou.com/2020/05/22/Variational-Auto-Encoders/</id>
    <published>2020-05-21T16:06:42.000Z</published>
    <updated>2020-06-07T08:21:11.194Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://www.youtube.com/watch?v=uaaqyVS9-rM" target="_blank" rel="noopener">Lecture on youtube</a></li><li><a href="https://arxiv.org/pdf/1606.05908.pdf" target="_blank" rel="noopener">tutorial on VAEs</a></li></ul><h1 id="Latent-Variable-Models"><a href="#Latent-Variable-Models" class="headerlink" title="Latent Variable Models"></a>Latent Variable Models</h1><h2 id="Setting"><a href="#Setting" class="headerlink" title="Setting"></a>Setting</h2><p>Formally, say we have a vector of latent variables $z​$ in a high-dimensional space $\mathcal Z​$ which we can easily sample according to some probability density function (PDF) $P(z)​$ defined over $\mathcal Z​$.  Then, say we have a family of deterministic functions $f (z; \theta)​$, parameterized by a vector $\theta​$ in some space $\Theta​$, where $f:\mathcal{Z} \times \Theta \rightarrow \mathcal{X}​$.</p><h2 id="Target"><a href="#Target" class="headerlink" title="Target"></a>Target</h2><p>We wish to optimize $\theta$ such that we can sample $z$ from $P(z)$ and, with high probability, $f (z; θ)$ will be like the $X$’s in our dataset. To make this notion precise mathematically, we are aiming maximize the probability of each $X$ in the training set under the entire generative process, according to:</p><script type="math/tex; mode=display">P(X)=\int P(X | z ; \theta) P(z) d z</script><p>In VAEs, the choice of this output distribution is often Gaussian, i.e., </p><script type="math/tex; mode=display">P(X | z ; \theta)=\mathcal{N}\left(X | f(z ; \theta), \sigma^{2} * I\right)</script><h1 id="Variational-Autoencoders"><a href="#Variational-Autoencoders" class="headerlink" title="Variational Autoencoders"></a>Variational Autoencoders</h1><p>VAE的数学推导和Autoencoders并没有太大的关系，为什么会被叫做VAE的原因在于最后从setup推导出的training objective由encoder和decoder两个部分组成，从而成为了AE的形式。</p><p>为了最大化上文中的等式</p><script type="math/tex; mode=display">P(X)=\int P(X | z ; \theta) P(z) d z</script><p>VAE需要解决以下两个问题：（1）如何定义latent variable $z$；（2）如何处理在 $z$ 上的积分. </p><h3 id="How-to-define-the-latent-variable"><a href="#How-to-define-the-latent-variable" class="headerlink" title="How to define the latent variable"></a>How to define the latent variable</h3><p>第一个问题中，我们知道latent variable决定了最终数据点的形成。以手写数字数据为例，用笔的角度，起笔的位置，行笔的速度都影响最终生成的数字图像。在为这些latent variable建模的时候我们需要遵守以下另两个原则：</p><ul><li>Avoid deciding by hand what information each dimension of $z$ encodes</li><li>Avoid explicitly describing the dependencies—i.e., the latent structure—between the dimensions of $z$.</li></ul><p>在VAE中我们简单地将latent variable限制在了normal distribution上：they assume that there is no simple interpretation of the dimensions of $z$, and instead assert that samples of $z$ can be drawn from a simple distribution, i.e., $\mathcal N (0, I)$, where $I$ is the identity matrix. </p><p>能够这么做的原因在于我们可以采用super powerful functions to map the normal distribution to some non-trivial distribution, 例如将其映射成为一个环. 我们只需要采用函数$g(z)=z/10+z/|z|$即可，下图中的左图表示从normal distribution上采样的数据点，右图则表示经过映射后的数据点分布。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X = np.random.multivariate_normal([<span class="number">0</span>, <span class="number">0</span>], [[<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>,<span class="number">1</span>]], data_size)</span><br><span class="line">Z = X / <span class="number">10</span> + X / np.sqrt(np.square(X).sum(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/05/24/fZhHodiXSnGIJjT.png" alt="image.png"></p><h3 id="How-to-deal-with-the-Integral"><a href="#How-to-deal-with-the-Integral" class="headerlink" title="How to deal with the Integral"></a>How to deal with the Integral</h3><p>在完成以上简化后我们有$P(z)=\mathcal N(z|0, I)$. 对于一般的机器学习问题来说，我们只需要得到$P(x|z;\theta)$的公式，对其进行Monte-Carlo estimate &amp; optimization就可以完成，但其中的问题是对于高维数据我们需要非常多的sample才能够有比较accurate estimation. 因此这不是一个tractable的方法。</p><h2 id="Setting-up-the-Objective"><a href="#Setting-up-the-Objective" class="headerlink" title="Setting up the Objective"></a>Setting up the Objective</h2><p>The key idea behind the variational autoencoder is to attempt to sample values of $z$ that are likely to have produced $X$, and compute $P(X)$ just from those. This means that we need a new function $Q(z|X)$ which can take a value of $X$ and give us a distribution over $z$ values that are likely to produce $X$. 这样我们在分布 $Q$ 的帮助下，就非常容易计算$E_{z \sim Q} P(X | z)$，但这只是隐变量$z$在分布$Q$下$P(X)$的估计，和真实的$P(X)$之间是有差距的，为了达到最终目的”optimize $P(X)$”，我们需要”relate $E_{z∼Q}P(X|z)$ and $P(X)$”. </p><p>这里我们选择KL divergence来描述$Q(z)$和$P(z|X)$间的差距：</p><script type="math/tex; mode=display">\mathcal{D}[Q(z) \| P(z | X)]=E_{z \sim Q}[\log Q(z)-\log P(z | X)]</script><p>使用贝叶斯条件概率公式，我们可以得到：</p><script type="math/tex; mode=display">\mathcal{D}[Q(z) \| P(z | X)]=E_{z \sim Q}[\log Q(z)-\log P(X | z)-\log P(z)]+\log P(X)</script><p>移项，得到：</p><script type="math/tex; mode=display">\log P(X)-\mathcal{D}[Q(z) \| P(z | X)]=E_{z \sim Q}[\log P(X | z)]-\mathcal{D}[Q(z) \| P(z)]</script><p>在这条式子中，我们需要注意两点：首先，第一项$\log P(X)$对于给定的$X$来说是一个常数；第二，式子中用于将sample $X$映射到隐变量分布的$Q$是任意的分布，not just a distribution which does a good job mapping $X$ to the z’s that can produce $X$. </p><p>因为我们比较感兴趣推断$P(X)$，所以我们可以让”$Q$ depend on $X$, and in particular, one which makes $\mathcal{D}[Q(z) ||P(z | X)]$ small”. </p><script type="math/tex; mode=display">\log P(X)-\mathcal{D}[Q(z | X) \| P(z | X)]=E_{z \sim Q}[\log P(X | z)]-\mathcal{D}[Q(z | X) \| P(z)]</script><blockquote><p>This equation serves is the core of the variational autoencoder, and it’s worth spending some time thinking about what it says . In two sentences, the left hand side has the quantity we want to maximize: $log P(X)$ (plus an error term, which makes $Q$ produce $z$’s that can reproduce a given $X$; this term will become small if $Q$ is high-capacity). The right hand side is something we can optimize via stochastic gradient descent given the right choice of $Q$ (although it may not be obvious yet how). Note that the framework—in particular, the right hand side of the Equation—has suddenly taken a form which looks like an autoencoder, since $Q$ is “encoding” $X$ into $z$, and $P$ is “decoding” it to reconstruct $X$. We’ll explore this connection in more detail later.</p></blockquote><p>总的来说，我们想要极大化左边的式子，也就是使$P(X)$能够尽量变大。同时假设如果我们有一个high-capacity $Q$，极大化左边的式子能够ideally使$Q$和$P$两者完全match. 左边的式子是没有办法直接优化的，上式给出了一个等价优化表达式，所以我们只需要关注在右式的优化即可。</p><h2 id="Optimizing-the-Objective"><a href="#Optimizing-the-Objective" class="headerlink" title="Optimizing the Objective"></a>Optimizing the Objective</h2><p>通常我们将$Q(z|X)$限制在一个normal distribution上，即$Q(z|X)=\mathcal{N}(z | \mu(X ; \vartheta), \Sigma(X ; \vartheta))$，其中$\mu$和$\Sigma$分别为由参数$\theta$决定的deterministic function. 根据最开始我们对隐变量$z$做的假设：assert that samples of $z$ can be drawn from a simple normal distribution. 于是上式的最后一项简化成了两个multi-variant Gaussian distribution之间的KL divergence. 于是在我们的情况下，可以将该项简化成：</p><script type="math/tex; mode=display">\begin{aligned} \mathcal{D}[\mathcal{N}(\mu(X), \Sigma(X)) \| \mathcal{N}(0, I)]=&\frac{1}{2}\left(\operatorname{tr}(\Sigma(X))+(\mu(X))^{\top}(\mu(X))-k-\log \operatorname{det}(\Sigma(X))\right) \end{aligned}</script><p>右式的第一项，我们可以通过在$Q$中多次采样计算平均值的方式来estimate. 从数据集$D$出发，我们重新写一下以上的优化目标式：</p><script type="math/tex; mode=display">\begin{aligned} E_{X \sim D}[\log P(X)-\mathcal{D}[Q(z | X) \| P(z | X)]] &=\\ E_{X \sim D}\left[E_{z \sim O}\right.&[\log P(X | z)]-\mathcal{D}[Q(z | X) \| P(z)]] \end{aligned}</script><p>将优化式内部的期望拿掉，因为本来我们也就打算采用schotastic gradient descent.</p><script type="math/tex; mode=display">\log P(X | z)-\mathcal{D}[Q(z | X) \| P(z)]</script><blockquote><p>There is, however, a significant problem with this equation. $E_{z∼Q} [\log P(X|z)]$ depends not just on the parameters of $P$, but also on the parameters of $Q$. However, in the equation above, this dependency has disappeared! In order to make VAEs work, it’s essential to drive $Q$ to produce codes for $X$ that $P$ can reliably decode.</p></blockquote><p>从VAE在training阶段的行为来看，因为SGD没有办法将loss传回给网络中间的sample层，所以这里VAE采用了一个reparameterization trick，具体如下图所示。</p><p><img src="https://i.loli.net/2020/05/25/85QniDcsRlALEya.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=uaaqyVS9-rM&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Lecture on y
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
  </entry>
  
</feed>
