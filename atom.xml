<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>In Love with CodeCode</title>
  
  <subtitle>Haizhou&#39;s Blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.shihaizhou.com/"/>
  <updated>2020-07-28T08:38:41.845Z</updated>
  <id>http://www.shihaizhou.com/</id>
  
  <author>
    <name>Haizhou Shi</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>EM Algorithm</title>
    <link href="http://www.shihaizhou.com/2020/07/24/EM-Algorithm/"/>
    <id>http://www.shihaizhou.com/2020/07/24/EM-Algorithm/</id>
    <published>2020-07-24T05:22:07.000Z</published>
    <updated>2020-07-28T08:38:41.845Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li>Pattern Recognition and Machine Learning. </li></ul><h1 id="Gaussian-Mixtures"><a href="#Gaussian-Mixtures" class="headerlink" title="Gaussian Mixtures"></a>Gaussian Mixtures</h1><p>Suppose  a $K$-dimensional one-hot binary random variable $z$ having a 1-of-$K$ representation in which a particular element $z_k$ equals to 1 and all other elements are equal to 0. Thus the random variable $z$ satisfies $z_k\in\{0,1\}$ and $\sum_k z_k=1$.  </p><p>The Guassian mixture distribution is written as a linear superposition of Gaussians in the form as follows:</p><script type="math/tex; mode=display">\begin{align}p(x) &= \mathbb E_z[p(x|z)] = \sum_z p(x|z) p (z) \\&= \sum_{k=1}^{K} p(x|z_k=1) p (z_k=1) \\&= \sum_{k=1}^{K} \mathcal N(x|\mu_k, \Sigma_k) p (z_k=1) \\ &= \sum_{k=1}^{K} \pi_k \mathcal N(x|\mu_k, \Sigma_k) \\ \end{align}</script><p>where $\pi_k$ represents the marginal probability of the $z_k$, in other words, $\pi_k$ is the prior probability of $z_k=1$.  The insight behind the setting above is that for every given data point $x^{(n)}$, there is a corresponding latent variable $z^{(n)}$. </p><p>We use quantity $\gamma (z_k)$ to denote the conditional probability $p(z_k=1|x)$, which has another name ‘posterior’. </p><script type="math/tex; mode=display">\gamma\left(z_{k}\right) \equiv p\left(z_{k}=1 | x\right) = \frac{\pi_{k} \mathcal{N}\left({x} | {\mu}_{k}, {\Sigma}_{k}\right)}{\sum_{j=1}^{K} \pi_{j} \mathcal{N}\left({x} | {\mu}_{j}, {\Sigma}_{j}\right)}</script><h2 id="Maximum-Likelihood"><a href="#Maximum-Likelihood" class="headerlink" title="Maximum Likelihood"></a>Maximum Likelihood</h2><p>Suppose we have the observation of a set of i.i.d data points $X=\{ x^{(1)}, \cdots , x^{(N)} \}$, the log of the probability of this dataset is as follows:</p><script type="math/tex; mode=display">\ln p(X|\pi,\mu, \Sigma) = \sum_{n=1}^{N} \ln \left\{\sum_{k=1}^{K} \pi_{k} \mathcal{N}\left({x}^{(n)} \mid {\mu}_{k}, {\Sigma}_{k}\right)\right\}</script><p>The difficulty arises from the presence of the summation over $k$ that appears inside the logarithm, so that the logarithm function no longer acts directly on the Gaussian. If we set the derivatives of the log likelihood to zero, we will no longer obtain a closed form solution. To solve this inconvenience is one of the motivations for the EM algorithm.</p><h2 id="EM-for-Gaussian-Mixtures"><a href="#EM-for-Gaussian-Mixtures" class="headerlink" title="EM for Gaussian Mixtures"></a>EM for Gaussian Mixtures</h2><p>The posterior $\gamma (z_{nk})$ of the sample $x^{(n)}$, according to the previous definition, could be written in the following form: </p><script type="math/tex; mode=display">\gamma\left(z_{nk}\right) \equiv p\left(z_{k}=1 | x^{(n)}\right) = \frac{\pi_{k} \mathcal{N}\left({x^{(n)}} | {\mu}_{k}, {\Sigma}_{k}\right)}{\sum_{j=1}^{K} \pi_{j} \mathcal{N}\left({x^{(n)}} | {\mu}_{j}, {\Sigma}_{j}\right)}</script><p>There are three sets of parameters in the Gaussian mixtures: $\pi, \mu, \Sigma$. Now let’s optimize them one by one. First we set the derivatives of the log likelihood $p(X|\pi, \mu, \Sigma)$ to 0 with respect to the means $\mu_k$ of the Gaussian components, yielding</p><script type="math/tex; mode=display">0=-\sum_{n=1}^{N} \underbrace{\frac{\pi_{k} \mathcal{N}\left(x^{(n)} | \mu_{k}, \Sigma_{k}\right)}{\sum_{j} \pi_{j} \mathcal{N}\left(x^{(n)} | \mu_{j}, \Sigma_{j}\right)}}_{\gamma\left(z_{n k}\right)} \Sigma_{k}\left(x^{(n)}-\mu_{k}\right)</script><p>Define:</p><script type="math/tex; mode=display">N_{k}=\sum_{n=1}^{N} \gamma\left(z_{n k}\right)</script><p>Thus we have <strong>i) the solution of</strong> $\mu_k$: </p><script type="math/tex; mode=display">\mu_{k}=\frac{1}{N_{k}} \sum_{n=1}^{N} \gamma\left(z_{n k}\right) x^{(n)}</script><p>Similarly, we can derive <strong>ii) the solution of</strong> $\Sigma_k$, which depends on  $\mu_k$:</p><script type="math/tex; mode=display">\Sigma_{k}=\frac{1}{N_{k}} \sum_{n=1}^{N} \gamma\left(z_{n k}\right)\left(x^{(n)}-\mu_{k}\right)\left(x^{(n)}-\mu_{k}\right)^{\mathrm{T}}</script><p>Finally we derive the process of optimizing the posterior $\pi$ using the techinque of Lagrange multiplier since the optimization of $\pi$ is restrained by the following constraint: </p><script type="math/tex; mode=display">\sum_{k=1}^K \pi_k =1</script><p>Thus we maximize the following quantity: </p><script type="math/tex; mode=display">\ln p(X|\pi, \mu, \Sigma) + \lambda \left(\sum_{k=1}^K \pi_k -1\right)</script><p>which gives us the following equation: </p><script type="math/tex; mode=display">0=\sum_{n=1}^{N} \frac{\mathcal{N}\left({x}^{(n)} | {\mu}_{k}, {\Sigma}_{k}\right)}{\sum_{j} \pi_{j} \mathcal{N}\left({x}^{(n)} | {\mu}_{j}, {\Sigma}_{j}\right)}+\lambda</script><p>If we now multiply both sides by $\pi_k$ and sum over $k$ making use of the constraint, we find $\lambda = -N$. Using this to eliminate $\lambda$ and rearranging we obtain <strong>iii) the optimal of</strong>  $\pi_k$: </p><script type="math/tex; mode=display">\pi_k = \frac{N_k}{N}</script><p>The solutions above don’t give us an “closed-form” solution for the parameters of the mixtures model because the responsibilities $\gamma(z_{nk})$ depend on those parameters in a complex way through the expression of this posterior shown in the previous session, while it does suggest an iterative method, which is exactly EM, of optimizing the Gaussian mixtures model: </p><ul><li><strong>Initialization</strong>. Initialize the parameter set which consists of $\{\pi, \mu, \Sigma\}$. </li><li>Repeat following processes until convergence:<ul><li><strong>E-step</strong>. Estimate the posterior (responsibility) $\gamma(z_{nk})$. </li><li><strong>M-step</strong>. Maximize the parameter set $\{\pi, \mu, \Sigma \}$ by the support of the posterior. </li><li><strong>Log likelihood estimation</strong>. Estimate the log likelihood to check whether convergence happens. </li></ul></li></ul><h1 id="General-EM-Algorithm"><a href="#General-EM-Algorithm" class="headerlink" title="General EM Algorithm"></a>General EM Algorithm</h1><p>Consider a probabilistic model in which we collectively denote all of the observed variables by $X$ and all of the hidden variables by $Z$. The joint distribution $p(X, Z|\theta)$ is governed by a set of parameters denoted $\theta$. Our goal is to maximize the log likelihood function that is given by</p><script type="math/tex; mode=display">\ln \left[ p(X|\theta) \right] =\ln \left[ \sum_Z p (X,Z|\theta) \right]</script><p><strong>The basic premise of the general EM algorithm</strong> is that the direct optimization over the likelihood function $p(X|\theta)$ is hard while the optimization over the complete-data likelihood $p(X,Z|\theta)$ is much easier. As we can see in the equation above, the sum of the probability is inside the log, the optimization of which is non-trivial even for a simple normal distribution. By introducing another distribution $q(Z)$, we can decompose the objective (log likelihood) into two components: </p><script type="math/tex; mode=display">\ln p({X} | {\theta})=\mathcal{L}(q, {\theta})+\operatorname{KL}(q \| p)</script><p>where </p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}(q, {\theta}) &=\sum_ q({Z}) \ln \left\{\frac{p({X}, {Z} | {\theta})}{q({Z})}\right\} \\\operatorname{KL}(q \| p) &= \sum_ q({Z}) \ln \left\{\frac{q({Z})}{p({Z} | {X}, {\theta})}\right\}\end{aligned}</script><p>The EM algorithm is a two-stage iterative optimization technique for finding maximum likelihood solutions. We can use the decomposition above to define the EM algorithm and to demonstrate that it does indeed maximize the log likelihood. </p><h2 id="E-step"><a href="#E-step" class="headerlink" title="E-step"></a>E-step</h2><p>First during the E step, we are minimizing the KL-divergence between $q(Z)$ and $p(Z|X)$. This optimization has nothing to do with the parameter set, thus will not change the final objective, log likelihood. To minimize the KL-divergence, we set the introduced distribution $q$ to the posterior and by doing so, we make the lower bound $\mathcal L (q, \theta)$ equal to the log likelihood. Suppose the current parameter is $\theta^{\text{old}}$, then after the E-step we have</p><script type="math/tex; mode=display">q(Z) = p(Z|X, \theta^{\text{old}})</script><p>Substitute $q$ with $p$ in the lower bound $\mathcal L (q, \theta)$ we obtain </p><script type="math/tex; mode=display">\mathcal L (q, \theta) = \sum_Z p(Z|X, \theta^{\text{old}}) \ln \left\{ \frac{p(X, Z|\theta)}{p(Z|X, \theta^{\text{old}})} \right\}</script><h2 id="M-step"><a href="#M-step" class="headerlink" title="M-step"></a>M-step</h2><p>Then we can maximize the $\mathcal L(q, \theta)$ w.r.t $\theta$. Instead of directly optimizing $\mathcal L$, we first get rid of the denominator term inside the $\ln$ function, since it’s just the entropy of the posterior, which is a constant: </p><script type="math/tex; mode=display">\begin{align}\mathcal L(q, \theta) &= \sum_Z p(Z|X, \theta^{\text{old}})\ln p(X,Z|\theta) + H(Z|X, \theta^{\text{old}}) \\&= \mathcal Q(\theta, \theta^{\text{old}}) + \operatorname{const}\end{align}</script><p>Then we can optimize the term $\mathcal Q$. Note that now the probability term is directly wrapped in the $\ln$ function. Thus if the probability model is of the exponential familiy, in this case Gaussian, the optimization is way simpler, which is demonstarated in the Gaussian mixtures model. Another thing to note here is the <strong>monotonous improvement of the log likelihood</strong>. </p><p>Before the M-step, $\mathcal L$ equals to the log likelihood because of the E-step. Then $\mathcal L$, the lower bound of the log likelihood is optimized w.r.t $\theta$ yielding $\theta^{\text{new}}$. Since the current $q$ is optimal w.r.t the old parameter set $\theta^{\text{old}}$, which indicates that the KL-divergence is not zero, there is guaranteed a gap between the updated $\mathcal L$ and the current log likelihood and proves that log likelihood increases. The following figure illustrates the process described above. </p><p><img src="https://i.loli.net/2020/07/28/c5zvfudUJto1RKl.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pattern Recognition and Machine Learning. &lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;Gaussian-Mixtures&quot;&gt;&lt;a href=&quot;#Gaussi
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
  </entry>
  
  <entry>
    <title>RL environment &amp; stable-baselines</title>
    <link href="http://www.shihaizhou.com/2020/07/19/RL-environment-stable-baselines/"/>
    <id>http://www.shihaizhou.com/2020/07/19/RL-environment-stable-baselines/</id>
    <published>2020-07-19T11:27:47.000Z</published>
    <updated>2020-07-19T12:50:16.571Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Customizing-Environments"><a href="#Customizing-Environments" class="headerlink" title="Customizing Environments"></a>Customizing Environments</h1><p>为了客制化符合自己需要的RL environment, 我们首先需要理解RL agent和env交互的主要逻辑：在每次开始前environment对环境中的各种变量进行重置并返回环境初始的observation <code>obs</code>，对应父类<code>gym.Env</code>中的<code>reset()</code>方法；agent将环境给出的<code>obs</code>映射到<code>action</code>上；环境接收到<code>action</code>之后对环境中的物理变量进行改变，并返回给agent一个四元组<code>(obs, reward, done, info)</code> ，其中<code>done</code>是一个表示环境的一个episode 是否结束的标志变量。</p><p>根据以上的逻辑，通用的 customized environments 只需要继承<code>gym.Env</code>上的四个函数，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomEnv</span><span class="params">(gym.Env)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        super(CustomEnv, self).__init__()</span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> </span></span><br><span class="line">        self.reset(*args)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> reset the state of the env.</span></span><br><span class="line">        <span class="keyword">return</span> obs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, action)</span>:</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> internal state change according to action.</span></span><br><span class="line">        <span class="keyword">return</span> obs, reward, done, info</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">render</span><span class="params">(self, mode=<span class="string">'human'</span>)</span>:</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> render the internal state of the env.</span></span><br></pre></td></tr></table></figure><h2 id="Action-amp-Obs-Space"><a href="#Action-amp-Obs-Space" class="headerlink" title="Action &amp; Obs Space"></a>Action &amp; Obs Space</h2><p>如果我们正在解决的是以algorithm为导向的项目，以上的逻辑已经满足了大部分的需求（因为model部分是我们自己开发的）。但是有时我们需要利用他人已经做好的RL algorithm来解决实际问题，而不太希望自己去处理算法细节，以上的实现就会缺少两个比较重要的components，分别是<code>action_space</code>和<code>observation_space</code>，它们限制了agent网络的input shape和output shape. </p><p><code>gym.spaces</code>已经实现了非常多的space类型，比较常见的是<code>Discrete</code>和<code>Box</code>. The <code>Discrete</code> space allows a fixed range of non-negative numbers. The <code>Box</code> space represents an n-dimensional box, so valid observations will be bounded by each <code>low</code>/<code>high</code> bound at each dimension. </p><p>除了以上两个常见类型（它们被大多数的agent algorithm所支持）之外，还有<code>MultiDiscrete</code>和<code>MultiBinary</code>. 我们的项目中就使用到了<code>MultiBinary</code>的space，因为我们需要对每个object去进行0-1分类来决定是否这个object应该被执行某种操作. 这两种类型不是非常常见，而且有些RL算法对于这种类型的space不支持. <code>Tuple</code>是可以将多个不同质的<code>action_space</code>组合在一起的<code>space</code>类型，但是它基本不被现成的RL算法库支持. </p><p>因此我们给出了<code>CustomEnv</code>所有需要实现的骨架代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomEnv</span><span class="params">(gym.Env)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        super(CustomEnv, self).__init__()</span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span></span></span><br><span class="line">        </span><br><span class="line">        self.action_space = <span class="literal">None</span></span><br><span class="line">        self.observation_space = <span class="literal">None</span></span><br><span class="line">        self.reset(*args)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> reset the state of the env.</span></span><br><span class="line">        <span class="keyword">return</span> obs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, action)</span>:</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> internal state change according to action.</span></span><br><span class="line">        <span class="keyword">return</span> obs, reward, done, info</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">render</span><span class="params">(self, mode=<span class="string">'human'</span>)</span>:</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> render the internal state of the env.</span></span><br></pre></td></tr></table></figure><h1 id="RL-Libraries"><a href="#RL-Libraries" class="headerlink" title="RL Libraries"></a>RL Libraries</h1><p>这个章节记录一下好用的RL算法库，从而帮助我们尽快完成模型的训练. </p><h2 id="Stable-baselines"><a href="#Stable-baselines" class="headerlink" title="Stable-baselines"></a>Stable-baselines</h2><p>stable-baselines是</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Customizing-Environments&quot;&gt;&lt;a href=&quot;#Customizing-Environments&quot; class=&quot;headerlink&quot; title=&quot;Customizing Environments&quot;&gt;&lt;/a&gt;Customizing En
      
    
    </summary>
    
    
      <category term="RL" scheme="http://www.shihaizhou.com/tags/RL/"/>
    
  </entry>
  
  <entry>
    <title>Bias-Variance Decompostion</title>
    <link href="http://www.shihaizhou.com/2020/06/28/Bias-Variance-Decomposition/"/>
    <id>http://www.shihaizhou.com/2020/06/28/Bias-Variance-Decomposition/</id>
    <published>2020-06-28T09:57:25.000Z</published>
    <updated>2020-06-30T05:49:36.533Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://www.youtube.com/watch?v=KtNwjbWbnh8" target="_blank" rel="noopener">youtube demonstration on point estimation</a></li><li><a href="http://www.inf.ed.ac.uk/teaching/courses/mlsc/Notes/Lecture4/BiasVariance.pdf" target="_blank" rel="noopener">course note mlsc</a></li><li><a href="http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/" target="_blank" rel="noopener">http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/</a></li><li>Slides of lecture No.4 in Prof.Cai’s Machine Learning course.</li><li><a href="https://www.youtube.com/watch?v=zUJbRO0Wavo" target="_blank" rel="noopener">Cornel CS4780 ML course on Bias-Variance Decomposition, which explains everything!</a></li></ul><p>网络上的资料覆盖了3种 bias-variance 的分解，让我在最初学习的时候有些迷惑。这篇博客依次介绍了这3种在思维复杂度上完全不同的分解推导，可以看到虽然它们都披着 Bias-Variance Decompostion 的外衣，但表述的含义是完全不同的。最为重要的是第一章节的第三section “Dataset Comes in Play”，是对这个问题最完整的分析，参考资料为Prof.Kilian Weinberger的ML课程. </p><h1 id="Bias-Variance-Decomposition"><a href="#Bias-Variance-Decomposition" class="headerlink" title="Bias-Variance Decomposition"></a>Bias-Variance Decomposition</h1><h2 id="Decomposition-of-Point-Estimation"><a href="#Decomposition-of-Point-Estimation" class="headerlink" title="Decomposition of Point Estimation"></a>Decomposition of Point Estimation</h2><p>First recall the <strong>variance</strong> definition.</p><script type="math/tex; mode=display">\operatorname{var}(\theta) = \mathbb E[\theta^2] - \mathbb E[\theta]^2</script><p>Suppose that we have a point estimate $\hat \theta$ of an oracle parameter $\theta$, which is a constant vector. Substituting the “$\theta$” above with $\hat \theta - \theta$, we have: </p><script type="math/tex; mode=display">\begin{align}\operatorname{var}(\hat \theta) &= \operatorname{var}(\theta-\hat \theta) \\&= \mathbb E\left[(\theta-\hat \theta)^2\right] - \left(\mathbb E[\theta-\hat\theta]\right)^2 \\&= \operatorname{EPE}(\hat \theta) - \left(\theta-\mathbb E\left[\hat\theta\right]\right)^2 \\&= \operatorname{EPE}(\hat \theta) - \left(\operatorname{bias}(\hat \theta)\right)^2 \\\Rightarrow \operatorname{EPE}(\hat \theta) &= \operatorname{var}(\hat \theta) + \left(\operatorname{bias}(\hat \theta)\right)^2\end{align}</script><h2 id="Decomposition-of-EPE"><a href="#Decomposition-of-EPE" class="headerlink" title="Decomposition of EPE"></a>Decomposition of EPE</h2><p>We use the <strong>Expected Prediction Error (EPE)</strong> of the estimate $\hat f$ as: </p><script type="math/tex; mode=display">\begin{align}\operatorname{EPE}(\hat f) &= \mathbb E_{x\sim X} [\hat f(x)-f(x)]^2 \\&= \int p(x) [\hat f(x)-f(x)]^2 dx \end{align}</script><p>The target of the machine learning is to make the EPE low. Suppose the oracle mapping from input $x$ to label $y$ is $f$ satisfying $y=f(x)$. And we have an estimate $\hat y =\hat f(x)$. </p><p>By the definition of the EPE, we can write it out in the form of $y$ and $\hat y$:</p><script type="math/tex; mode=display">\operatorname{EPE}(\hat f) = \mathbb E\left[(y-\hat y)^2\right]</script><p>In order the extract the term of variance of $\hat y$, we add a constant term $\mathbb E(\hat y)$ into the equation, yielding:</p><script type="math/tex; mode=display">\begin{align}\operatorname{EPE}(\hat f) &= \mathbb E\left[(y-\hat y)^2\right] \\&= \mathbb E\left[(y- \mathbb E(\hat y) + \mathbb E(\hat y) - \hat y)^2\right] \\&= \mathbb E \left[ (y-\mathbb E(\hat y))^2 \right] + \mathbb E\left[(\mathbb E(\hat y ) -\hat y)^2\right] + \mathbb E \left[ (y-\mathbb E(\hat y))(\mathbb E(\hat y ) -\hat y) \right]\end{align}</script><p>The last term is $0$, which is easy to prove, yielding:</p><script type="math/tex; mode=display">\begin{align}\operatorname{EPE}(\hat f) &= \mathbb E \left[ (y-\mathbb E(\hat y))^2 \right] + \mathbb E\left[(\mathbb E(\hat y ) -\hat y)^2\right] \\&= \mathbb E \left[ (y-\mathbb E(\hat y))^2 \right] + \operatorname{var}[\hat y] \\&= \operatorname{bias}^2+\operatorname{variance} + (\operatorname{noise})\end{align}</script><h2 id="Dataset-Comes-in-Play"><a href="#Dataset-Comes-in-Play" class="headerlink" title="Dataset Comes in Play"></a>Dataset Comes in Play</h2><p>In reality, considering the variance of the output value is not really making any sense since the problem itself might be challenging and the ground truth is of high variance. Then what does it mean when we decompose the learning objective into bias squared and variance and, oddly, where is noise? In this section, we will try to anatomize the problem of bais-variance decomposition, taking the dataset and the variance of the algorithm into account. </p><p>First let’s suppose the dataset is $D=\{(x_1,y_1), \cdots, (x_n,y_n)\}$, in which each element is independently drawn from the joint distribution: $(x,y)\sim P$.  Therefore $D\sim P^n$. </p><p>Then suppose we have a learning algorithm $\mathcal A$, which takes in a dataset as an input and learns an output function $h$. We denote this function as conditioned on $D$:</p><script type="math/tex; mode=display">h_D=\mathcal A(D)</script><p>We want to understand the property of the learned function $h$, not conditioned on certain dataset but the average performance of it. To achieve so, we can set the average function $\bar h$ as the expectation of $h$ over the dataset distribution:</p><script type="math/tex; mode=display">\begin{align}\bar h &= \mathbb E_{D\sim P^n}\left[\mathcal A(D)\right] \\&= \int_{D} h_D P(D) d D\end{align}</script><p>The test error of the learner $h$ is the expected error over the training dataset $D$ and the test samples drawn independently from $P$. </p><script type="math/tex; mode=display">\operatorname{EPE}(\mathcal A) = \mathbb E_{(x,y)\sim P; D\sim P^n} \left[ (h_D(x)-y)^2 \right]</script><p>Since we want to estimate the variance of the learner (how it fluctuates w.r.t its average $\bar h$), we add this term into the EPE: </p><script type="math/tex; mode=display">\begin{align}\operatorname{EPE}(\mathcal A) &= \mathbb E_{(x,y)\sim P, D\sim P^n} \left[ (h_D(x)-y)^2 \right] \\&= \mathbb E_{(x,y), D} \left[ (h_D(x)-\bar{h}(x) + \bar{h}(x) -  y)^2 \right] \\&= \underbrace{\mathbb E_{x,D} \left[ \left((h_D(x)-\bar{h}(x)\right)^2 \right]}_{\text{variance}} + \underbrace{\mathbb E_{x,y} \left[ \left(\bar{h}(x) -  y\right)^2 \right]}_{\text{term}_1} + 2 \underbrace{\mathbb E_{(x,y), D} \left[ (h_D(x)-\bar{h}(x))( \bar{h}(x) -  y) \right]}_{\text{term}_2} \\\end{align}</script><p>The explanation of each meaningful term will be left in the following paragraphs, and before we get there let’s first eliminate $\text{term}_2$ by proving it’s $0$. </p><script type="math/tex; mode=display">\begin{align}\text{term}_2&= \mathbb E_{(x,y), D} \left[ (h_D(x)-\bar{h}(x))( \bar{h}(x) -  y) \right] \\&= \mathbb E_{(x,y)} \left[ \mathbb E_{D}[(h_D(x)-\bar{h}(x))]( \bar{h}(x) -  y) \right] \\&= \mathbb E_{(x,y)} \left[ (\mathbb E_{D}[h_D(x)]-\bar{h}(x))( \bar{h}(x) -  y) \right] \\&= \mathbb E_{(x,y)} \left[ (\bar{h}(x)-\bar{h}(x))( \bar{h}(x) -  y) \right] \\&= 0\end{align}</script><p>Now let’s cope with $\text{term}_1$. Denote the expectation of the true value given certain $x$ as:</p><script type="math/tex; mode=display">\bar y(x)=\mathbb E_{y|x}[y(x)]</script><p>The same trick applied again, we rewrite $\text{term}_1$ as: </p><script type="math/tex; mode=display">\begin{align}\text{term}_1 &= \mathbb E_{x,y} \left[ \left(\bar{h}(x) -  y\right)^2 \right] \\&= \mathbb E_{x,y} \left[ \left(\bar{h}(x) - \bar y(x) + \bar y(x) - y\right)^2 \right] \\&= \underbrace{\mathbb E_{x} \left[  (\bar{h}(x)-\bar y(x))^2  \right]}_{\text{bias}^2} + \underbrace{\mathbb E_{x,y} \left[  (\bar y(x)-y)^2  \right]}_{\text{noise}} + \underbrace{\mathbb E_{x,y} \left[  (\bar{h}(x)-\bar y(x))(\bar y(x)-y)  \right]}_{\text{term}_3} \\\end{align}</script><p>Now we can prove that $\text{term}_3 = 0$:</p><script type="math/tex; mode=display">\begin{align}\text{term}_3 &= \mathbb E_{x,y} \left[  (\bar{h}(x)-\bar y(x))(\bar y(x)-y)  \right] \\&= \mathbb E_{x} \left[ \mathbb E_{y|x} \left[(\bar{h}(x)-\bar y(x))(\bar y(x)-y)  \right]\right] \\&= \mathbb E_{x} \left[(\bar{h}(x)-\bar y(x)) \mathbb E_{y|x} \left[(\bar y(x)-y)  \right]\right] \\&= \mathbb E_{x} \left[(\bar{h}(x)-\bar y(x))  (\bar y(x)- \mathbb E_{y|x} [y])  \right] \\&= \mathbb E_{x} \left[(\bar{h}(x)-\bar y(x))  (\bar y(x)- \bar y(x)) \right] \\&= 0\end{align}</script><p>Thus we have the final representation: </p><script type="math/tex; mode=display">\operatorname{EPE}(\mathcal A) = \underbrace{\mathbb E_{x,D} \left[ \left((h_D(x)-\bar{h}(x)\right)^2 \right]}_{\text{variance}} + \underbrace{\mathbb E_{x} \left[  (\bar{h}(x)-\bar y(x))^2  \right]}_{\text{bias}^2} + \underbrace{\mathbb E_{x,y} \left[  (\bar y(x)-y)^2  \right]}_{\text{noise}}</script><h1 id="Bias-Variance-Tradeoff-in-Regularization-Method"><a href="#Bias-Variance-Tradeoff-in-Regularization-Method" class="headerlink" title="Bias-Variance Tradeoff in Regularization Method"></a>Bias-Variance Tradeoff in Regularization Method</h1><p>Does it (decomposition) tell us that we cannot achieve low variance and low bias at the same time? Of course not! This target of making them both low is the whole point of the machine learning field. <strong>This decomposition helps us understand how regularization method works.</strong> </p><p>In WGAN, we learn about the K-lipschitz condition, which limits the changing speed of the the model. When using the regularization method (e.g., l1-norm and l2-norm), we are actually making the same limitation as the K-lipschitz condition (refer to the following figure). </p><p><img src="https://i.loli.net/2020/06/28/7xBty53Mq4FmlJU.png" alt="image.png"></p><p>Thus the regularization method, by adding an additional loss term into the original objective, makes the output more smooth, which is equivalent to “reducing the variance of the output”. Though reducing the variance is a desirable pursuit, it cannot be achieved without sacrifice, which, in this case, is the ramification of increasing the bias of the model. </p><p>The following figure shows the tradeoff of the bias and the variance with regularization term, in which $\lambda$ represents the weight of the regularization.</p><p> <img src="https://i.loli.net/2020/06/28/YFLRf3oOewaGmCX.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=KtNwjbWbnh8&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;youtube demo
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
  </entry>
  
  <entry>
    <title>Bagging</title>
    <link href="http://www.shihaizhou.com/2020/06/27/Bagging-and-Boosting/"/>
    <id>http://www.shihaizhou.com/2020/06/27/Bagging-and-Boosting/</id>
    <published>2020-06-27T07:18:04.000Z</published>
    <updated>2020-10-20T06:48:24.815Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h1><p>Bagging方法属于ensemble类的方法，表示 Bootstrap AGGregetING. Bagging方法由bootstrap和aggregation两部分组成. </p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><strong>Bootstrapping</strong>. Given a training set $D=\left\{\left(\mathbf{x}_{1}, y_{1}\right), \ldots\left(\mathbf{x}_{n}, y_{n}\right)\right\}$, we first sample (with replacement) $T$ sets of $n$ elements from $D$, forming $T$ quasi replica training sets $\{D_{1}, D_{2}, \ldots D_{T}\}$. Then we train a machine $f_i$ on each small dataset $D_i, \space i=1,\cdots, T$. Bootstrapping中最需要理解的关键点是 sample with replacement过程。需要注意的是每个dataset包含的数据量都是和原本的dataset相同的，得到的数据集是可以存在重复元素的数据集. 而并不是从大数据集中分割出小数据集。这么做的意义是为了保证每个子数据集中的元素都满足i.i.d条件，从而达到降低variance的效果。</p><p><strong>Aggregation</strong>. Aggregation阶段我们将在小数据集上训练的base learner $\{f_i\}_{i=1}^{T}$ 进行某种方式的聚合.<br>For regression: </p><script type="math/tex; mode=display">\bar{f}(\mathbf{x})=\frac{1}{T}\sum_{i=1}^{T} f_{i}(\mathbf{x})</script><p>For classification, we can do average and decide or the majority voting:</p><script type="math/tex; mode=display">\begin{align}\text{simple:}\quad \bar{f}(\mathbf{x})&=\operatorname{sign}\left(\sum_{i=1}^{T} f_{i}(\mathbf{x})\right) \\\text{majority voting:}\quad\bar{f}(\mathbf{x}) &= \operatorname{sign}\left(\sum_{i=1}^{T} \operatorname{sign}\left(f_{i}(\mathbf{x})\right)\right)\end{align}</script><h2 id="Theoretical-Guarantees"><a href="#Theoretical-Guarantees" class="headerlink" title="Theoretical Guarantees"></a>Theoretical Guarantees</h2><p>As for the question why it helps improve the performance compared with the learner which is trained on the whole dataset, we need to use the formal form of the <strong>EPE</strong> (recall <a href="http://www.shihaizhou.com/2020/06/28/Bias-Variance-Decomposition/">my blog of the bias-variance decomposition</a>). </p><script type="math/tex; mode=display">\operatorname{EPE}(\mathcal A) = \underbrace{\mathbb E_{x,D} \left[ \left((h_D(x)-\bar{h}(x)\right)^2 \right]}_{\text{variance}} + \underbrace{\mathbb E_{x} \left[  (\bar{h}(x)-\bar y(x))^2  \right]}_{\text{bias}^2} + \underbrace{\mathbb E_{x,y} \left[  (\bar y(x)-y)^2  \right]}_{\text{noise}}</script><p>If we are able to access the input distribution, we can sample infinite number of independent datasets of size $n$: $\mathcal D=\{D_i\}_{i=1}^\infty$ and average the learners to reduce the variance. Then the variance would be close to zero, simply because:</p><script type="math/tex; mode=display">h_{\mathcal D}(x)=\lim_{T\rightarrow\infty}\frac{1}{T}\sum_{i=1}^{T}h_{D_i}(x)=\mathbb E_{D\sim P^n}[h_D](x) = \bar h(x)</script><p>Unfortunately, we can’t get access to so large number of the datsets. But in Bagging algorithm, <strong>the method of sampling with replacement guarantees that each sampled dataset is a dataset consisting of i.i.d samples,</strong> which helps reduce the variance of the base learners. To note here, if we sample $m&lt;n$ data samples from the original dataset $D$, then there is no theoretical proof that the variance is reduced compared with the learner trained on the whole dataset: since the bias-variance decomposition is conditioned on the dataset size. </p><h2 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h2><p>Bagging的理论证明告诉我们它能够显著优化base learner的variance而对bias没有优化的效果. 那么什么样的base learner能够从bagging上benefit最大？答案是具有high variance and low bias的base learner: decision  tree. </p><p><img src="https://i.loli.net/2020/06/30/IRHvloA35q1BOUM.png" alt="image.png"></p><p>最常见的关于RF的错误认识是认为RF单一地采取了random feature selection. 但事实上RF作为bagging方法，构建i.i.d的sub-dataset依旧是不可缺少的. 所以RF是同时具备random samples和random feature selection这两个随机性的。在构建每一个单独的decision tree时，参与影响每一次decision boundary决策的是随机选择的$K$个feature. 而通过实验人们发现 $K=\sqrt {d}$ 往往是最优的选择. </p><p>关于Random Forest最美妙的性质在于：</p><ol><li>首先RF几乎不需要任何超参，唯一的超参$K$也在经验上有非常好的设置 $K=\sqrt {d}$. </li><li>其次RF不需要进行train-val的数据集划分：因为在进行dataset sample时sub-dataset一定会出现重复的样本，那么原数据集中没有被采样到的样本就可以用于这个base learner的validation，这叫做Out Of Bag (OOB) validation. </li><li>最后是RF的训练是增量式的，不存在sub-dataset number这样的超参，我们只需要在error曲线平滑时停止训练即可. </li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Bagging&quot;&gt;&lt;a href=&quot;#Bagging&quot; class=&quot;headerlink&quot; title=&quot;Bagging&quot;&gt;&lt;/a&gt;Bagging&lt;/h1&gt;&lt;p&gt;Bagging方法属于ensemble类的方法，表示 Bootstrap AGGregetING. 
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
  </entry>
  
  <entry>
    <title>Learning Representations by Maximizing Mutual Information Across Views</title>
    <link href="http://www.shihaizhou.com/2020/06/24/Learning-Representations-by-Maximizing-Mutual-Information-Across-Views/"/>
    <id>http://www.shihaizhou.com/2020/06/24/Learning-Representations-by-Maximizing-Mutual-Information-Across-Views/</id>
    <published>2020-06-24T06:13:07.000Z</published>
    <updated>2020-06-24T06:45:22.787Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/abs/1906.00910" target="_blank" rel="noopener">paper “Learning Representations by Maximizing Mutual Information Across Views”</a></li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.00910&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;paper “Learning Represe
      
    
    </summary>
    
    
      <category term="MI" scheme="http://www.shihaizhou.com/tags/MI/"/>
    
  </entry>
  
  <entry>
    <title>Momentum Contrast for Unsupervised Visual Representation Learning</title>
    <link href="http://www.shihaizhou.com/2020/06/23/Momentum-Contrast-for-Unsupervised-Visual-Representation-Learning/"/>
    <id>http://www.shihaizhou.com/2020/06/23/Momentum-Contrast-for-Unsupervised-Visual-Representation-Learning/</id>
    <published>2020-06-23T07:55:16.000Z</published>
    <updated>2020-10-20T03:14:56.213Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/abs/1911.05722" target="_blank" rel="noopener">MoCo paper</a> </li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. We present Momentum Contrast (MoCo) as a way of building large and consistent dictionaries for unsupervised learning with a contrastive loss.<br>We maintain the dictionary as a queue of data samples: the encoded repre- sentations of the current mini-batch are enqueued, and the oldest are dequeued. The queue decouples the dictionary size from the mini-batch size, allowing it to be large.<br>More- over, as the dictionary keys come from the preceding sev- eral mini-batches, a slowly progressing key encoder, imple- mented as a momentum-based moving average of the query encoder, is proposed to maintain consistency.</p><h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><h2 id="Contrastive-Learning-as-Dictionary-Look-up"><a href="#Contrastive-Learning-as-Dictionary-Look-up" class="headerlink" title="Contrastive Learning as Dictionary Look-up"></a>Contrastive Learning as Dictionary Look-up</h2><p>Consider an encoded query $q$ and a set of encoded samples $\{k_0, k_1, k_2, \cdots \}$ that are the keys of a dictionary. 假定只有一个和 query $q$ 匹配的 key $k_+$, 那么contrastive loss InfoNCE 为以下形式：</p><script type="math/tex; mode=display">\mathcal{L}_{q}=-\log \frac{\exp \left(q \cdot k_{+} / \tau\right)}{\sum_{i=0}^{K} \exp \left(q \cdot k_{i} / \tau\right)}</script><p>其中 $\tau$ 为temperature hyper-parameter. 直觉上来看我们可以将以上loss看做是 (K+1)-way softmax-based classifier that tries to classify $q$ as $k_+$. 其中 $q=f_q(x^q)$ 和 $k=f_k(x^k)$ 分别由encoder $f_q$ 和 $f_k$ 编码得到，两者的encoder可以采取 identical/parameter-sharing/different 这三种模式，通常在contrastive learning的语境下会采取最前者。</p><h2 id="Momentum-Contrast"><a href="#Momentum-Contrast" class="headerlink" title="Momentum Contrast"></a>Momentum Contrast</h2><p>从这个角度来说，CL可以被看做为每一个input value $x$ 建立一个key，从而我们有字典 $\{\operatorname{key:} f_k(x);\operatorname{val:}x\}$. MoCo的关键假设是：</p><blockquote><p>Our hypothesis is that good features can be learned by a large dictionary that covers a rich set of negative samples, while the encoder for the dictionary keys is kept as consistent as possible despite its evolution. </p></blockquote><p>MoCo 将 dictionary 看作 “a queue of data samples”. 这样可以将dictionary size设置得比 mini-batch size要大得多从而非常显著地提升负样本的数量：这是因为mini-batch要求内存能够容纳 $\operatorname{mini-batch}\times\operatorname{image-size}$, 而对于queue来说内存需要容纳 $\operatorname{queue-size}\times\operatorname{key-size}$. 非常显然的key的大小远远小于input image. queue 更新的规则是最近的mini-batch of samples替换最早的mini-batch. </p><p>将 $f_q$ 直接作为 $f_k$ 会导致表现变得很差。We hypothesize that such failure is caused by the rapidly changing encoder that reduces the key representations’ consistency. We propose a momentum update to address this issue. 以下为momentum update的具体形式：</p><script type="math/tex; mode=display">\theta_{\mathrm{k}} \leftarrow m \theta_{\mathrm{k}}+(1-m) \theta_{\mathrm{q}}</script><p>其中通过BP学习的参数只有 $\theta_q$. In experiments, a relatively large momentum (e.g., m = 0.999, our default) works much better than a smaller value (e.g., m = 0.9), suggesting that a slowly evolving key encoder is a core to making use of a queue. </p><p>下图表示了mini-batch/memory bank/MoCo架构的区别.</p><p><img src="https://i.loli.net/2020/06/23/g5ulQIAZscDLBaP.png" alt="image.png"></p><p>下面的类torch伪代码则展示了整个MoCo的训练过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># f_q, f_k: encoder networks for query and key # queue: dictionary as a queue of K keys (CxK) # m: momentum</span></span><br><span class="line"><span class="comment"># t: temperature</span></span><br><span class="line"></span><br><span class="line">f_k.params = f_q.params <span class="comment"># initialize</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> loader: <span class="comment"># load a minibatch x with N samples</span></span><br><span class="line">    x_q = aug(x) <span class="comment"># a randomly augmented version</span></span><br><span class="line">    x_k = aug(x) <span class="comment"># another randomly augmented version</span></span><br><span class="line">    q = f_q.forward(x_q) <span class="comment"># queries: NxC </span></span><br><span class="line">    k = f_k.forward(x_k) <span class="comment"># keys: NxC</span></span><br><span class="line">    k = k.detach() <span class="comment"># no gradient to keys</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># positive logits: Nx1</span></span><br><span class="line">    l_pos = bmm(q.view(N,<span class="number">1</span>,C), k.view(N,C,<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># negative logits: NxK</span></span><br><span class="line">    l_neg = mm(q.view(N,C), queue.view(C,K))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># logits: Nx(1+K)</span></span><br><span class="line">    logits = cat([l_pos, l_neg], dim=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># contrastive loss, Eqn.(1)</span></span><br><span class="line">    labels = zeros(N) <span class="comment"># positives are the 0-th </span></span><br><span class="line">    loss = CrossEntropyLoss(logits/t, labels)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># SGD update: query network</span></span><br><span class="line">    loss.backward() update(f_q.params)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># momentum update: key network</span></span><br><span class="line">    f_k.params = m*f_k.params+(<span class="number">1</span>-m)*f_q.params</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># update dictionary</span></span><br><span class="line">    enqueue(queue, k) <span class="comment"># enqueue the current minibatch</span></span><br><span class="line">    dequeue(queue) <span class="comment"># dequeue the earliest minibatch</span></span><br></pre></td></tr></table></figure><p>为了更好地理解MoCo的更新过程，绘制MoCo示意图如下：</p><p><img src="https://i.loli.net/2020/06/23/gtwiHGlX5Pc9ZOS.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1911.05722&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;MoCo paper&lt;/a&gt; &lt;/li&gt;
&lt;/
      
    
    </summary>
    
    
      <category term="Contrastive" scheme="http://www.shihaizhou.com/tags/Contrastive/"/>
    
  </entry>
  
  <entry>
    <title>Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning</title>
    <link href="http://www.shihaizhou.com/2020/06/22/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning/"/>
    <id>http://www.shihaizhou.com/2020/06/22/Bootstrap-Your-Own-Latent-A-New-Approach-to-Self-Supervised-Learning/</id>
    <published>2020-06-22T12:00:30.000Z</published>
    <updated>2020-06-23T07:44:58.286Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/abs/2002.05709" target="_blank" rel="noopener">SimCLR paper</a></li><li><a href="https://arxiv.org/abs/2006.07733" target="_blank" rel="noopener">BYOL paper</a></li><li><a href="https://www.investopedia.com/terms/e/ema.asp" target="_blank" rel="noopener">Exponential Moving Average</a></li><li><a href="https://www.zhihu.com/question/402452508" target="_blank" rel="noopener">zhihu question about BYOL</a></li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>在self-supervised learning方法中非常流行的contrastive learning大量依赖负样本的采样方法和batch size. 本工作的动机基于一个随机的发现：首先随机初始化了一个网络，这个网络得到的representation (on top of which is a mere linear classifier) 在ImageNet上仅仅取得了$1.4\%$ 的top-1 accuracy. 然后将这个网络的representation作为target network，接着训练一个online network去拟合这个target network, 得到的online network可以在ImageNet上得到$18.4\%$ top-1 accuracy. 这说明了self-bootstraping可能是一个很好的优化目标：将新得到的online network作为新的target network, 然后接着初始化一个新的online network去拟合target network. 重复这个过程就有可能得到质量不断提升的representation. 在实践中，BYOL采用的是”<a href="https://www.investopedia.com/terms/e/ema.asp" target="_blank" rel="noopener">Exponential Moving Average (EMA)</a> of the online network”. </p><h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><p><img src="https://i.loli.net/2020/06/23/Bquselgn31dw97W.png" alt="image.png"></p><p>BYOL的目标是学习到$f_\theta$将原有的input映射到representation space.<br>Online network 由参数 $\theta$ model的，共有三个阶段：首先从原有的input image选取一个view (由具体的data augmentation方案决定)；接着分别通过encoder $f_\theta$, projector $g_\theta$ and a predictor $q_\theta$.<br>Target network 的架构和online network相同，参数 $\xi$ 是exponential moving average of the online parameters: given a <strong>target decay rate</strong> $\tau \in[0,1]$, we update $\xi$ as follows</p><script type="math/tex; mode=display">\xi \leftarrow \tau \xi+(1-\tau) \theta</script><p>最终的loss就是一个regression task, 其中 $\operatorname {sg}(z^\prime)$ 意思是stop-gradient, 以下等式的中间项表示在projector空间经过normalize之后的regression loss. （作者表示虽然可以直接在representation空间进行预测，但是前人工作<a href="https://arxiv.org/abs/2002.05709" target="_blank" rel="noopener">SimCLR</a>已经证明了使用projection会有更好的表现）</p><script type="math/tex; mode=display">\mathcal{L}_{\theta}^{\mathrm{BYOL}} \triangleq\left\|\overline{q_{\theta}}\left(z_{\theta}\right)-\bar{z}_{\xi}^{\prime}\right\|_{2}^{2}=2-2 \cdot \frac{\left\langle q_{\theta}\left(z_{\theta}\right), z_{\xi}^{\prime}\right\rangle}{\left\|q_{\theta}\left(z_{\theta}\right)\right\|_{2} \cdot\left\|z_{\xi}^{\prime}\right\|_{2}}</script><p>同时为了对称化以上的loss term，作者将两个view $v, v^\prime$ 互换位置代入 online network 和 target network得到 $\widetilde{\mathcal{L}}_{\theta}^{\mathrm{BYOL}}$ . 每一个step仅仅只用 $\mathcal{L}_{\theta}^{\mathrm{BYOL}}+\widetilde{\mathcal{L}}_{\theta}^{\text {ВУОС }}$ update online network的参数. </p><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><p><img src="https://i.loli.net/2020/06/23/Ryu3aA15ngPksvZ.png" alt="image.png"></p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>评价representation的quality可以在两个方面进行：首先将representation空间fix住然后将其输入一个linear classifier. 其次可以将representation的encoder作为网络的初始化参数，在部分的training set上做fine-tuning，可以被归类到semi-supervised learning上. 另外本文还测试了将模型向不同的数据集进行迁移得到的不同效果. </p><p><img src="https://i.loli.net/2020/06/23/X5gu6SZvbanL7il.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/06/23/7abokeTMdXWtLDs.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/06/23/QBDx2IgnwsA1VSr.png" alt="image.png"></p><h2 id="Ablations"><a href="#Ablations" class="headerlink" title="Ablations"></a>Ablations</h2><p>More robust for smaller batch size and less image augmentation (compared to CL).<br><img src="https://i.loli.net/2020/06/23/r9QbpFZTJjhSkzV.png" alt="image.png"></p><p>该方法对于target network的保留权重 $\tau$ 较为敏感.<br><img src="https://i.loli.net/2020/06/23/f8c9Kb6hWzXneqE.png" alt="image.png"></p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Nevertheless, BYOL remains dependent on existing sets of augmentations that are specific to vision applications. To generalize BYOL to other modalities (e.g., audio, video, text, . . . ) it is necessary to obtain similarly suitable augmentations for each of them. Designing such augmentations may require significant effort and expertise. Therefore, automating the search for these augmentations would be an important next step to generalize BYOL to other modalities.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;SimCLR paper&lt;/a&gt;&lt;/li&gt;
&lt;
      
    
    </summary>
    
    
      <category term="Representation Learning" scheme="http://www.shihaizhou.com/tags/Representation-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Lipschitz Continuity</title>
    <link href="http://www.shihaizhou.com/2020/06/20/Lipschitz-Continuity/"/>
    <id>http://www.shihaizhou.com/2020/06/20/Lipschitz-Continuity/</id>
    <published>2020-06-20T08:38:57.000Z</published>
    <updated>2020-06-22T12:04:34.722Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://zhuanlan.zhihu.com/p/27554191" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/27554191</a></li><li><a href="https://en.wikipedia.org/wiki/Lipschitz_continuity" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Lipschitz_continuity</a></li></ul><h1 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h1><p>Intuitively, a Lipschitz continuous function is limited in how fast it can change: there exists a real number such that, for every pair of points on the graph of this function, the absolute value of the slope of the line connecting them is not greater than this real number. </p><p><strong>Definition</strong> (Lipschitz Continuity). Given two metric spaces $(X, d_X)$ and $(Y, d_Y)$, where $d_X$ denotes the metric on the set $X$ and $d_Y$ is the metric on set $Y$, a function $f : X \mapsto Y$ is called <strong>Lipschitz continuous</strong> if there exists a real constant $K \geq 0$ such that, for all $x_1$ and $x_2$ in $X$, </p><script type="math/tex; mode=display">d_{Y}\left(f\left(x_{1}\right), f\left(x_{2}\right)\right) \leq K d_{X}\left(x_{1}, x_{2}\right)</script><p>If $K=1$, it’s called a <strong>short map</strong>.<br>If $0 \leq K &lt; 1$ and $f$ maps a metric space to itself, the function is called a <strong>contraction</strong>.<br>In particular, a real-valued function $f : R \mapsto R$ is called Lipschitz continuous:</p><script type="math/tex; mode=display">\left|f\left(x_{1}\right)-f\left(x_{2}\right)\right| \leq K\left|x_{1}-x_{2}\right|</script><p><strong>Definition</strong> (Lipschitz Continuous Gradient). A real-valued function $f$ is <strong>Lipschitz continuous gradient</strong> if: </p><script type="math/tex; mode=display">\left\|f^{\prime}(x)-f^{\prime}(y)\right\| \leqslant K\|x-y\|</script><p><strong>Definition</strong> (Lipschitz Continuous Hessian). A real-valued function $f$ is <strong>Lipschitz continuous hessian</strong> if: </p><script type="math/tex; mode=display">\left\|f^{\prime \prime}(x)-f^{\prime \prime}(y)\right\| \leqslant K\|x-y\|</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/27554191&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://zhuanlan.z
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
  </entry>
  
  <entry>
    <title>Legendre Transform and Fenchel Conjugate</title>
    <link href="http://www.shihaizhou.com/2020/06/17/Legendre-Transform-and-Fenchel-Conjugate/"/>
    <id>http://www.shihaizhou.com/2020/06/17/Legendre-Transform-and-Fenchel-Conjugate/</id>
    <published>2020-06-17T05:57:34.000Z</published>
    <updated>2020-06-17T08:49:25.362Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://www.andrew.cmu.edu/course/33-765/pdf/Legendre.pdf" target="_blank" rel="noopener">CMU tutorial on Legendre transform</a></li><li><a href="https://www.youtube.com/watch?v=vgLq90cOI_M" target="_blank" rel="noopener">Youtube short explanation on Legendre transform</a></li><li><a href="http://www.onmyphd.com/?p=legendre.fenchel.transform" target="_blank" rel="noopener">http://www.onmyphd.com/?p=legendre.fenchel.transform</a></li></ul><h1 id="Legendre-Transform"><a href="#Legendre-Transform" class="headerlink" title="Legendre Transform"></a>Legendre Transform</h1><h2 id="Information-in-Functions"><a href="#Information-in-Functions" class="headerlink" title="Information in Functions"></a>Information in Functions</h2><p>函数的本质是映射，映射包含了信息，那么如何衡量一个函数包含的信息量？对于离散函数，一种想法是在值域上计算信息熵通过衡量值域上分布的混乱度来进行估算，但这并没有包含关于这个函数的所有信息：定义域信息、N-阶导函数信息都被丢失了。同时下面的一个简单函数我们就已经很难找到一种quantify它所包含的信息：</p><script type="math/tex; mode=display">f:\left\{\begin{array}{l}\mathbb{R} \rightarrow \mathbb{R}_{0}^{+} \\x \mapsto x^{2}\end{array}\right.</script><p>计算函数包含的绝对信息也许不可行，但是受到可数集上定义”size”方法的启发，我们或许可以找到一种定义两个函数包含信息量的相对关系。例如函数$y=f(x)$和它的inverse function $x=f^{-1}(y)$ 就应该包含有等量的 information，原因是它们在被表示成集合上的映射图时是完全一样的。</p><p>或许我们可以将一个函数transform成另一个函数而保持他们的信息不变，Legendre Transform就是在这个语境下的一种function transform方法。</p><h2 id="Aim"><a href="#Aim" class="headerlink" title="Aim"></a>Aim</h2><p>现有一个函数 $\mathrm{y}: x \mapsto \mathrm{y}(x)$, 它包含了很多信息，例如对于每一个给定的 $x$ 对应的函数值 $y$. 同时它也包含了<strong>slope</strong> at any given $x$. Legendre Transform的目标是将原函数转换为一个关于slope $p$ 的函数，同时保证这两个函数是具有同样的信息的。定义：</p><script type="math/tex; mode=display">p:=\mathrm{y}^{\prime}(x)</script><p><strong>Failed attempt.</strong> 一个非常直观的想法就是我们将 $x$ 用 $p$ 来表示，即 $x = \mathrm y ^{\prime -1}(p)$, 然后反向代回原来的函数 $\mathrm y$. 由此我们得到以下的transformed $\tilde{\mathrm y}$: </p><script type="math/tex; mode=display">\mathrm{y}(x) \rightarrow \tilde{\mathrm{y}}(p)=\mathrm{y}(x(p))=\mathrm{y}\left(\mathrm{y}^{\prime-1}(p)\right)</script><p>那么我们需要验证这个transform导致了一部分信息的丢失，最好的方法是通过counter example，对于函数：</p><script type="math/tex; mode=display">\mathrm{y}: x \mapsto \frac{1}{2}\left(x-x_{0}\right)^{2}</script><p>根据上式我们计算出：</p><script type="math/tex; mode=display">\tilde{\mathrm{y}}(p) = \mathrm{y}(x(p))=\frac{1}{2}\left(x(p)-x_{0}\right)^{2}=\frac{1}{2} p^{2}</script><p>我们发现最终的表达式里关于 $x_0$ 的信息丢失了：对于由不同 $x_0$ 构成的函数集合，它们都transform到了同一个函数上。但是我们只需要</p><h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><p><strong>Definition</strong> (Legendre Transformation). <strong>Legendre Transformation</strong> from a function $\mathrm y(x)$ to a new function $\mathrm y^\star (p)$ is defined as follow, where $p=\mathrm y^\prime (x)$ and no information is lost iff function $\mathrm y$ is <strong>convex</strong> (or concave, which is omitted in this blog):</p><script type="math/tex; mode=display">\mathrm y^\star (p) := \sup_x\{xp - \mathrm y(x)\}</script><p>为什么函数需要是convex: 因为只有convex的函数才能够建立 $x$ 和 $p$ 的一一映射，这也是Legendre Transform的一大限制。同时需要注意的是 $p=\mathrm y^\prime (x)$ 不是先于这个定义的条件，而是通过解决这个关于 $x$ 的最大化问题得到的结果：关于 $x$ 求导并使其导数为0，得到：</p><script type="math/tex; mode=display">0=\frac{\partial}{\partial x}\{x p-\mathrm y(x)\}=p-\mathrm y^{\prime}(x)</script><p>所以以上定义式等价于以下去掉 $\sup$ 符号的式子（在网上更多见的版本）：</p><script type="math/tex; mode=display">\mathrm y^\star (p) := xp - \mathrm y(x), \quad \operatorname{where} \space p=\mathrm y^\prime (x)</script><h2 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h2><h3 id="Geometric-interpretation"><a href="#Geometric-interpretation" class="headerlink" title="Geometric interpretation"></a>Geometric interpretation</h3><p>在空间上我们可以认为Legendre Transformation建立了某点 $x$ 对应斜率 $p$ 和该点tangent的截距之间的函数关系。具体证明非常简单，作图即可。以下是帮助理解的图例：</p><p><img src="https://i.loli.net/2020/06/17/A68WCO34VatnkGF.png" alt="image.png"></p><h3 id="Inverse-of-derivatives"><a href="#Inverse-of-derivatives" class="headerlink" title="Inverse of derivatives"></a>Inverse of derivatives</h3><p>经过transform之后的函数 $\mathrm y^\star(p)$ 关于其自变量的导数和原函数有以下关系：</p><script type="math/tex; mode=display">\mathrm{y}^{\star \prime} = \frac{\partial \mathrm{y}^{\star}(p)}{\partial p}=\underbrace{\frac{\partial \mathrm{y}(x)}{\partial x}}_{p} \frac{\partial x}{\partial p}-x(p)-\frac{\partial x}{\partial p} p=-x(p) =-\mathrm{y}^{\prime-1}</script><h3 id="Inverse-of-Legendre-Transformation"><a href="#Inverse-of-Legendre-Transformation" class="headerlink" title="Inverse of Legendre Transformation"></a>Inverse of Legendre Transformation</h3><p>The Legendre Transformation of the Legendre Transformation of a function $\mathrm y$ is $\mathrm y$ itself, which is easy to prove. </p><script type="math/tex; mode=display">^{\star \star} \mathrm{y}=\mathrm{y}</script><p>同时因为这条式子，Legendre Transformation是一个不会导致函数信息损失的transformation. </p><h3 id="Finding-the-min-value"><a href="#Finding-the-min-value" class="headerlink" title="Finding the min value"></a>Finding the min value</h3><p>在一些情况下我们不方便直接去求原函数的最小值 $\mathrm y(x)_{min}$，但我们知道 $\mathrm y(x)$ 在取到极小值的时候一阶导数 $p=\mathrm y^\prime (x)=0$. 从而我们在知道了Legendre Transformation的情况下可以直接求极小值：</p><script type="math/tex; mode=display">\begin{align}&\mathrm y^\star(0)=0x - \mathrm y(x)_{\min} \\\Rightarrow \space &\mathrm y(x)_{\min} = -\mathrm y^\star(0)\end{align}</script><h1 id="Fenchel-Conjugate"><a href="#Fenchel-Conjugate" class="headerlink" title="Fenchel Conjugate"></a>Fenchel Conjugate</h1><p>寻找一个非凸函数的凸共轭的目标是：对于在（某个定义域内）任意斜率的直线，我们要找出这个使得该直线最靠近该非凸函数的截距值。如下图，对于给定斜率为 $s$ 的直线，我们想要找到截距 $b$ 使直线尽量靠近函数。最简单的想法是对所有 $x$ 求导，在导数等于零的点中找到一个最接近的直线。下图中，非常明显 $b_0$ 优于 $b_1$. </p><p><img src="https://i.loli.net/2020/06/17/sBnIEid1bKouwDp.png" alt="image.png"></p><p>This more general rule applies to non-differentiable or non-convex functions. So, when a line with slope $\mathbf s$ crosses $f(\mathbf x)$, we have:</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{s}^{T} \mathbf{x}+b &=f(\mathbf{x}) \\b &=f(\mathbf{x})-\mathbf{s}^{T} \mathbf{x}\end{aligned}</script><p>and we want the smallest value of $b$ for all $x$. Then:</p><script type="math/tex; mode=display">\begin{aligned}b &=\inf _{\mathbf{x}}\left(f(\mathbf{x})-\mathbf{s}^{T} \mathbf{x}\right) \\&=\inf _{\mathbf{x}}\left(-\left(\mathbf{s}^{T} \mathbf{x}-f(\mathbf{x})\right)\right) \\&=-\sup _{\mathbf{x}}\left(\mathbf{s}^{T} \mathbf{x}-f(\mathbf{x})\right)\end{aligned}</script><p>所以从这里来看我们上面定义的Legendre Transform实际上是希望能够建立一个从slope到截距的映射，而这里我们的Fenchel Conjugate也是为了建立这样的映射：$f^{\star}(\mathbf{s})=-b$. 故而我们得到了Fenchel Conjugate的表示：</p><script type="math/tex; mode=display">f^{\star}(\mathbf{s})=-b=\sup _{\mathbf{x}}\left(\mathbf{s}^{T} \mathbf{x}-f(\mathbf{x})\right)</script><p>This is the <strong>Legendre-Fenchel transform</strong>, also known as <strong>convex conjugate</strong>. Note that now the transform is <strong>not reversible</strong>, i.e., you cannot get the original function by applying the transform to the transform. On the other hand, <strong>the transform of the transform is convex</strong>, even if the original function is not.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.andrew.cmu.edu/course/33-765/pdf/Legendre.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
  </entry>
  
  <entry>
    <title>Generative Adversarial Networks</title>
    <link href="http://www.shihaizhou.com/2020/06/16/Generative-Adversarial-Networks/"/>
    <id>http://www.shihaizhou.com/2020/06/16/Generative-Adversarial-Networks/</id>
    <published>2020-06-16T11:52:31.000Z</published>
    <updated>2020-08-01T16:01:30.726Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://math.stackexchange.com/questions/2435464/show-that-max-function-on-mathbb-rn-is-convex" target="_blank" rel="noopener">Prooving max function is convex.</a></li><li><a href="https://vincentherrmann.github.io/blog/wasserstein/" target="_blank" rel="noopener">blog on WGAN</a></li></ul><h1 id="Vanilla-GAN"><a href="#Vanilla-GAN" class="headerlink" title="Vanilla GAN"></a>Vanilla GAN</h1><p>Deep Learning研究的问题本质上是希望学习从一个数据空间到另一个数据空间的映射；更准确地，一个distribution向另一个distribution的映射。在分类任务中，这两个分布分别为合法数据点的分布向0-1分布的映射；在回归任务中，这两个分布分别为特征空间内数据点的分布向预测空间内的分布。</p><p>在GAN的语境下，我们希望能够找到一个generator $G$ 将任意一个随机分布（在实践中通常是 Normal Distribution 或 Uniform Distribution）映射到生成网络分布 $P_G$ 中，并且我们希望 $P_G$ 和 $P_{data}$ 的分布能够尽量接近，divergence被用来度量两个分布的相似度，常见的为KL divergence和JS divergence. 以下为优化目标。</p><script type="math/tex; mode=display">G^* = \arg\min_G \operatorname{Div}(P_G, P_{data})</script><p>GAN的核心思想在于将寻找最优generator $G$ 的过程刻画成一个由generator和discriminator两者构成的minmax game：</p><script type="math/tex; mode=display">G^* = \arg \min_G \max_DV(D, G)</script><p>where</p><script type="math/tex; mode=display">V(D,G)=\mathbb E_{x\sim P_{data}}[\log D(x)] + \mathbb E_{x\sim P_{G}}[\log (1-D(x=G(z))]</script><h2 id="Relation-to-JS-Divergence"><a href="#Relation-to-JS-Divergence" class="headerlink" title="Relation to JS Divergence"></a>Relation to JS Divergence</h2><p>下面我们证明以上的优化函数在discriminator取最优的情况下等价于 $P_G$ 和 $P_{data}$ 的 JS divergence $\operatorname{JS}(P_G | P_{data})$. </p><p>首先我们将上式</p><script type="math/tex; mode=display">\begin{align}V(D,G) &=\mathbb E_{x\sim P_{data}}[\log D(x)] + \mathbb E_{x\sim P_{G}}[\log (1-D(x=G(z))] \\&= \int P_{data}(x)\log D(x) dx + \int P_{G}(x)\log (1-D(x=G(z))) dx \\&= \int [P_{data}(x)\log D(x) +  P_{G}(x)\log (1-D(x=G(z)))] dx \end{align}</script><p>从优化discriminator function $D$ 的角度，我们希望通过构造 $D$ 使得上式对于给定的generator $G$ 能够被最大化。最优的 $D(x)$ 在任意一个 $x$ 的取值下都是最优的：</p><script type="math/tex; mode=display">D^* = [D(x)]^* \quad \forall x \in \operatorname{dom}_V</script><p>对于给定的输入 $x$，对应该输入的最优解 $ [D(x)]^*$ 为：</p><script type="math/tex; mode=display">\begin{align}[D(x)]^* =\arg \max_{D(x)} [P_{data}(x)\log D(x) +  P_{G}(x)\log (1-D(x))] \end{align}</script><p>对该式两边关于$D(x)$求偏导并令其为0，得到：</p><script type="math/tex; mode=display">\frac{\partial }{\partial D(x)}[P_{data}(x)\log D(x) +  P_{G}(x)\log (1-D(x))]= \frac{P_{data}}{D(x)}-\frac{P_{G}}{1-D(x)}=0</script><p>由此解得，</p><script type="math/tex; mode=display">[D(x)]^* = \frac{P_{data}(x)}{P_{data}(x)+P_G(x)}</script><p>将 $D^*$ 带入 $V(D,G)$ 则有以下推论：</p><script type="math/tex; mode=display">\begin{align}V(D^*,G) &= E_{x\sim P_{data}}\left[\log \frac{P_{data}(x)}{P_{data}(x) + P_G(x)}\right] + E_{x\sim P_{G}}\left[\log \frac{P_{G}(x)}{P_{data}(x) + P_G(x)}\right] \\& \operatorname{let:}\quad {P_M=\frac{(P_{data}+P_G)}{2}} \\&=E_{x\sim P_{data}}\left[\log \frac{P_{data}(x)/2}{P_M(x)}\right] + E_{x\sim P_{G}}\left[\log \frac{P_{G}(x)/2}{P_M(x)}\right] \\&= -2\log 2 + \operatorname{KL}(P_{data}\|P_M) + KL(P_G\|P_M)   \\&=-2\log 2 + 2 \operatorname{JS}(P_G\|P_{data})\end{align}</script><p>因此优化GAN的训练目标近似等价于优化两个distribution之间的JS divergence. 从另外一个角度来看，任意的discriminator $D$ 函数是 $\operatorname{JS}(P_{data} | P_G)$ 的一个下界，优化两者之间的divergence的逻辑就是先找到这个divergence的一个下界，然后最大化这个下界，然后以这个下界作为优化目标的estimate，再优化目标。</p><h1 id="f-GAN"><a href="#f-GAN" class="headerlink" title="f-GAN"></a>f-GAN</h1><p>Vanilla GAN的优化目标是JS divergence $\operatorname{JS}(P_{data} | P_G)$, 而f-GAN这篇工作则把满足一定特性的不同divergence都归在了同一个f-divergence的框架下，并且提出了对应在minmax game中优化函数的表达式。</p><h2 id="The-f-divergence-Family"><a href="#The-f-divergence-Family" class="headerlink" title="The f-divergence Family"></a>The f-divergence Family</h2><p>A large class of different divergences are the so called <strong>f-divergences</strong>, also known as the Ali-Silvey distances. Given two distributions $P$ and $Q$ that possess, respectively, an absolutely continuous density function $p$ and $q$ with respect to a base measure $dx$ defined on the domain $\mathcal{X}$ , we define the f-divergence:</p><script type="math/tex; mode=display">D_{f}(P \| Q)=\int_{\mathcal{X}} q(x) f\left(\frac{p(x)}{q(x)}\right) \mathrm{d} x</script><p>where the generator function $f: \mathbb R_+ \rightarrow \mathbb R$ is a <strong>convex</strong>, lower-semicontinuous function satisfying $f (1) = 0<br>$.</p><h2 id="Estimating-f-divergence"><a href="#Estimating-f-divergence" class="headerlink" title="Estimating f-divergence"></a>Estimating f-divergence</h2><p>在f-GAN原文中，作者称利用了convex conjugate来对下界进行estimate. 但是由于在上一章节作者将f-divergence中的函数限定在了convex function集合上，所以使用的Fenchel conjugate也就退化成了Legendre Transformation. 这一退化对于下面estimate的推导非常关键. </p><p>首先我们写出一个任意一个函数的Fenchel conjugate</p><script type="math/tex; mode=display">f^{*}(t)=\sup _{u \in \operatorname{dom}_{f}}\{u t-f(u)\}</script><p>Since $f$ is convex, we have</p><script type="math/tex; mode=display">f(u)=f^{*}(f^*(u))=\sup _{t \in \operatorname{dom}_{f^*}}\{u t-f^*(t)\}</script><p>将上式带入 f-divergence, we have</p><script type="math/tex; mode=display">D_{f}(P \| Q)=\int_{\mathcal{X}} q(x) \sup _{t \in \operatorname{dom}_{f^{*}}}\left\{t \frac{p(x)}{q(x)}-f^{*}(t)\right\} \mathrm{d} x</script><p><strong>Definition</strong> (Jensen Inequality). If $X$ is a random variable and $\varphi$ is a convex function, then following inequality holds. </p><script type="math/tex; mode=display">\varphi(\mathbb{E}[X]) \leq \mathbb{E}[\varphi(X)]</script><p>同时因为 $\sup$ 和 $\max$ 一样都是convex function. 所以利用Jensen Inequality，得到</p><script type="math/tex; mode=display">\begin{aligned}D_{f}(P \| Q) &=\int_{\mathcal{X}} q(x) \sup _{t \in \operatorname{dom}_{f^{*}}}\left\{t \frac{p(x)}{q(x)}-f^{*}(t)\right\} \mathrm{d} x \\& \geq \sup _{T \in \mathcal{T}}\left(\int_{\mathcal{X}} p(x) T(x) \mathrm{d} x-\int_{\mathcal{X}} q(x) f^{*}(T(x)) \mathrm{d} x\right) \\&=\sup _{T \in \mathcal{T}}\left(\mathbb{E}_{x \sim P}[T(x)]-\mathbb{E}_{x \sim Q}\left[f^{*}(T(x))\right]\right)\end{aligned}</script><p>where $t=T(x)​$, $T: \mathcal{X} \rightarrow \mathbb{R}​$. 接下来我们推导原文中略过的部分：在Legendre Transformation的语境下我们可以求出相对紧的下界 $T^*​$ (注意不要和conjugate的符号混淆). </p><p>首先因为Legendre Transformatio中 $f$ is convex, 所以我们可以求出 $f^*(t)=\sup_x\{xt-f(x)\}$ 的唯一解：</p><script type="math/tex; mode=display">\begin{align}\max_t \space &xt-f(x)\\\Rightarrow \space & 0= \frac{\partial}{\partial x}\left[xt-f(x)\right] = t-f^\prime(x) \\\Rightarrow \space & t = f^\prime(x)\\\Leftrightarrow \space & x = (f^{\prime})^{-1}(t)\end{align}</script><p>将最后的结果代回 $f^*(t)$ 得到</p><script type="math/tex; mode=display">f^*(x) =x f^{\prime-1}(x)-f(f^{\prime-1}(x))</script><p>对于 $f^*(x)$ 求关于自变量 $x$ 的一阶导数得到</p><script type="math/tex; mode=display">\begin{align}\frac{\partial f^*(x)}{\partial x} &= f^{\prime-1}(x) + x \frac{\partial f^{\prime-1} (x)}{\partial x} - \frac{\partial f(f^{\prime-1} (x))}{\partial f^{\prime-1} (x)} \frac{\partial f^{\prime-1} (x)}{\partial x} \\&= f^{\prime-1}(x) + x \frac{\partial f^{\prime-1} (x)}{\partial x} - f^{\prime}(f^{\prime-1} (x))\frac{\partial f^{\prime-1} (x)}{\partial x} \\&= f^{\prime-1}(x)\end{align}</script><p>利用以上结果，我们希望去找到对应的 $t=T(x)$ 表达式使得 $\sup$ 能够被最大化，从而有</p><script type="math/tex; mode=display">\begin{align}&\frac{\partial}{\partial t}\left[t \frac{p(x)}{q(x)}-f^{*}(t)\right] = 0 \\\Leftrightarrow \quad & f^{\prime -1}(t) = \frac{p(x)}{q(x)} \\\Rightarrow \quad & T^*(x) = t^* = f^{-1}\left(\frac{p(x)}{q(x)}\right)\end{align}</script><p>以下展示了不同的f-divergence和对应的最优estimate $T^*$ </p><p><img src="https://i.loli.net/2020/06/17/oOAszpE5t8W6rqj.png" alt="image.png"></p><h2 id="Variational-Divergence-Minimization-VDM"><a href="#Variational-Divergence-Minimization-VDM" class="headerlink" title="Variational Divergence Minimization (VDM)"></a>Variational Divergence Minimization (VDM)</h2><p>To this end, we follow the generative-adversarial approach and use two neural networks, $Q$ and $T$ . $Q$ is our generative model, taking as input a random vector and outputting a sample of interest. We parametrize $Q$ through a vector $\theta$ and write $Q_\theta$. $T$ is our variational function, taking as input a sample and returning a scalar. We parametrize $T$ using a vector ω and write $T_\omega$ .</p><script type="math/tex; mode=display">F(\theta, \omega)=\mathbb{E}_{x \sim P}\left[T_{\omega}(x)\right]-\mathbb{E}_{x \sim Q_{\theta}}\left[f^{*}\left(T_{\omega}(x)\right)\right]</script><p>以上给出了基于f-divergence的minmax game. </p><h1 id="WGAN"><a href="#WGAN" class="headerlink" title="WGAN"></a>WGAN</h1><p>WGAN使用了在f-GAN family 之外的用于衡量两个分布的距离函数: Earth-Mover (EM) distance or Wasserstein-1. </p><p><strong>Definition</strong> (Earth-Mover Distance). The <strong>Earth-Mover distance</strong> between two distribution $\mathbb P_r$ and $\mathbb P_g$ is defined as</p><script type="math/tex; mode=display">W\left(\mathbb{P}_{r}, \mathbb{P}_{g}\right)=\inf _{\gamma \in \Pi\left(\mathbb{P}_{r}, \mathbb{P}_{g}\right)} \mathbb{E}_{(x, y) \sim \gamma}[\|x-y\|]</script><p>where $\Pi (\mathbb P_r, \mathbb P_g)$ denotes the set of all joint distributions $\gamma (x,y)$ whose marginals are respectively $\mathbb P_r$ and $\mathbb P_g$. Intuitively, $\gamma (x,y)$ indicates how much “mass” must be transported from $x$ to $y$ in order to transform the distributions $\mathbb P_r$ into the distribution $\mathbb P_g$. The EM distance then is the “cost” of the optimal transport plan. </p><p>计算 $\inf$ 需要我们最小化一个目标函数找到对应的最小值。对于离散型随机变量我们可以将这个问题转化成一个线性规划问题，然后使用线性规划进行求解。对于现实问题例如输入为image的高维数据点来说是不切实际的，因此在WGAN中作者采用了Kantorovich-Rubinstein duality来解决这个优化问题，具体的推导我们暂且放一下，以后有空的时候再一并整理优化问题的内容。</p><script type="math/tex; mode=display">W\left(\mathbb{P}_{r}, \mathbb{P}_{\theta}\right)=\sup _{\|f\|_{L} \leq 1} \mathbb{E}_{x \sim \mathbb{P}_{r}}[f(x)]-\mathbb{E}_{x \sim \mathbb{P}_{\theta}}[f(x)]</script><p>其中 $|f|_{L} \leq 1$ 表示的是符合1-Lipschitz condition的实值函数，如果我们考虑参数化两个函数，就能得到以下的最大化目标：</p><script type="math/tex; mode=display">\max _{w \in \mathcal{W}} \mathbb{E}_{x \sim \mathbb{P}_{r}}\left[f_{w}(x)\right]-\mathbb{E}_{z \sim p(z)}\left[f_{w}\left(g_{\theta}(z)\right]\right.</script><p>从这个角度我们看到了WGAN也是遵从一个minmax game的形式的. 那么为什么WGAN是一个更好的优化目标呢？这是因为WGAN能够量化反应训练质量，以下图为例，两个分布逐渐靠近，WGAN的优化目标是能够反映出来的，但是对于以divergence为基础的训练目标来说，两者的距离会在重合的一瞬间降为0. </p><p><img src="https://i.loli.net/2020/06/22/EBCLkwRrMvsfSJ4.png" alt="image.png"></p><p>WGAN需要对参数化的函数需要进行限制：那就是将函数限制在K-Lipschitz的function family中。文章的方法就是做weight clipping. 当然这个方法导致的具体的$K$是多少，我们就没有办法得知了。</p><blockquote><p>In order to have parameters $W$ lie in a compact space, something simple we can do is clamp the weights to a fixed box (say $W = [−0.01, 0.01]^l$) after each gradient update.</p></blockquote><p><img src="https://i.loli.net/2020/06/22/QGrKpkiconIBEaY.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://math.stackexchange.com/questions/2435464/show-that-max-function-on-mathbb-rn-is-
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
      <category term="GAN" scheme="http://www.shihaizhou.com/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>Manifold Learning</title>
    <link href="http://www.shihaizhou.com/2020/06/14/Manifold-Learning/"/>
    <id>http://www.shihaizhou.com/2020/06/14/Manifold-Learning/</id>
    <published>2020-06-14T11:51:27.000Z</published>
    <updated>2020-06-16T11:45:38.547Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://scikit-learn.org/stable/modules/manifold.html" target="_blank" rel="noopener">sklearn page introducing different manifold learning methods</a></li><li><a href="http://www.cad.zju.edu.cn/reports/%C1%F7%D0%CE%D1%A7%CF%B0.pdf" target="_blank" rel="noopener">Prof.He’s slides on manifold learning</a></li><li><a href="https://www.youtube.com/watch?v=yBwpo-L80Mc" target="_blank" rel="noopener">hopkins lecture on LLE</a></li></ul><h1 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h1><p>High-dimensional datasets can be very difficult to visualize. While data in two or three dimensions can be plotted to show the inherent structure of the data, equivalent high-dimensional plots are much less intuitive. To aid visualization of the structure of a dataset, the dimension must be reduced in some way.</p><p>Manifold Learning can be thought of as an attempt to generalize linear frameworks like PCA to be sensitive to non-linear structure in data. Though supervised variants exist, the typical manifold learning problem is unsupervised: it learns the high-dimensional structure of the data from the data itself, without the use of predetermined classifications.</p><p><strong>Definition</strong> (Geodesic Distance). A geodesic line is the shortest path between two points on a curved surface, like Earth (referring to the following figure). </p><p><img src="https://i.loli.net/2020/06/14/y6joT37BIbcMdPu.png" alt="image.png"></p><p>我认为在Manifold Learning这个领域的最重要假设在于：足够接近的数据点的测地线距离等于他们的欧氏距离。也就是说一小簇点在高维空间可以被看做分布在一个平面上，进而满足线性关系。</p><h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><p>以下方法建立在共同的notation下：我们有一高维数据点集合 $X=\{x_i\}_{i=1}^{N} \sub \mathbb R^D$，现有函数 $f: \mathbb R^D \rightarrow \mathbb R^d$为将高维数据空间映射到表征空间的映射函数，映射后的表征空间$(Y, \rho)$ 是一个欧拉空间，其中：</p><script type="math/tex; mode=display">\begin{align}Y &=\{y_i\}_{i=1}^{N} \sub \mathbb R^d \\\rho_{ij} &=\rho(y_i, y_j) = \|y_i-y_j\|_2\end{align}</script><p>需要注意的是，在Manifold Learning中的$f$经常是没有办法用于新样本的推断的。</p><h2 id="Multidimensional-Scaling-MDS"><a href="#Multidimensional-Scaling-MDS" class="headerlink" title="Multidimensional Scaling (MDS)"></a>Multidimensional Scaling (MDS)</h2><p>本质上来说，Manifold Learning的学习目标是希望在表征空间上保持整个数据分布的geometry property. 因为我们认为高维数据点在高维空间是分布在一个流型上的，所以距离度量方式并不应当采用欧氏距离，例如如下的优化目标就不合适：</p><script type="math/tex; mode=display">\min_f\sum_{i,j}(\rho_{ij}-d_{ij})^2</script><p>其中$d_{ij}=d_E(x_i,x_j)$，表示在样本空间的欧氏距离。但基于manifold learning的假设，也就是在一个数据点的很小邻域内，样本间可以近似看作是线性的，因此我们可以只考虑每一个点的邻近点的欧氏距离需要在表征空间中被近似，所以以下的优化目标可以看做保持了样本分布的geometry attributes：</p><script type="math/tex; mode=display">\min_f \sum_{i=1}^N\sum_{j\sim i}(\rho_{ij}-d_{ij})^2</script><p>以上的优化目标就是MDS的主体部分，它是non-trivial的，虽然我们可以使用梯度下降进行优化。</p><h3 id="MDS-on-dot-product"><a href="#MDS-on-dot-product" class="headerlink" title="MDS on dot product"></a>MDS on dot product</h3><p>考虑MDS的使用dot product作为度量两个数据点的距离：$K_{ij}=x_i^Tx_j$. 全局优化目标为：</p><script type="math/tex; mode=display">\min_f \sum_{i,j}(y_i^Ty_j -K_{ij})^2</script><p>于是等价于优化以下目标</p><script type="math/tex; mode=display">\min \|Y^TY-K\|_F^2</script><p>也就是说MDS在dot product情况下退化 成了基于SVD的dimensional reduction method. </p><script type="math/tex; mode=display">\begin{align}K &= U\Sigma U^T \\Y &= U_{1:d}\Sigma_{i:d}\end{align}</script><h2 id="Isomap"><a href="#Isomap" class="headerlink" title="Isomap"></a>Isomap</h2><p>One of the earliest approaches to manifold learning is the Isomap algorithm, short for Isometric Mapping.  Isomap seeks a lower-dimensional embedding which maintains geodesic distances between every two points. </p><h3 id="Estimating-Geodestic-Distance"><a href="#Estimating-Geodestic-Distance" class="headerlink" title="Estimating Geodestic Distance"></a>Estimating Geodestic Distance</h3><p>We could estimate the geodestic distance by constructing an adjacency graph on which the shortest distance between two nodes is the estimation of their geodestic distance. We can set the adjacency matrix following the rule: </p><script type="math/tex; mode=display">W_{i j}=\left\{\begin{array}{cc}1 & \left\|x_{i}-x_{j}\right\|<\epsilon \\0 & \text { otherwise }\end{array}\right.</script><p>在sklearn主页上，该算法步骤主要由三步构成，第一是为每个点构建Nearest Neighbors，连接这些neighbors就得到了一个在manifold上的连通图；接着需要计算任意两个点之间的geodestic distance；最后使用Partial Eigenvalue Decomposition求得降维后的表示。</p><p>Isomap使用 MDS 计算映射后的坐标$y$，使得映射坐标下的欧氏距离与原来的测地线距离尽量相等.</p><script type="math/tex; mode=display">\min _{f} \sum_{i, j}\left(\rho_{ij} - d_{M}\left(x_{i}, x_{j}\right) \right)^{2}</script><h2 id="Locally-Linear-Embedding"><a href="#Locally-Linear-Embedding" class="headerlink" title="Locally Linear Embedding"></a>Locally Linear Embedding</h2><p>Locally linear embedding (LLE) seeks a lower-dimensional projection of the data which preserves distances within local neighborhoods. It can be thought of as a series of local Principal Component Analyses which are globally compared to find the best non-linear embedding.</p><p>“流形在局部可以近似等价于欧氏空间”是 LLE 分析方法的出发点。LLE认为一个中心数据点 $x_i$ 能够被处于其小邻域内的点$\{x_j\}_{j \sim i}$线性重构，重构的权重可能是我们想要在表征空间上保留的geometry attributes。</p><script type="math/tex; mode=display">x_i \approx \sum_{j\sim i} W_{ij}x_j</script><p>因而该算法由下面两步构成，首先计算点之间的最优重构权重需要解决以下的优化问题，对$d$维的表征空间，我们需要使用$k\geq d$的kNN来寻找邻接节点。</p><script type="math/tex; mode=display">\begin{align}&\min_{W}\sum_{i=1}^{N}\left\|x_{i}-\sum_{j \sim i} W_{i j} x_{j}\right\|^{2} \\&s.t. \sum_{j\sim i}W_{ij}=1,\quad \forall i \in \{1, 2, \cdots, N\}\end{align}</script><p>接着以此作为目标使映射空间中的表征保留该权重，其中对于表征空间上的单位向量的限制是为了防止0向量这个trivial solution.</p><script type="math/tex; mode=display">\begin{align}\min_Y &\sum_{i=1}^{N} \left\|y_{i}-\sum_{j \sim i} W_{i j} y_{j}\right\|^{2} \\s.t. &\sum_{i=1}^{N} y_i = 0 \\& YY^T = I_d\end{align}</script><h2 id="Laplacian-Eigenmap"><a href="#Laplacian-Eigenmap" class="headerlink" title="Laplacian Eigenmap"></a>Laplacian Eigenmap</h2><p>是一种Spectral Embedding方法，可以说是从Spectral Clustering衍生出来的，根本的目标依旧是希望在高维空间接近的点在降维之后也更为接近。我们的优化目标为：</p><script type="math/tex; mode=display">J(y) = \sum_{i,j}W_{ij}(y_i-y_j)^2=y^{\dagger}Ly</script><p>我们可以将optimization problem进行一些转化：</p><script type="math/tex; mode=display">\arg\min y^{\dagger}Ly \Leftrightarrow \arg\min \langle Ly, y\rangle</script><p>它们都受到了以下约束：</p><script type="math/tex; mode=display">\begin{align}y^{\dagger} L y &=1 \\y^{\dagger} D 1 &=0\end{align}</script><p>其中第一条是约束向量的scale，第二条是为了避免 $y=1$的trivial solution.  </p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>意图对Manifold Learning不使用传统解析解而使用NN+BP的方法进行优化，并观察在representation空间上的表现。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://scikit-learn.org/stable/modules/manifold.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;sk
      
    
    </summary>
    
    
      <category term="Basics" scheme="http://www.shihaizhou.com/tags/Basics/"/>
    
      <category term="Manifold" scheme="http://www.shihaizhou.com/tags/Manifold/"/>
    
  </entry>
  
  <entry>
    <title>Disentangled Representation Learning via Mutual Information Estimation</title>
    <link href="http://www.shihaizhou.com/2020/06/10/Learning-Disentangled-Representations-via-Mutual-Information-Estimation/"/>
    <id>http://www.shihaizhou.com/2020/06/10/Learning-Disentangled-Representations-via-Mutual-Information-Estimation/</id>
    <published>2020-06-10T09:03:39.000Z</published>
    <updated>2020-06-10T10:39:59.943Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/abs/1912.03915" target="_blank" rel="noopener">Learning Disentangled Representations via Mutual Information Estimation</a></li></ul><h1 id="Method-Description"><a href="#Method-Description" class="headerlink" title="Method Description"></a>Method Description</h1><p>Given a pair of images sharing some attributes, we aim to create a low-dimensional representation which is split into two parts: a shared representation that captures the common information between the images and an exclusive representation that contains the specific information of each image.</p><p><strong>Two stages of training.</strong> First, the shared representation is learned via cross mutual information estimation and maximization. Secondly, mutual information maximization is performed to learn the exclusive representation <strong>while minimizing the mutual information</strong> between the shared and exclusive representations.</p><p><img src="https://i.loli.net/2020/06/10/MBR4Nm38ZoYDCHS.png" alt="image.png"></p><p>两个样本的representation包括了shared information and exclusive information. </p><p>首先在训练的第一阶段，我们希望他们的<strong>shared representation能够尽量相同</strong>，于是文章采用了叫做cross mutual information maximization的方法，将 (sample，local patch, representation) 三元组 $(X, C_X, R_X)$ 和另一个样本的三元组$(Y, C_Y, R_Y)$进行”交叉互信息最大化”:</p><script type="math/tex; mode=display">\max [I(X, R_Y)+I(Y, R_X)] + [I(C_X,R_Y)+I(C_Y,R_X)]</script><p>接着，我们希望他们的<strong>exclusive representation能尽量不同</strong>。于是文章在第二阶段，固定了生成shared representation的模型参数，同时在两方面训练网络：最大化合并之后的representation的global/local MI，同时最小化exclusive/shared MI. </p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>关于实验的和整个任务的setting，有以下的问题和作者进行交流。</p><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><p>Dear Eduardo Hugo Sanchez,</p><p>After reading your wonderful paper “Learning Disentangled Representations via Mutual Information Estimation”, I have one question regarding the setup of your training procedure, which is:</p><p>How do you decide between which two images the MI is maximized during training? Say in Colorful MNIST, if you put the images of the same digit together and follow your obejective, then can I conclude that you’re actually telling the model to learn a (linear, in some cases) seperable representation regarding the digit classification? Below is how I come to this conclusion:<br>If we are maximizing the MI between all the images containing the same digit, then during the process of MI maximization, we will shuffle the whole batch of the data in order to form the negative samples, which will be feed into the critic function. Since the critic function has to distinguish the samples like (X,Y)=(black7, red7) and the shuffled samples like (X,Y’)=(black7, yellow10), we are explicitly telling the model to classify the digits.<br>If we train the whole network in a totally unsupervised way, i.e., training sample pairs like (X,Y)=(black7, yellow10) are randomly showing up asking the model to learn the shared information between them, how the method is able to learn the disentangled representations is really confusing me… </p><p>Furthermore, did you do the ablation study of the “cross mutual information maximization” technique? How did it go? </p><h2 id="Reply-from-author"><a href="#Reply-from-author" class="headerlink" title="Reply from author"></a>Reply from author</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1912.03915&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Learning Disentangled R
      
    
    </summary>
    
    
      <category term="MI" scheme="http://www.shihaizhou.com/tags/MI/"/>
    
  </entry>
  
  <entry>
    <title>InfoAE - Unpublished</title>
    <link href="http://www.shihaizhou.com/2020/06/09/InfoAE-Unpublished/"/>
    <id>http://www.shihaizhou.com/2020/06/09/InfoAE-Unpublished/</id>
    <published>2020-06-09T04:17:07.000Z</published>
    <updated>2020-06-09T08:21:02.053Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/abs/1904.08613" target="_blank" rel="noopener">arxiv preprint paper “DISENTANGLED REPRESENTATION LEARNING WITH INFORMATION MAXIMIZING AUTOENCODER”</a></li><li><a href="https://github.com/ABedychaj/InfoAE" target="_blank" rel="noopener">InfoAE github repo (not sure)</a></li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>该工作将 GAN 和 AE 进行了融合，虽然和InfoGAN一样提到了想要maximize MI，但是该篇工作并没有使用InfoMax或是Variational InfoMax直接对他们的MI进行优化，而是采用了一个classifier迫使encoder编码分类信息，最终文章纯粹依靠非监督信息在分类任务上取得了非常高的得分。</p><h1 id="InfoAE"><a href="#InfoAE" class="headerlink" title="InfoAE"></a>InfoAE</h1><p><img src="https://i.loli.net/2020/06/09/kJjnc1KpyzSQg37.png" alt="image.png"></p><p>InfoAE可以分为Conditional GAN和Autoencoder两个部分，如上图所示。</p><p><strong>Conditional GAN</strong>的部分对应于上图的红色流程+黄色流程：首先 $z$ 是prior random noise，$c$ 是latent code；在实验中取 $c$ 为 $K=10$ 的one-hot coding. 经过Generator Network $G$ 之后映射到了latent representation space $r$；接着 $r$ 通过Decoder Network得到了fake sample $\hat {x_g}$. 将 fake sample 和 true sample进行对比从而得到了GAN loss. 同时因为该方法是为了下游的分类任务服务，所以我们希望Encoder能够将分类信息进行编码。但是对于一个完全无标签的非监督学习样本$x$来说，我们是没有办法显式地为他进行分类的，唯一编码了分类信息的变量就是latent code $c$. 因此我们将得到的中间表示 $r$ 重新经过一个Classifier Network，将得到的分类结果重新映射回 $c$. </p><p>在<strong>Autoencoder</strong>部分就是最简单的Reconstruction Error，对应于上图中的绿色流程。</p><p>除此之外，InfoAE 还将 Autoencoder 和 Generator 生成的 <strong>representation space进行了alignment</strong>. 具体方法也是采用GAN的discriminative training将两个分布的divergence变小。我的问题是：为什么需要将原本的input和representation进行concatenation再最小化两个分布的divergence？为什么不可以直接将两个representation进行discriminative training?</p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>We have evaluated the model on MNIST dataset and received outstanding results. InfoAE is trained on MNIST training data without any labels. After trainning, We encoded the test data with Encoder, E and got classification label with the Classifier, C. Then we clustered the test data according to label and received classification accuracy of 98.9 (±.05), which is better than the popular methods as shown in Table 1.</p><p><img src="https://i.loli.net/2020/06/09/vfh5pcbKreg8xna.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1904.08613&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;arxiv preprint paper “D
      
    
    </summary>
    
    
      <category term="MI" scheme="http://www.shihaizhou.com/tags/MI/"/>
    
      <category term="GAN" scheme="http://www.shihaizhou.com/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>InfoGAN</title>
    <link href="http://www.shihaizhou.com/2020/06/08/InfoGAN/"/>
    <id>http://www.shihaizhou.com/2020/06/08/InfoGAN/</id>
    <published>2020-06-08T11:37:22.000Z</published>
    <updated>2020-10-20T06:56:14.832Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/abs/1606.03657" target="_blank" rel="noopener">InfoGAN paper</a></li><li><a href="https://pdfs.semanticscholar.org/61f4/f67fc0e73fa3aef8628aae53a4d9b502d381.pdf" target="_blank" rel="noopener">Understanding Mutual Information and its Use in InfoGAN</a></li><li><a href="https://github.com/Natsu6767/InfoGAN-PyTorch" target="_blank" rel="noopener">pytorch-infogan github repo</a></li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p><strong>Problem of unsupervised representation learning.</strong> While unsupervised learning is ill-posed because the relevant downstream tasks are unknown at training time, a disentangled representation, one which explicitly represents the salient attributes of a data instance, should be helpful for the relevant but unknown tasks. Thus, to be useful, an unsupervised learning algorithm must in effect correctly guess the likely set of downstream classification tasks without being directly exposed to them.</p><p><strong>How to learn disentangled representation in this paper.</strong> In this paper, we present a simple modification to the generative adversarial network objective that encourages it to learn interpretable and meaningful representations. We do so by maximizing the mutual information between a fixed small subset of the GAN’s noise variables and the observations, which turns out to be relatively straightforward.</p><h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><h2 id="Mutual-Information-for-Inducing-Latent-Codes"><a href="#Mutual-Information-for-Inducing-Latent-Codes" class="headerlink" title="Mutual Information for Inducing Latent Codes"></a>Mutual Information for Inducing Latent Codes</h2><p>在GAN的生成中，generator将prior distribution映射到样本空间的过程并没有对“如何利用这个distribution”进行限制. As a result, it is possible that the noise will be used by the generator in a highly entangled way, causing the individual dimensions of $z$ to not correspond to semantic features of the data.</p><p>In this paper, rather than using a single unstructured noise vector, we propose to decompose the input noise vector into two parts: (i) $z$, which is treated as source of incompressible noise; (ii) $c$, which we will call the latent code and will target the salient structured semantic features of the data distribution. </p><p>We provide the generator network with both the incompressible noise z and the latent code c, so the form of the generator becomes $G(z, c)$. However, in standard GAN, the generator is free to ignore the additional latent code $c$ by finding a solution satisfying $P_G(x|c) = P_G(x)$. 为了解决trivial codes的问题，我们认为code $c$ 和生成结果generator distribution $G(z, c)$之间有较强的dependency，也就是说 $I(c; G(z,c))​$ 应当较高. 最终，InfoGAN优化的目标为以下：</p><script type="math/tex; mode=display">\min _{G} \max _{D} V_{I}(D, G)=V(D, G)-\lambda I(c ; G(z, c))</script><h2 id="Variational-Mutual-Information-Maximization"><a href="#Variational-Mutual-Information-Maximization" class="headerlink" title="Variational Mutual Information Maximization"></a>Variational Mutual Information Maximization</h2><p>PASS. We will use InfoMax method to reproduce the experimental results. </p><p><img src="https://i.loli.net/2020/06/08/toTXH429CPzJhG8.png" alt="image.png"></p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>GAN的实验部分通常都比较偏经验性，并没有太好的Numerical Metrics来进行比较。值得一提的是，InfoGAN和之前看到的一篇未投稿工作一样，在MNIST数据集上验证时将latent code设置成了$K=10$ 的one-hot coding。我认为在这种setting下，representation进入下游分类任务取得很好的成绩是因为10-way classification是关于下游任务非常重要的信息泄露。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1606.03657&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;InfoGAN paper&lt;/a&gt;&lt;/li&gt;

      
    
    </summary>
    
    
      <category term="MI" scheme="http://www.shihaizhou.com/tags/MI/"/>
    
      <category term="GAN" scheme="http://www.shihaizhou.com/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>Discriminative Clustering by Regularized Information Maximization</title>
    <link href="http://www.shihaizhou.com/2020/06/04/Discriminative-Clustering-by-Regularized-Information-Maximization/"/>
    <id>http://www.shihaizhou.com/2020/06/04/Discriminative-Clustering-by-Regularized-Information-Maximization/</id>
    <published>2020-06-04T11:02:37.000Z</published>
    <updated>2020-06-07T15:13:29.158Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://papers.nips.cc/paper/4154-discriminative-clustering-by-regularized-information-maximization.pdf" target="_blank" rel="noopener">ICLR-2010 paper “Discriminative Clustering by Regularized Information Maximization”.</a></li><li><a href="https://arxiv.org/abs/1907.13625" target="_blank" rel="noopener">ICLR-2020 paper “On Mutual Information Maximization for Representation Learning”</a>.</li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>本文是采用MI来做Unsupervised Discriminative Clustering的工作，在”On Mutual Information Maximization for Representation Learning”中被提到，其中的表述如下：</p><blockquote><p>It is folklore knowledge that maximizing MI does not necessarily lead to useful representations. Already Linsker (1988) talks in his seminal work about constraints, while a manifestation of the problem in clustering approaches using MI criteria has been brought up by Bridle et al. (1992) and subsequently addressed using regularization by Krause et al. (2010).</p></blockquote><p>我们想要探究的是为什么maximizing MI不能够带来下游任务表现的提升？虽然前人的工作已经经验性地展示了这一点：MI的优化和下游任务表现的提升是disproportional的，通过提供constraints使得MI作为一个有效的目标，那么什么样的constraint才是好的constraint？设计一个新的constraint需要遵守怎样的guideline？这些也是我们想要研究的方向。这篇添加了regularization的工作可能是一个比较好的起始点。</p><p>这篇文章的动机在于当前的clustering方法没有很好地引入probabilistic model, 因此文章：</p><blockquote><p>We propose a principled probabilistic approach to discriminative clustering, by formalizing the problem as unsupervised learning of a conditional probabilistic model.</p><p>We identify two fundamental, competing quantities, class balance and class separation, and develop an information theoretic objective function which trades off these quantities.</p><p>Our approach corresponds to maximizing mutual information between the empirical distribution on the inputs and the induced label distribution, regularized by a complexity penalty. Thus, we call our approach Regularized Information Maximization (RIM).</p></blockquote><h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><h2 id="Discriminative-Clustering"><a href="#Discriminative-Clustering" class="headerlink" title="Discriminative Clustering"></a>Discriminative Clustering</h2><p>给定数据集$X$，目标是学习到一个conditional model $p(y|x, W)$ with parameters $W$ which predicts a distribution over label values $y \in \{1,\cdots,K\}$ given an input vector $x$.</p><h2 id="Information-Maximization"><a href="#Information-Maximization" class="headerlink" title="Information Maximization"></a>Information Maximization</h2><p>将Information Maximization方法应用到unsupervised clustering中不需要显示得定义cluster number. 同时因为最终的representation就是离散的类别标签，所以我们可以直接估计input-label两者的MI. 以下是作者论述构建出MI作为优化目标的clustering method.</p><p>首先，discriminative clustering方法的第一个原则是不希望在数据分布密集的地方划分decision boundary. 前人的工作说明了conditional entropy term $-\frac{1}{N} \sum_{i} H\left\{p\left(y | \mathbf{x}_{i}, \mathbf{W}\right)\right\}$ very effectively captures the cluster assumption when training probabilistic classifiers <strong>with partial labels</strong>. </p><p>但是仅仅优化conditional entropy也是不够的。我们知道降低conditional entropy的目标是希望我们probabilistic model对于自己分类结果更加“确定”. 而因为我们的聚类方法没有对$K$的数量进行限制，所以”simply removing decision boundaries”，将所有的样本都归为同一类就能够骗过我们的优化目标，而得到非常糟糕的discriminative model. </p><p>为了避免以上提到的degenerate solution, 文章提出要做到类间平衡 (class balance). 计算每个类别的marginal label distribution，我们得到：</p><script type="math/tex; mode=display">\hat{p}(y ; \mathbf{W})=\int \hat{p}(\mathbf{x}) p(y | \mathbf{x}, \mathbf{W}) d \mathbf{x}=\frac{1}{N} \sum_{i} p\left(y | \mathbf{x}_{i}, \mathbf{W}\right)</script><p>所以类间平衡的要点在于使每个类别尽量能够均匀地分布，即最大化类分布的熵。结合两者，我们得到了最大化MI的优化目标：</p><script type="math/tex; mode=display">I_{\mathbf{W}}\{y ; \mathbf{x}\}=H\{\hat{p}(y ; \mathbf{W})\}-\frac{1}{N} \sum_{i} H\left\{p\left(y | \mathbf{x}_{i}, \mathbf{W}\right)\right\}</script><p>在clustering任务中，前人也已经发现了这个优化目标有trivial solution：”However, they note that $I_{\mathbf{W}}\{y ; \mathbf{x}\}$ may be trivially maximized by a conditional model that classifies each data point $x_i$ into its own category $y_i$, and that classifiers trained with this objective tend to fragment the data into a large number of categories”. </p><h2 id="Regularized-Information-Maximization"><a href="#Regularized-Information-Maximization" class="headerlink" title="Regularized Information Maximization"></a>Regularized Information Maximization</h2><p>在此基础上，文章提出了使用一个正则项 $R(W;\lambda)$ 对MI进行约束：This term penalizes conditional models with complex decision boundaries in order to yield sensible clustering solutions. Our objective function is</p><script type="math/tex; mode=display">F(\mathbf{W} ; \mathbf{X}, \lambda)=I_{\mathbf{W}}\{y ; \mathbf{x}\}-R(\mathbf{W} ; \lambda)</script><p>在Multilogit-regression任务中文章给出了参数的模作为正则项。本质上正则项就是为了防止过多的decision boundary被发现从而约束RIM的类别数量，一般来说，$\lambda$ 越大类数越小。</p><h1 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h1><p><img src="https://i.loli.net/2020/06/07/jEHZYqNQc8oF4Sn.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://papers.nips.cc/paper/4154-discriminative-clustering-by-regularized-information-m
      
    
    </summary>
    
    
      <category term="MI" scheme="http://www.shihaizhou.com/tags/MI/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch Techniques</title>
    <link href="http://www.shihaizhou.com/2020/05/28/Pytorch-Techniques/"/>
    <id>http://www.shihaizhou.com/2020/05/28/Pytorch-Techniques/</id>
    <published>2020-05-28T13:39:09.000Z</published>
    <updated>2020-08-02T07:33:46.927Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html" target="_blank" rel="noopener">pytorch official guide “training CNN classifiers on CIFAR-10”</a></li></ul><h1 id="Functions"><a href="#Functions" class="headerlink" title="Functions"></a>Functions</h1><h3 id="torch-nn-Conv2d"><a href="#torch-nn-Conv2d" class="headerlink" title="torch.nn.Conv2d"></a>torch.nn.Conv2d</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Conv2d(</span><br><span class="line">    in_channels, </span><br><span class="line">    out_channels, </span><br><span class="line">    kernel_size, </span><br><span class="line">    stride=<span class="number">1</span>, </span><br><span class="line">    padding=<span class="number">0</span>, </span><br><span class="line">    dilation=<span class="number">1</span>, </span><br><span class="line">    groups=<span class="number">1</span>, </span><br><span class="line">    bias=<span class="literal">True</span>, </span><br><span class="line">    padding_mode=<span class="string">'zeros'</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>Pytorch和tensorflow中（也和我们一般理解的）的图像表示形式不同：torch中的channel维在height和width之前，所以input的tensor形状为$(N,C_{\text{in}}, H, W)$. </p><h3 id="torch-nn-ZeroPad2d"><a href="#torch-nn-ZeroPad2d" class="headerlink" title="torch.nn.ZeroPad2d"></a>torch.nn.ZeroPad2d</h3><p>在二维Tensor的左右上下四个方向添加zero-padding. </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pad = nn.ZeroPad2d(padding=(1, 2, 3, 4))</span><br><span class="line">y = pad(x)</span><br></pre></td></tr></table></figure><h3 id="torchvision-transforms-Pad"><a href="#torchvision-transforms-Pad" class="headerlink" title="torchvision.transforms.Pad"></a>torchvision.transforms.Pad</h3><p>有的时候我们希望对于数据的处理都封装在load_data这一层中，所以我们希望能够使用torchvision里的<code>transform.Compose</code>. 以下展示了将MNIST上<code>(1, 28, 28)</code>的数据转换为能够匹配用于CIFAR-10的DCGAN架构的代码。注意Pad需要在<code>ToTensor()</code>层前，因为该方法适配于PIL Image对象而不是Tensor对象。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([transforms.Pad(padding=(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>), fill=<span class="number">0</span>),</span><br><span class="line">                                transforms.ToTensor(),</span><br><span class="line">                                transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))])</span><br></pre></td></tr></table></figure><p>在BYOL工作中，我们需要得到一个batch的PIL image然后对该batch进行两次同样的随机化的transform. 注意<code>torchvision.transform</code>对图像的操作都封装在了dataloader中. 背后的原理是可以使用多线程对多个图像进行并行的transform最后将PIL图像转换为tensor组装成为一个batch的tensor可以加速这个操作，这是pytorch的开发人员在设计transform这个机制的哲学，因此他们不认为对batched transform的支持是必须的. 这就给我们的开发带了一些问题. </p><p>一个解决方案是采用<a href="https://kornia.github.io/" target="_blank" rel="noopener">kornia这个pytorch支持库</a>，在<code>dataloader</code>层面我们只需要一个<code>ToTensor</code>的<code>transform</code>即可，然后通过kornia定义一个自己的<code>nn.Sequential</code>即可完成要求. </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html&quot; target=&quot;_blank&quot; rel=
      
    
    </summary>
    
    
      <category term="Code" scheme="http://www.shihaizhou.com/tags/Code/"/>
    
  </entry>
  
  <entry>
    <title>Information Competing Process for Learning Diversified Representations</title>
    <link href="http://www.shihaizhou.com/2020/05/27/Information-Competing-Process-for-Learning-Diversified-Representations/"/>
    <id>http://www.shihaizhou.com/2020/05/27/Information-Competing-Process-for-Learning-Diversified-Representations/</id>
    <published>2020-05-27T06:57:20.000Z</published>
    <updated>2020-05-28T02:17:06.212Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/abs/1906.01288" target="_blank" rel="noopener">NIPs-2019 paper “Information Competing Process for Learning Diversified Representations”</a></li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>Aiming to enrich the information carried by feature representations, ICP separates a representation into two parts with different mutual information constraints. The separated parts are forced to accomplish the downstream task independently in a competitive environment which prevents the two parts from learning what each other learned for the downstream task. Such competing parts are then combined synergistically to complete the task.</p><h2 id="Representation-Collaboration"><a href="#Representation-Collaboration" class="headerlink" title="Representation Collaboration"></a>Representation Collaboration</h2><p>The Competitive Collaboration method is the most relevant to our work. It defines a three-player game with two competitors and a moderator, where the moderator takes the role of a critic and the two competitors collaborate to train the moderator. Unlike Competitive Collaboration, the proposed ICP enforces two (or more) representation parts to be complementary through different mutual information constraints for the same downstream task by a competitive environment, which endows the capability of learning more discriminative and disentangled representations.</p><h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><h2 id="Objectives"><a href="#Objectives" class="headerlink" title="Objectives"></a>Objectives</h2><p>首先将supervised和self-supervised两个setting中的objective进行一下统一. In supervised setting, $t$ represents the label of input $xt$. In self-supervised setting, $xt$ represents the input $tx$ itself. This leads to the unified objective function linking the representation $rx$ of input $x$ and target $t$ as:</p><script type="math/tex; mode=display">\max \mathcal I(r,t)</script><h2 id="Seperating-Representations"><a href="#Seperating-Representations" class="headerlink" title="Seperating Representations"></a>Seperating Representations</h2><p>Directly separate the representation $r$ into two parts $[z, y]$. Specifically, we constrain the information capacity of representation part $z$ while increasing the information capacity of representation part $y$.</p><script type="math/tex; mode=display">\max [\mathcal{I}(r, t)+\alpha \mathcal{I}(y, x)-\beta \mathcal{I}(z, x)]</script><h2 id="Representation-Competition"><a href="#Representation-Competition" class="headerlink" title="Representation Competition"></a>Representation Competition</h2><p>ICP prevents $z$ and $y$ from knowing what each other learned for the downstream task, which is realized by enforcing $z$ and $y$ independent of each other.</p><script type="math/tex; mode=display">\max [\mathcal{I}(r, t)+\alpha \mathcal{I}(y, x)-\beta \mathcal{I}(z, x)+\mathcal{I}(z, t)+\mathcal{I}(y, t)-\gamma \mathcal{I}(z, y)]</script><p>在形式上和我们想要做的InfoBal非常接近，我们将以上式子带入unsupervised learning的情境下，也就是target $t$ = input $x$:</p><script type="math/tex; mode=display">\begin{align}&\max [\mathcal{I}(r, t)+\alpha \mathcal{I}(y, x)-\beta \mathcal{I}(z, x)+\mathcal{I}(z, t)+\mathcal{I}(y, t)-\gamma \mathcal{I}(z, y)] \\\Leftrightarrow& \max [\mathcal{I}(r, x)+\alpha \mathcal{I}(y, x)-\beta \mathcal{I}(z, x)+\mathcal{I}(z, x)+\mathcal{I}(y,x)-\gamma \mathcal{I}(z, y)] \\\Leftrightarrow& \max [\mathcal{I}(r, x)+(\alpha+1) \mathcal{I}(y, x)+(1-\beta) \mathcal{I}(z, x)-\gamma \mathcal{I}(z, y)] \\\Leftrightarrow& \max [\mathcal{I}(r, x)+\alpha^\prime \mathcal{I}(y, x)+\beta^\prime \mathcal{I}(z, x)-\gamma \mathcal{I}(z, y)] \\\end{align}</script><p>where $\alpha^\prime &gt; 1$ and $\beta^\prime &lt; 1$. 如果把$z$和$y$分别看作数据的两个view，那么中间两项就代表不同view和$X$ 之间的相关度；第一项代表全局representation和input之间的相关度；而最后一项就是view之间的InfoMin项。</p><p><img src="https://i.loli.net/2020/05/27/W6RfgsN5GdKo9Ab.png" alt="image.png"></p><h2 id="Minimizing-MI"><a href="#Minimizing-MI" class="headerlink" title="Minimizing MI"></a>Minimizing MI</h2><p>本文采用的最小化MI的方法来源于variational inference. 同Maximizing MI的方法类似的，因为是为了最小化，所以我们希望能够找到$I(x,z)$的upper bound，然后直接去最小化这个upper bound. 首先我们有：</p><script type="math/tex; mode=display">\begin{align}\mathcal{I}(z, x) &=\iint P(z, x) \log \frac{P(z, x)}{P(z) P(x)} d x d z \\&= \iint P(z, x) \log P(z | x) d x d z-\int P(z) \log P(z) d z\end{align}</script><p>Let $Q(z)$ be a variational approximation of $P(z)$, we have:</p><script type="math/tex; mode=display">K L[P(z) \| Q(z)] \geq 0 \Rightarrow \int P(z) \log P(z) d z \geq \int P(z) \log Q(z) d z</script><p>将该不等式带入MI的定义式，我们得到了一个tractable upper bound (why???). </p><script type="math/tex; mode=display">\mathcal{I}(z, x) \leq \iint P(z | x) P(x) \log \frac{P(z | x)}{Q(x)} d x d z=\mathbb{E}_{x \sim P(x)}[K L[P(z | x) \| Q(z)]]</script><p>which enforces the extracted $z$ conditioned on $x$ to a predefined distribution $Q(z)$ such as a standard Gaussian distribution.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.01288&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;NIPs-2019 paper “Inform
      
    
    </summary>
    
    
      <category term="MI" scheme="http://www.shihaizhou.com/tags/MI/"/>
    
  </entry>
  
  <entry>
    <title>InfoBal - Ideas (failed)</title>
    <link href="http://www.shihaizhou.com/2020/05/26/InfoBal-Ideas/"/>
    <id>http://www.shihaizhou.com/2020/05/26/InfoBal-Ideas/</id>
    <published>2020-05-26T14:25:21.000Z</published>
    <updated>2020-10-20T07:02:14.896Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/abs/1907.13625" target="_blank" rel="noopener">ICLR-2020 paper “On Mutual Information Maximization for Representation Learning”</a></li><li><a href="https://arxiv.org/abs/1906.05849" target="_blank" rel="noopener">paper “Contrastive Multiview Coding”</a></li><li><a href="https://arxiv.org/abs/2005.10243" target="_blank" rel="noopener">paper “What Makes for Good Views for Contrastive Learning”</a></li><li><a href="https://arxiv.org/abs/1906.01288" target="_blank" rel="noopener">NIPs-2019 paper “Information Competing Process for Learning Diversified Representations”</a></li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p><a href="https://arxiv.org/abs/1906.05849" target="_blank" rel="noopener">CMC</a>的动机在于：重要的信息在多个view中被共享，所以我们应当最大化多个view的mutual information. 在<a href="https://arxiv.org/abs/2005.10243" target="_blank" rel="noopener">CMC作者的延续性工作</a>中，他认为只考虑view-invariant的信息是不够的，最佳的信息应当是和下游任务高度相关的，因此提出了InfoMin principle, 其核心思想是在input的两个view连同整体input和下游任务标签的MI保持一致的前提下，尽量使两个view之间的MI降低。而<a href="https://arxiv.org/abs/1907.13625" target="_blank" rel="noopener">ICLR-2020的文章</a>则将以往提出的三个MI优化目标（分别为global/local MI, CMC, CPC）都归入了统一的multiview理论框架下，认为这三个优化目标是这个框架下平行的优化目标。</p><p>我们探究global/local MI和CMC之间的关系，希望探究出它们的关系以及尝试构造一个完全基于MI的优化目标，希望这个优化目标能够让我们的representation学会在复杂特征信息中学会平衡，并且使模型能够学到不同方面和性质的information. </p><h1 id="Theory"><a href="#Theory" class="headerlink" title="Theory"></a>Theory</h1><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p>Suppose the input dataset is $X$. Consider the simplest setting of only 2 views $V_1$ and $V_2$ of $X$, which is parameterized by $\theta_1$ and $\theta_2$:</p><script type="math/tex; mode=display">\begin{align}V_1 &= g_1(X;\theta_1) \\V_2 &= g_2(X;\theta_2)\end{align}</script><p>In original InfoMax principle, the objective of learning representation of single view is to maximize:</p><script type="math/tex; mode=display">I(V; X)</script><p>When there are multiple views involved, we want them to be simultaneously maximized:</p><script type="math/tex; mode=display">\sum_i^2 I(V_i;X)</script><p>In InfoMin principle, the objective is to minimize the MI between different views <strong>under certain (complicated) constraints</strong>. </p><script type="math/tex; mode=display">I(V_1; V_2)</script><p>The above two objectives without constraints may not be so favorable analytically, since they both have trivial solutions which result in useless representation. </p><ul><li>For the original InfoMax principle: let function $g$ be bijective function, all the information of $X$ is preserved and the MI is maximized, no good form of representation is learned. </li><li>For the multiview InfoMin principle: let $V_1$ and $V_2$ output constant vectors, and their MI is 0, which is minimized.  </li></ul><h2 id="InfoBal-Objective"><a href="#InfoBal-Objective" class="headerlink" title="InfoBal Objective"></a>InfoBal Objective</h2><p>To solve this problem, notice that there exists an inequality: </p><script type="math/tex; mode=display">I(V_1;V_2) \leq \min(I(X;V_1), I(X;V_2))</script><p>which yields:</p><script type="math/tex; mode=display">\frac{1}{2}\sum_i^2 I(V_i;X) - I(V_1;V_2) \geq 0</script><p>So we can change our objective of representation learning with $M$ views to maximizing the following:  </p><script type="math/tex; mode=display">J(\Theta) = \frac{1}{M}\sum_i^MI(V_i;X) - \frac{2}{M(M-1)}\sum_{1\leq i < j\leq M}I(V_i,V_j) \geq 0</script><p>where $\Theta=\{\theta_1, \theta_2, \cdots, \theta_M\}$ represents the set of the view encoders, and is guaranteed bounded. </p><p>The reason why this formula is better than previous ones is that it helps the model to balance the information capacity and multiview diversity of the representation. And also in the two extreme cases where two single objectives can’t deal with morbid representation, the new objective function equally hates them: </p><p><strong>In the first morbid scenario</strong>, when every $V_i$ preserves all the info about $X$, i.e., </p><script type="math/tex; mode=display">I(V_i; X)=I(X;X)=H(X), \forall i\in\{1,\cdots,M\}</script><p>which is quite favorable, since the views (representations) are super powerful. While without constraints, we can assume the happening of the worst: each view is exactly identical, which makes the multiview setting not learning diverse representations. This case is discouraged by our new objective:</p><script type="math/tex; mode=display">\begin{align}J(\Theta) &= \frac{1}{M}\sum_i^MI(V_i;X) - \frac{2}{M(M-1)}\sum_{1\leq i < j\leq M}I(V_i,V_j) \\&= \frac{1}{M} \sum H(X) - \frac{2}{M(M-1)}\sum H(X) \\&= 0\end{align}</script><p><strong>In the second morbid scenario</strong>, when MI across the views are minimized, which means $V_i$ is irrelevant to each other, i.e., $I(V_i, V_j)=0$, this InfoMin situation could lead to extreme loss of information:</p><script type="math/tex; mode=display">I(V_i;X) = 0, \forall i\in\{1,\cdots,M\}</script><p>while in our objective, this is also discouraged:</p><script type="math/tex; mode=display">\begin{align}J(\Theta) &= \frac{1}{M}\sum_i^MI(V_i;X) - \frac{2}{M(M-1)}\sum_{1\leq i < j\leq M}I(V_i,V_j) \\&= \frac{1}{M} \sum 0 - \frac{2}{M(M-1)}\sum 0 \\&= 0\end{align}</script><h2 id="Global-Local-InfoBal"><a href="#Global-Local-InfoBal" class="headerlink" title="Global/Local InfoBal"></a>Global/Local InfoBal</h2><p>The final representation $R$ is the result of view aggregation:</p><script type="math/tex; mode=display">R = f(\{V_i\}_{i=1}^M;\phi)</script><p>In global/local MI maximization, the target is to maximize the Mutual Information between the global and the local representation of the data:</p><script type="math/tex; mode=display">J(\Theta, \phi) = \frac{1}{M}\sum_i^MI(V_i;R)</script><p>which lacks theoretical derivation why this is a more favorable objective. We could also do this adaptation to our new objective by replacing original input $X$ with final representation $R$:</p><script type="math/tex; mode=display">J(\Theta,\phi) = \frac{1}{M}\sum_i^MI(V_i;R) - \frac{2}{M(M-1)}\sum_{1\leq i < j\leq M}I(V_i,V_j)</script><p><img src="https://i.loli.net/2020/06/02/uFQgKn3bLodySl5.png" alt="image.png"></p><h2 id="Weighted-InfoBal"><a href="#Weighted-InfoBal" class="headerlink" title="Weighted InfoBal"></a>Weighted InfoBal</h2><p>Another potential optimiztion for InfoBal is to assign two weight matrix: view weight and correlation weight (这个名字不确定). </p><script type="math/tex; mode=display">J(\Theta,\phi) = \frac{1}{M}\sum_i^M w_i I(V_i;R) - \frac{2}{M(M-1)}\sum_{1\leq i < j\leq M} W_{i,j} I(V_i,V_j)</script><h2 id="Optimizing-InfoBal"><a href="#Optimizing-InfoBal" class="headerlink" title="Optimizing InfoBal"></a>Optimizing InfoBal</h2><p>One way is to <strong>adversarially optimize</strong> the InfoBal training objective, since the InfoMin is a minmax problem.</p><p><img src="https://i.loli.net/2020/05/26/iuf73oWIjRVbq2n.png" alt="image.png"></p><p>Another way is to <strong>found the upper bound</strong> of the objective and minimize the upper bound, which is introduced in another similar paper (refer to <a href="https://arxiv.org/abs/1906.01288" target="_blank" rel="noopener">NIPs-2019 paper “Information Competing Process for Learning Diversified Representations”</a>). This one involves the knowledge of variational inference, which will be investigated in the future. </p><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>将InfoBal模型的几个部分进行整理和划分，从Representation生成模型的角度一共能够分为以下四个部分：</p><ul><li>View Encoder: 从raw input进行多个view的编码模块</li><li>View Aggregator: 从view representation聚合成最终representation的模块</li><li>InfoMax critic: 用于MI maximization的parameterized DV representation模块</li><li>InfoMin discriminator: 最小化view之间的MI的discriminator模块</li></ul><p>而从InfoBal的优化目标来看，不同的term对于模型参数的影响也是不同的，下图展示了InfoBal的流程，以及不同loss term对InfoBal encoder架构的影响。</p><p><img src="https://i.loli.net/2020/06/02/uFQgKn3bLodySl5.png" alt="image.png"></p><p>We conducted the experiments on CIFAR10 dataset using the 2-view setting: instead of simultaneously maximizing the MI between the representation and the feature map vectors, we split the feature map into top/bottom parts and maximize the MI. </p><p>To evaluate the quality of the final representation, we plug the representation into a one-layer neural classifier as DIM does. We train the classifier for 4 epochs, which is by test converged, and evaluate the accuracy of the classifier. </p><p><img src="https://i.loli.net/2020/06/04/dclaybIKpjvnZxh.png" alt="image.png"></p><p>Change into JSD critic function, and we have the result: </p><p><img src="https://i.loli.net/2020/06/04/wLOtzi7qvHxA2N6.png" alt="image.png"></p><p>When trained in much longer period, we can even find out that the InfoBal objective is constantly doing harm to the representation quality. </p><p><img src="https://i.loli.net/2020/06/06/hudjfHWQ4As3Lia.png" alt="image.png"></p><p>The figure above is the experimental result, about which we have some questions and concerns: </p><ul><li>The lower bound of MI is increasing along training, while the downstream task accuracy is not improved (as much) as the MI estimate. </li><li>The improvement compared to the original starting points is rather marginal. </li><li>InfoBal objective is constantly hurting the performance of the representation learning model, despite the fact that the lower bound MI estimate value is nearly the same, which also implies that the MI is not so relevant to the representation quality. </li></ul><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p><strong>Up to now, we can conclude that the new objectice InfoBal is a failed attempt.</strong> One thing worth trying though, is to reimplement the DIM’s global/local objective in multiple (way more than 2) views and shed some light on our future exploration.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1907.13625&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ICLR-2020 paper “On Mut
      
    
    </summary>
    
    
      <category term="MI" scheme="http://www.shihaizhou.com/tags/MI/"/>
    
  </entry>
  
  <entry>
    <title>What Makes for Good Views for Contrastive Learning</title>
    <link href="http://www.shihaizhou.com/2020/05/26/What-Makes-for-Good-Views-for-Contrastive-Learning/"/>
    <id>http://www.shihaizhou.com/2020/05/26/What-Makes-for-Good-Views-for-Contrastive-Learning/</id>
    <published>2020-05-26T02:17:14.000Z</published>
    <updated>2020-05-28T07:16:30.680Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/abs/2005.10243" target="_blank" rel="noopener">paper “What Makes for Good Views for Contrastive Learning”</a></li><li><a href="https://arxiv.org/pdf/1406.2661.pdf" target="_blank" rel="noopener">paper “GAN”</a></li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>Despite the success of the Contrastive Multiview Coding (CMC), the influence of different view choices has been less studied. In this paper, we use empirical analysis to better understand the importance of view selection, and argue that we should reduce the mutual information (MI) between views while keeping task-relevant information intact. To verify this hypothesis, we devise unsupervised and semi-supervised frameworks that learn effective views by aiming to reduce their MI. </p><h2 id="Structure-of-Introduction"><a href="#Structure-of-Introduction" class="headerlink" title="Structure of Introduction"></a>Structure of Introduction</h2><ul><li>CMC relies on the fundamental assumption that important information is share across views, which means it’s view-invariant. </li><li>Then which viewing conditions should it be invariant to?</li><li>We therefore seek representations with enough invariance to be robust to inconsequential variations but not so much as to discard information required by downstream tasks.</li><li>We investigate this question in two ways.<ul><li>Optimal choice of views depends critically on the downstream task.</li><li>For many common ways of generating views, there is a sweet spot in terms of downstream performance where the mutual information (MI) between views is neither too high nor too low.</li></ul></li><li>InfoMin principle: A good set of views are those that share the minimal information necessary to perform well at the downstream task.</li></ul><h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><p><strong>Definition 4.1.</strong> (Sufficient Encoder) The encoder $f_1$ of $v_1$ is sufficient in the contrastive learning framework if and only if $I(v_1; v_2) = I(f_1(v_1); v_2)$.</p><p><strong>Definition 4.2.</strong> (Minimal Sufficient Encoder) A sufficient encoder $f_1$ of $v_1$ is minimal if and only if $I(f_1(v_1);v_1) \leq I(f(v_1);v_1) \forall f$, that are sufficient. Among those encoders which are sufficient, the minimal ones only extract relevant information of the contrastive task and will throw away other information.</p><p><strong>Definition 4.3.</strong> (Optimal Representation of a Task) For a task $\mathcal T$ whose goal is to predict a semantic label $y$ from the input data $x$, the optimal representation $z^\star$ encoded from $x$ is the minimal sufficient statistic with respect to $y$. 以上说明了$z^\star$保留了所有用于和task $\mathcal T$相关的信息，因此被称作optimal的。</p><h2 id="InfoMin-Principle"><a href="#InfoMin-Principle" class="headerlink" title="InfoMin Principle"></a>InfoMin Principle</h2><p>作者认为good view一定是和下游任务相关联的，The following InfoMin proposition articulates which views are optimal supposing that we know the specific downstream task $\mathcal T$ in advance.</p><p><img src="https://i.loli.net/2020/05/26/gDBvHGEa9cWwMtF.png" alt="image.png"></p><p>在附录中从Proposition 4.1能够推论出，遵循constraints</p><script type="math/tex; mode=display">I\left(\mathbf{v}_{1} ; \mathbf{y}\right)=I\left(\mathbf{v}_{2} ; \mathbf{y}\right)= I(\mathbf{x} ; \mathbf{y})</script><p>得到的view最优解满足以下：</p><script type="math/tex; mode=display">I\left(\mathbf{v}_{1}^{*} ; \mathbf{v}_{2}^{*}\right) = I(\mathbf{x} ; \mathbf{y})</script><p>在这个阶段为了说明不同view之间的information是如何影响downstream task的，文章构造了Colorful-Moving-MNIST数据集，简单来说就是带有背景的移动MNIST手写数字视频数据。</p><p><img src="https://i.loli.net/2020/05/26/z3H4UbjQFkqxdsa.png" alt="image.png"></p><p>假设一个数据点是$x_{1:t}$，那么将第一个view固定为$v_1=x_{1:k}$. 在这样的设置下，因为MNIST的手写数字移动方式是固定的，所以我们可以从前2帧推知后面帧的行动，因此$I(v_1, x)$是最大化的。同时构造出三个对应不同下游任务的$v_2$，通过最大化两个view之间的MI来学到representation，最后将这个representation带入下游任务中检查其表现，得到了以下表格，证明了不同view之间分享的information将会影响representation在下游任务上的表现。</p><p><img src="https://i.loli.net/2020/05/26/ifzZwYcdBXgrMex.png" alt="image.png"></p><h2 id="Unsupervised-InfoMin"><a href="#Unsupervised-InfoMin" class="headerlink" title="Unsupervised InfoMin"></a>Unsupervised InfoMin</h2><p>该工作实现了最小化MI的方法：那就是采用adversarial startegy解决以下的minmax problem:</p><script type="math/tex; mode=display">\min _{g} \max _{f_{1}, f_{2}} I_{N C E}^{f_{1}, f_{2}}\left(g(X)_{1} ; g(X)_{2: 3}\right)</script><p>这里我们复习一下GAN中adversarial training method. </p><p><img src="https://i.loli.net/2020/05/26/iuf73oWIjRVbq2n.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2005.10243&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;paper “What Makes for G
      
    
    </summary>
    
    
      <category term="MI" scheme="http://www.shihaizhou.com/tags/MI/"/>
    
  </entry>
  
  <entry>
    <title>Contrastive Multiview Coding</title>
    <link href="http://www.shihaizhou.com/2020/05/25/Contrastive-Multiview-Coding/"/>
    <id>http://www.shihaizhou.com/2020/05/25/Contrastive-Multiview-Coding/</id>
    <published>2020-05-25T08:16:33.000Z</published>
    <updated>2020-06-08T15:44:19.595Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Materials</strong></p><ul><li><a href="https://arxiv.org/abs/1906.05849" target="_blank" rel="noopener">paper “Contrastive Multiview Coding”</a></li></ul><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>对于人类，同一事物在不同sensory channel的信息是高度相关的. We study this hypothesis under the framework of multiview contrastive learning, where we learn a representation that aims to maximize mutual information between different views of the same scene but is otherwise compact.</p><h2 id="Structure-of-Introduction"><a href="#Structure-of-Introduction" class="headerlink" title="Structure of Introduction"></a>Structure of Introduction</h2><ul><li>Autoencoders treat bits equally. </li><li>We revisit the classic hypothesis that the good bits are the ones that are shared between multiple views of the world. This hypothesis corresponds to the inductive bias that the way you view a scene should not affect its semantics.</li><li>Our goal is therefore to learn representations that capture information shared between multiple sensory channels but that are otherwise compact (i.e. discard channel-specific nuisance factors). </li><li>Our main contribution is to set up a framework to extend these ideas to any number of views. </li></ul><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><p>文章从Multiview的角度将两大类方法进行了对比，分别为Predictive Learning和Contrastive Learning. </p><h2 id="Predictive-Learning"><a href="#Predictive-Learning" class="headerlink" title="Predictive Learning"></a>Predictive Learning</h2><p>Autoencoder方法被归为Predictive Learning的范畴中。这类方法最大的问题在于其优化目标objective假设了每个pixel之间是相互独立的，thereby reducing their ability to model correlations or complex structure. 为什么叫predictive learning呢，是因为在Multiview的setting下，我们希望建立一个从view1-&gt;representation-&gt;view2的映射，最小化这个预测之间的loss. 这个形式和autoencoder有些类似，关于multiview predictive learning和autoencoder之间的关系，我们做以下图示进行说明： </p><p><img src="https://i.loli.net/2020/05/25/OaH7mYLwsuPNrU8.png" alt="image.png"></p><p>其实在这样的角度理解multiview predictive coding，就是不同view之间不能够有信息沟通。<strong>相当于我们在AE的架构上做了更多的locality的限制，同时认为不同view之间在信息量上是等价的（能够从一个view推测出另一个view）</strong>，而该篇工作就建立在这个假设的进一步发展上：</p><blockquote><p>The good bits are the ones that are shared between multiple views of the world. </p></blockquote><p>因此使用Mutual Information来最优化不同view之间共享的信息就成了该篇工作的重点和亮点。</p><h2 id="Contrastive-Learning"><a href="#Contrastive-Learning" class="headerlink" title="Contrastive Learning"></a>Contrastive Learning</h2><p>Multiview Contrastive Learning的基本思想是：将同一个样本的不同view为一个正样本对$\left\{v_{1}^{i}, v_{2}^{i}\right\}_{i=1}^{N}$，对于第i个样本我们能够构造不同样本的不同view为一个负样本对$\left\{v_{1}^{i}, v_{2}^{j}\right\}_{j=1}^{K}$。通过训练一个critic function $h_\theta$来区分正负样本对，从而得到representation. 构造contrast loss为以下：</p><script type="math/tex; mode=display">\mathcal{L}_{\text {contrast}}=-\underset{S}{\mathbb{E}}\left[\log \frac{h_{\theta}(x)}{h_{\theta}(x)+\sum_{i=1}^{k} h_{\theta}\left(y_{i}\right)}\right]</script><p>从而计算$V_1$和$V_2$之间的contrast loss为：</p><script type="math/tex; mode=display">\mathcal{L}_{\text {contrast }}^{V_{1}, V_{2}}=-\mathbb E_{\left\{v_{1}^{1}, v_{2}^{1}, \ldots, v_{2}^{k+1}\right\}}\left[\log \frac{h_{\theta}\left(\left\{v_{1}^{1}, v_{2}^{1}\right\}\right)}{\sum_{j=1}^{k+1} h_{\theta}\left(\left\{v_{1}^{1}, v_{2}^{j}\right\}\right)}\right]</script><p>对称化两个view之间的loss：</p><script type="math/tex; mode=display">\mathcal{L}\left(V_{1}, V_{2}\right)=\mathcal{L}_{\text {contrast}}^{V_{1}, V_{2}}+\mathcal{L}_{\text {contrast}}^{V_{2}, V_{1}}</script><p>对于Multiview的setting，作者还给出了两种经常遇到的情况，分别是view中有一个Core View和全连接view的情况。</p><p><img src="https://i.loli.net/2020/05/25/Ou9tqDnbzIGhlN2.png" alt="image.png"></p><p>根据以上分别可以构造两个Loss function: </p><script type="math/tex; mode=display">\begin{align}\mathcal{L}_{C}&=\sum_{j=2}^{M} \mathcal{L}\left(V_{1}, V_{j}\right) \\\mathcal{L}_{F}&=\sum_{1 \leq i<j \leq M} \mathcal{L}\left(V_{i}, V_{j}\right)\end{align}</script><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>文章一共做了三组实验：</p><ul><li>Two established image representation learning benchmarks: <strong>ImageNet and STL-10</strong></li><li>Video representation learning tasks with 2 views: image and optical flow modalities</li><li>More than 2 views. </li></ul><h3 id="How-does-mutual-information-affect-representation-quality"><a href="#How-does-mutual-information-affect-representation-quality" class="headerlink" title="How does mutual information affect representation quality?"></a>How does mutual information affect representation quality?</h3><p>文章认为，Mutual Information Maximization不是让CMC成功的关键，关键是在于CMC背后关于representation的假设：“好的信息应该在不同的view之间被共享”，从而最大化不同view之间representation的MI，才能导致好的representation. </p><p>文章在高清2K的图片中random crop $64\times 64$的patch来进行Mutual Informatio之间的训练，唯一变量是patch之间间隔的pixel距离，通过两个view之间的MI estimation的差距来比较MI对最终downstream classification的影响，结果如下右图所示（左图表示的不同的RGB通道之间的关联，结果有些违反常识）.</p><p><img src="https://i.loli.net/2020/05/25/gsbQRm5A6ef7o3F.png" alt="image.png"></p><p>根据右图显示的结果，作者得出结论：</p><blockquote><p>Here we see that views with too little or too much MI perform worse; a sweet spot in the middle exists which gives the best representation. That there exists such a sweet spot should be expected. If two views share no information, then, in principle, there is no incentive for CMC to learn anything. If two views share all their information, no nuisances are discarded and we arrive back at something akin to an autoencoder or generative model, that simply tries to represent all the bits in the multiview data.</p><p>These experiments demonstrate that the relationship between mutual information and representation quality is meaningful but not direct. Selecting optimal views, which just share relevant signal, may be a fruitful direction for future research.</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Materials&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.05849&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;paper “Contrastive Mult
      
    
    </summary>
    
    
      <category term="MI" scheme="http://www.shihaizhou.com/tags/MI/"/>
    
      <category term="Contrastive" scheme="http://www.shihaizhou.com/tags/Contrastive/"/>
    
  </entry>
  
</feed>
